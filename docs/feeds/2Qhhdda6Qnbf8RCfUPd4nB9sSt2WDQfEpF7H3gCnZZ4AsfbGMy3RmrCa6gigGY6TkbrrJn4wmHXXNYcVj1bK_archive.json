{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Rust in 2025: Language interop and the extensible compiler","url":"https://smallcultfollowing.com/babysteps/blog/2025/03/18/lang-interop-extensibility/","date":1742655784,"author":"/u/kibwen","guid":605,"unread":true,"content":"<p>For many years, C has effectively been the “lingua franca” of the computing world. It’s pretty hard to combine code from two different programming languages in the same process–unless one of them is C. The same could theoretically be true for Rust, but in practice there are a number of obstacles that make that harder than it needs to be. Building out <strong>silky smooth language interop</strong> should be a core goal of helping Rust to target <a href=\"https://smallcultfollowing.com/babysteps/\n/blog/2025/03/10/rust-2025-intro/\">foundational applications</a>. I think the right way to do this is not by extending rustc with knowledge of other programming languages but rather by building on Rust’s core premise of being an extensible language. By investing in building out an  we can allow crate authors to create a plethora of ergonomic, efficient bridges between Rust and other languages.</p><h2>We’ll know we’ve succeeded when…</h2><p>When it comes to interop…</p><ul><li>It is easy to create a Rust crate that can be invoked from other languages and across multiple environments (desktop, Android, iOS, etc). Rust tooling covers the full story from writing the code to publishing your library.</li><li>It is easy to carve out parts of an existing codebase and replace them with Rust. It is  easy to integrate Rust into C/C++ codebases.</li></ul><p>When it comes to extensibility…</p><ul><li>Rust is host to wide variety of extensions ranging from custom lints and diagnostics (“clippy as a regular library”) to integration and interop (ORMs, languages) to static analysis and automated reasoning^[math].</li></ul><p>In my head, I divide language interop into two core use cases. The first is what I call  (LCD), where people would like to write one piece of code and then use it in a wide variety of environments. This might mean authoring a core SDK that can be invoked from many languages but it also covers writing a codebase that can be used from both Kotlin (Android) and Swift (iOS) or having a single piece of code usable for everything from servers to embedded systems. It might also be creating <a href=\"https://bytecodealliance.org/\">WebAssembly components</a> for use in browsers or on edge providers.</p><p>What distinguishes the LCD use-case is two things. First, it is primarily unidirectional—calls mostly go  the other language  Rust. Second, you don’t have to handle all of Rust. You really want to expose an API that is “simple enough” that it can be expressed reasonably idiomatically from many other languages. Examples of libraries supporting this use case today are <a href=\"https://mozilla.github.io/uniffi-rs/latest/\">uniffi</a> and <a href=\"https://rust-diplomat.github.io/book/\">diplomat</a>. This problem is not new, it’s the same basic use case that <a href=\"https://component-model.bytecodealliance.org/\">WebAssembly components</a> are targeting as well as old school things like <a href=\"https://en.wikipedia.org/wiki/Component_Object_Model\">COM</a> and <a href=\"https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture\">CORBA</a> (in my view, though, each of those solutions is a bit too narrow for what we need).</p><p>When you dig in, the requirements for LCD get a bit more complicated. You want to start with simple types, yes, but quickly get people asking for the ability to make the generated wrapper from a given language more idiomatic. And you want to focus on calls  Rust, but you also need to support callbacks. In fact, to really integrate with other systems, you need generic facilities for things like logs, metrics, and I/O that can be mapped in different ways. For example, in a mobile environment, you don’t necessarily want to use tokio to do an outgoing networking request. It is better to use the system libraries since they have special cases to account for the quirks of radio-based communication.</p><p>To really crack the LCD problem, you also have to solve a few other problems too:</p><ul><li>It needs to be easy to package up Rust code and upload it into the appropriate package managers for other languages. Think of a tool like <a href=\"https://github.com/PyO3/maturin\">maturin</a>, which lets you bundle up Rust binaries as Python packages.</li><li>For some use cases,  is a very important constraint. Optimizing for size right now is hard to start. What’s worse, your binary has to include code from the standard library, since we can’t expect to find it on the device—and even if we could, we couldn’t be sure it was ABI compatible with the one you built your code with.</li></ul><h2>Needed: the “serde” of language interop</h2><p>Obviously, there’s enough here to keep us going for a long time. I think the place to start is building out something akin to the “serde” of language interop: the <a href=\"https://crates.io/crates/serde\">serde</a> package itself just defines the core trait for serialization and a derive. All of the format-specific details are factored out into other crates defined by a variety of people.</p><p>I’d like to see a universal set of conventions for defining the “generic API” that your Rust code follows and then a tool that extracts these conventions and hands them off to a backend to do the actual language specific work. It’s not essential, but I think this core dispatching tool should live in the rust-lang org. All the language-specific details, on the other hand, would live in crates.io as crates that can be created by anyone.</p><h2>Lang interop: the “deep interop” use case</h2><p>The second use case is what I call the  problem. For this use case, people want to be able to go deep in a particular language. Often this is because their Rust program needs to invoke APIs implemented in that other language, but it can also be that they want to stub out some part of that other program and replace it with Rust. One common example that requires deep interop is embedded developers looking to invoke gnarly C/C++ header files supplied by vendors. Deep interop also arises when you have an older codebase, such as the Rust for Linux project attempting to integrate Rust into their kernel or companies looking to integrate Rust into their existing codebases, most commonly C++ or Java.</p><p>Some of the existing deep interop crates focus specifically on the use case of invoking APIs from the other language (e.g., <a href=\"https://github.com/rust-lang/rust-bindgen\">bindgen</a> and <a href=\"https://duchess-rs.github.io/duchess/\">duchess</a>) but most wind up supporting bidirectional interaction (e.g., <a href=\"https://pyo3.rs/v0.23.5/\">pyo3</a>, [npapi-rs][], and <a href=\"https://neon-rs.dev\">neon</a>). One interesting example is <a href=\"https://cxx.rs\">cxx</a>, which supports bidirectional Rust-C++ interop, but does so in a rather opinionated way, encouraging you to make use of a subset of C++’s features that can be readily mapped (in this way, it’s a bit of a hybrid of LCD and deep interop).</p><h2>Interop with all languages is important. C and C++ are just more so.</h2><p>I want to see smooth interop with all languages, but C and C++ are particularly important. This is because they have historically been the language of choice for foundational applications, and hence there is a lot of code that we need to integrate with. Integration with C today in Rust is, in my view, “ok” – most of what you need is there, but it’s not as nicely integrated into the compiler or as accessible as it should be. Integration with C++ is a huge problem. I’m happy to see the Foundation’s <a href=\"https://rustfoundation.org/interop-initiative/\">Rust-C++ Interoperability Initiative</a> as well a projects like Google’s <a href=\"https://github.com/google/crubit\">crubit</a> and of course the venerable <a href=\"https://github.com/dtolnay/cxx\">cxx</a>.</p><p>The traditional way to enable seamless interop with another language is to “bake it in” i.e., Kotlin has very smooth support for invoking Java code and Swift/Zig can natively build C and C++. I would prefer for Rust to take a different path, one I call . The idea is to enable interop via, effectively, supercharged procedural macros that can integrate with the compiler to supply type information, generate shims and glue code, and generally manage the details of making Rust “play nicely” with another language.</p><p>In some sense, this is the same thing we do today. All the crates I mentioned above leverage procedural macros and custom derives to do their job. But procedural macrods today are the “simplest thing that could possibly work”: tokens in, tokens out. Considering how simplistic they are, they’ve gotten us remarkably, but they also have distinct limitations. Error messages generated by the compiler are not expressed in terms of the macro input but rather the Rust code that gets generated, which can be really confusing; macros are not able to access type information or communicate information between macro invocations; macros cannot generate code on demand, as it is needed, which means that we spend time compiling code we might not need but also that we cannot integrate with monomorphization. And so forth.</p><p>I think we should integrate procedural macros more deeply into the compiler. I’d like macros that can inspect types, that can generate code in response to monomorphization, that can influence diagnostics and lints, and maybe even customize things like method dispatch rules. That will allow all people to author crates that provide awesome interop with all those languages, but it will also help people write crates for all kinds of other things. To get a sense for what I’m talking about, check out <a href=\"https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/\">F#’s type providers</a> and what they can do.</p><p>The challenge here will be figuring out how to keep the stabilization surface area as small as possible. Whenever possible I would look for ways to have macros communicate by generating ordinary Rust code, perhaps with some small tweaks. Imagine macros that generate things like a “virtual function”, that has an ordinary Rust signature but where the body for a particular instance is constructed by a callback into the procedural macro during monomorphization. And what format should that body take? Ideally, it’d just be Rust code, so as to avoid introducing any new surface area.</p><h2>Not needed: the Rust Evangelism Task Force</h2><p>So, it turns out I’m a big fan of Rust. And, I ain’t gonna lie, when I see a prominent project pick some other language, at least in a scenario where Rust would’ve done equally well, it makes me sad. And yet I also know that if  project were written in Rust, that would be . I mean, who would we steal good ideas from?</p><p>I really like the idea of focusing our attention on <em>making Rust work well with other languages</em>, not on convincing people Rust is better . The easier it is to add Rust to a project, the more people will try it – and if Rust is truly a better fit for them, they’ll use it more and more.</p><p>This post pitched out a north star where</p><ul><li>a single Rust library can be easily used across many languages and environments;</li><li>Rust code can easily call and be called by functions in other languages;</li><li>this is all implemented atop a rich procedural macro mechanism that lets plugins inspect type information, generate code on demand, and so forth.</li></ul><p>How do we get there? I think there’s some concrete next steps:</p><ul><li>Build out, adopt, or extend an easy system for producing “least common denominator” components that can be embedded in many contexts.</li><li>Look for ways to extend proc macro capabilities and explore what it would take to invoke them from other phases of the compiler besides just the very beginning.<ul><li>An aside: I also think we should extend rustc to support compiling proc macros to web-assembly and use that by default. That would allow for strong sandboxing and deterministic execution and also easier caching to support faster build times.</li></ul></li></ul>","contentLength":10819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1jhaf9a/rust_in_2025_language_interop_and_the_extensible/"},{"title":"[Media] Perfect!","url":"https://www.reddit.com/r/rust/comments/1jh8zmp/media_perfect/","date":1742651803,"author":"/u/wpg4665","guid":609,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My first days with Rust from the perspective of an experienced C++ programmer (continued)","url":"https://www.reddit.com/r/rust/comments/1jh78e2/my_first_days_with_rust_from_the_perspective_of/","date":1742646138,"author":"/u/Rough-Island6775","guid":604,"unread":true,"content":"<p>Using AIs with questions such as how do I do this and that in Rust describing things that I know are there makes the transition smooth.</p><p>What first seemed like elaborate syntax makes perfect sense and probably as good as it can be.</p><p>I will read the Rust book and the reference to get formally educated but for now AI acts as a tutor answering things that it has seen plenty of times, noob questions.</p><p>The binary is larger, as expected, primarily (I think) due to the initial data structure is built in a function instead of hard-coded as a global.</p><p>Somewhat larger binary is expected and acceptable due to the built in safeties of Rust.</p><p>Without AI the learning curve is a bit steep and for a programming noob is probably off-putting. For an experienced C++ programmer is just: \"yeah, that's better\" and it keeps giving me a tiny smile every time that happens.</p><p>I begin to understand the cult like following Rust has because once a learning step in the curve is taken it feels like there is no going back.</p><p>I have a lot to learn, but for now, for my toy bare-metal application, I feel that this is the way forward.</p><p>p.s. I was pleasantly surprised by how extensive the core library is and that it works in [no_std] builds.</p>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fastrace: A Modern Approach to Distributed Tracing in Rust","url":"https://www.reddit.com/r/rust/comments/1jh2fzg/fastrace_a_modern_approach_to_distributed_tracing/","date":1742625553,"author":"/u/RealisticBorder8992","guid":608,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/RealisticBorder8992\"> /u/RealisticBorder8992 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why do strings have to be valid UTF-8?","url":"https://www.reddit.com/r/rust/comments/1jgxh3y/why_do_strings_have_to_be_valid_utf8/","date":1742607799,"author":"/u/SaltyMaybe7887","guid":606,"unread":true,"content":"<p>fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; { let mut file = std::fs::File::open(\"number\")?; let mut buf = [0_u8; 128]; let bytes_read = file.read(&amp;mut buf)?;</p><pre><code>let contents = &amp;buf[..bytes_read]; let contents_str = std::str::from_utf8(contents)?; let number = contents_str.parse::&lt;i128&gt;()?; println!(\"{}\", number); Ok(()) </code></pre><p>Why is it necessary to convert the slice of bytes to an ? When I run , it will validate that  is valid UTF-8. But to parse this string into an integer, I only care that each byte in the slice is in the ASCII range for digits as it will fail otherwise. It seems like the  adds unnecessary overhead. Is there a way I can avoid having to validate UTF-8 for a string in a situation like this?</p><p> I probably should have mentioned that the file is a cache file I write to. That means it doesn’t need to be human-readable. I decided to represent the number in little endian. It should probably be more efficient than encoding to / decoding from UTF-8. Here is my updated code to parse the file:</p><p>fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; { const NUM_BYTES: usize = 2;</p><pre><code>let mut file = std::fs::File::open(\"number\")?; let mut buf = [0_u8; NUM_BYTES]; let bytes_read = file.read(&amp;mut buf)?; if bytes_read &gt;= NUM_BYTES { let number = u16::from_le_bytes(buf); println!(\"{}\", number); } Ok(()) </code></pre><p>If you want to write to the file, you would do something like , so it’s the other way around.</p>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is the standard library for cryptographic operations in RUST.","url":"https://www.reddit.com/r/rust/comments/1jgqw88/what_is_the_standard_library_for_cryptographic/","date":1742589460,"author":"/u/paulex101","guid":607,"unread":true,"content":"<p>I've stumbled on quite some libraries but this seem to be the tops: - Ring</p><p>And for everyone there's always a warning \"Use at your own Risk\" i must say i find this funny and bothering at the same time coming from stable ecosystems e.g Java/Kotlin/JS </p><p>For context: I really just want to generate ECDH Key Pair, compute shared secrets and key derivations. </p><p>I'm just a few days new to Rust so please be nice!.</p>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","rust"]}