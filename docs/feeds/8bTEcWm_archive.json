{"id":"8bTEcWm","title":"COVER","displayTitle":"COVER","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":5,"items":[{"title":"Shishiro Hai – A Dynamic Crossover of eSports and VTuber Culture","url":"https://coveredge.cover-corp.com/en/list/2502","date":1753326000,"author":"cover_hirata","guid":604,"unread":true,"content":"<h2>The Vision Behind Shishiro Hai: Where eSports Meets VTubers</h2><p>In recent years, the esports industry has been gaining remarkable momentum. According to the <a href=\"https://www.lab-kadokawa.com/release/detail.php?id=0315\"> (Japan eSports White Paper) 2024 by KADOKAWA ASCII Research Laboratories Inc., </a>Japan’s domestic esports market reached JPY 14.685 billion in 2023 – a 117% increase from the previous year and looking ahead to 2025, the market is expected to approach 20 billion yen in scale. Some companies from a wide range of industries are stepping into the scene, becoming sponsors of professional esports teams and supporting major tournaments and events.&nbsp;</p><p>Did you know that the worlds of esports and VTubers are coming together more than ever before?</p><p>For those who aren’t so familiar with gaming, the esports scene and professional tournaments can sometimes feel a bit out of reach, but when it is a VTuber you already follow stepping into the arena, that barrier may not feel quite so high. Fans can cheer on their favorite VTubers as they improve from beginner level to becoming skilled players, and watch them connect with other creators through gaming. In the process, viewers naturally find themselves learning more about the game and enjoying it alongside their favorite talents.&nbsp;</p><p>Some VTubers have even gone beyond simply practicing on their own, with some embracing being coached by professional esports players to sharpen their skills and improve their chances of winning in tournaments and competitive matches. Coaching sessions are often shared as livestreams or videos, serving as a unique bridge between the esports and VTuber communities, and as these collaborations have continued over time, some VTubers and pro gamers have even developed bonds that feel almost like a mentor/apprentice relationship.&nbsp;</p><p>The lifelong gamer at heart received coaching from several professional players and steadily streamed her progress reaching Master Rank – a level widely recognized as the mark of an advanced player – in just one year, and has even been interviewed by media outlets about her tips when choosing gaming devices. It was not only her dream to compete in these kinds of tournaments but the challenge of hosting them that led to the creation of Shishiro Hai.</p><p>With Shishiro Botan taking the lead on tournament planning and day-of operations, and our company supporting coordination with partner organizations, the first Shishiro Hai STREET FIGHTER 6 powered by GALLERIA was held on March 31, 2024. This was followed by a second tournament on October 13-14 of the same year. Perhaps in recognition of her efforts to create and run these events, Shishiro Botan was honored with the title of VTuber of the Year 2024 in the Live Entertainment category at the JAPAN eSPORTS AWARDS 2024, announced at the year’s end.</p><h2>What Makes Shishiro Hai Unique?</h2><p>Online esports tournaments take place all the time in all kinds of formats, but here are a few key reasons why Shishiro Hai is unique.</p><p>Firstly, Shishiro Botan’s own influence and the strength of her brand have made it possible to bring in renowned players and commentators alongside fellow VTubers and creators from other communities, with the tournament not only attracting a range of different fanbases, but also creating dream matchups you won’t see anywhere else.&nbsp;</p><p>The commentary team features a lineup invited by Shishiro Botan personally, including esports commentator Kousuke Hiraiwa, formerly of Asahi Television Broadcasting, along with other experienced commentators and professional players. Not only does their involvement help ensure the tournament’s quality and legitimacy, but their clear commentary and insightful analysis also makes matches easier to follow and understand for viewers who may have never played STREET FIGHTER 6 themselves before or who are unfamiliar with the game altogether.</p><p>Another unique aspect is the inclusion of open entry slots. For example, when recruiting for the 3rd tournament, applicants needed to meet several criteria: hold a qualifying rank in STREET FIGHTER 6, have prior experience streaming the game, be able to broadcast from their own perspective during the event, and having at least 1,000 channel subscribers. Because of the format of the tournament, participants also had to be able to dedicate up to two full days exclusively for Shishiro Hai. Despite these conditions, there were an impressive 254 applications.&nbsp;</p><p>One particularly interesting aspect of Shishiro Hai is how the participants were eventually selected – live on Shishiro Botan’s own stream. With so many dedicated fans hoping to see their favorite players chosen, it is only natural that everyone would have different opinions during the selection process, but by making every step of the decision-making process more transparent, Shishiro Botan has been able to turn what could be tense proceedings into something truly entertaining that everyone can share and enjoy together.</p><p>Above all, what truly sets Shishiro Hai apart is its scale – it is one of the largest tournaments ever hosted by a VTuber. In the next section, we will have a look at data reports as well as the sponsorships and partner collaborations that have made this event possible.</p><h2>Shishiro Hai Achievements and Sponsorship Highlights</h2><h3>Shishiro Hai STREET FIGHTER 6 powered by GALLERIA</h3><p>The very first Shishiro Hai was held on March 31, 2024, with the main stream on Shishiro Botan’s channel reaching a peak of 80,045 concurrent viewers, and including mirror streams, this increased to 190,806. The tournament amazingly had 301,897 unique viewers overall and was trending at #4 on X in Japan.&nbsp;&nbsp;</p><p>Sponsors (in no particular order): Gaming PC GALLERIA (THIRDWAVE CORP) / nosh (nosh Inc.) / VICTRIX by PDP (SB C&amp;S Corp.) / Red Bull Japan (Red Bull Japan Co., Ltd.) / GiGO (GENDA GiGO Entertainment Inc.) / esports Challenger’s Park (QTnet, Inc.) / GOKURAKUYU HOLDINGS CO., LTD. / TOKYO DESIGN TECHNOLOGY CENTER / VAXEE (VAXEE) / TSUKUMO (Project White Co., Ltd.)</p><h3>The 2nd Shishiro Hai STREET FIGHTER 6 powered by GALLERIA</h3><p>The second tournament grew even larger, taking place over two days on October 13-14, 2024. On Day 1, the main broadcast peaked at 56,643 concurrent viewers, reaching 147,402 concurrent viewers when taking mirror streams into account, and 296,410 unique viewers in total. Day 2 attracted an even bigger audience, with the main stream peaking at 72,040 concurrent viewers (150,096 including mirrors) and a total of 305,969 unique viewers. 　</p><p>Sponsors (in no particular order): Gaming PC GALLERIA (THIRDWAVE CORP) / Red Bull Japan (Red Bull Japan Co., Ltd.）/ elgato / Aishitoto Co., Ltd. / VAXEE (VAXEE) / REALFORCE (Topre Corporation) / PWS JAPAN (Kanematsu PWS LTD.) / esports Challenger’s Park (QTnet,Inc.) / TOKYO DESIGN TECHNOLOGY CENTER / GRAPHT (MSY Inc.) / Yaro Ramen (FOOD REVAMP CO.,LTD.) /&nbsp; DMM Factory (<a href=\"http://dmm.com/\">DMM.com</a> LLC) / nosh (nosh Inc.)&nbsp;</p><h3>The 3rd Shishiro Hai STREET FIGHTER 6 powered by HP</h3><p>The 3rd Shishiro Hai went up yet another notch, with the tournament on Day 1 featuring a Master Division competition along with an Exhibition Match, followed by Grand Master and Legend Division tournaments on Day 2.&nbsp;</p><p>This time, the tournament had a slightly more creative format with a unique feature: a cross-divisional progression system. This unique approach, proposed by Shishiro Botan, allowed the winners of the Master and Grand Master Division tournaments to challenge players in the next division up. From the moment it was announced, this system was met with an enthusiastic response from both viewers and participants.&nbsp;</p><p>In fact, <a href=\"https://www.youtube.com/channel/UChLfthKoUV502J7gU9STArg\">Shibuya HAL</a>, who won the Master Division, and <a href=\"https://x.com/DONPISHA22\">Donpisha</a>, who took first place in the Grand Master Division, both went on to secure impressive victories in the higher divisions as well proving that the cross-divisional progression system not only worked as intended, but was responsible for some unforgettable moments unique to Shishiro Hai.&nbsp;</p><p>The exhibition match held during the latter half of Day 1 featured a team competition, with two groups of four players facing off against each other for both prizes and to avoid punishments, creating a fun and relaxed atmosphere. The mix of competitive tournaments and casual fun kept things fresh and engaging for beginners and veteran players, ensuring there was always something to enjoy for everyone.&nbsp;</p><p>Sponsors for the third event were as follows:HP Inc. / Red Bull Japan (Red Bull Japan Co., Ltd.) / GRAPHT (MSY Inc.) / TOKYO DESIGN TECHNOLOGY CENTER / VAXEE (VAXEE) / HUB (HUB CO., LTD.) / JOYPOLIS (CA Sega Joypolis Ltd.) / Turtle Beach (SB C&amp;S Corp.) / Ajinomoto Co., Inc. “あえて、®”</p><p>There have also been some standout initiatives that have made this event truly special.&nbsp;</p><p>First, thanks to HP Inc., the main sponsor of the tournament, Shishiro Botan hosted an interactive livestream on her channel ahead of the event where viewers could take part in special missions to unlock the Sponsorship Sale for the 3rd Shishiro Hai, which was successful thanks to everyone’s efforts in completing the challenges. This challenge-based initiative sought to involve the audience more and add to the festival atmosphere surrounding the tournament.&nbsp;</p><p>During the two days of the main event, public viewings were held at HUBs (UK-style pubs) across Japan, with special Shishiro Hai collaboration drinks also available from May 9 to 25.</p><p>Right before the tournament, the brand “あえて、®” (pronounced ) from Ajinomoto Co., Inc. was also announced as an official sponsor, but as they had only just launched its own esports initiatives, they contributed by offering their “あえて、®” frozen meal deliveries for the event. During the main event, there was a special segment featuring a fun quiz called  (cooked rice with seasonal ingredients) Blind Challenge that introduced some of the “あえて、®” range and highlighted the uniqueness of the meals in an engaging way.&nbsp;</p><h2><strong><strong>The Value Created by Shishiro Hai</strong></strong></h2><p>With the support of countless participants and partner companies, Shishiro Hai has evolved into an unprecedentedly influential tournament, opening the door for fans who usually follow VTuber streams to discover the energy and passion of the esports and professional gaming worlds. We even saw the emergence of a few Cinderella stories, with the spotlight shining on certain players in ways they hadn’t experienced before. Above all, everyone involved wants the same thing – to entertain viewers and to have fun themselves: a mindset evident in every part of the event and is one of the reasons Shishiro Hai is continued to be loved by all.&nbsp;</p><p>Of course, the viewers themselves are an essential part of what makes the tournament so special, but it goes beyond cheering for one’s favorite players in stream chat sections. The excitement and inspiration generated by Shishiro Hai can often be seen as the catalyst for all kinds of new connections and activities with many people joining sponsorship campaigns or discovering new creators to support even after watching their performances at this event.&nbsp;&nbsp;</p><p>Stay tuned for the upcoming second and third installments of this special feature, where we interview partners who were on board for Shishiro Hai, the team behind the project, and Shishiro Botan herself who gives us a sneak peek into the collaboration that goes on behind the scenes.</p><p>* Parts 2 and 3 of this article will be published soon.</p><p><sup>©CAPCOM* All company names, product names, and service names mentioned on this page are trademarks or registered trademarks of their respective companies.</sup></p>","contentLength":11522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CEDEC2025: COVER Engineers Discuss the Forefront of Live Production Technology","url":"https://coveredge.cover-corp.com/en/list/2471","date":1752112800,"author":"平松梨沙","guid":543,"unread":true,"content":"<p>COVER Corporation is pleased to announce that two of our engineers will present at , scheduled from July 22-24, 2025.</p><p> is one of Japan’s largest conferences dedicated to computer entertainment developers, providing a platform for sharing the latest gaming industry-related technologies and insights. In the two sessions featuring our engineers, we will showcase our cutting-edge technology and behind-the-scenes development that power the live performances of hololive production, where the real and virtual seamlessly come together.&nbsp;</p><h2>Remote Live Performances! Transmission Technology Enabling Real-Time Sessions with On-Site Bands</h2><p>Wednesday, July 24, 11:10 AM -12:10 PM (Venue #6)Speaker: Toshiyuki Ono (Livestream Technology Division Filming Team/Studio Maintenance Team Manager, Creative Production Department)</p><p>During this session, we will present our ultra-low latency transmission technology, enabling performers in remote studios to engage in real-time music sessions with live bands on site, and take a look behind the scenes at how live vocal performances by VTubers come to life, sharing insights from the forefront of network innovation.&nbsp;</p><h2>Feeling Like They Are Actually There: Lighting Design Enhancing the Realism of hololive production’s On-Site and AR Live Shows</h2><p>Wednesday, July 24, 4:40 PM – 5:40 PM (Venue #7)Speaker: Akitsugu Hirano (Development Team #3, Creative Production Department)</p><p>This session will explore lighting technologies that create an immersive experience of “actually being there” during both AR and on-site live performances. We will share our proprietary lighting system developed with Unity, how we have implemented ray tracing, and advanced techniques for precisely synchronizing real and virtual elements.</p><h5>Note: You will need to have a valid attendee pass to attend CEDEC. For more information, please check the <a href=\"https://cedec.cesa.or.jp/2025/ticket_guide/\">website</a> (Japanese only).</h5>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delivering New Experiences to Fans While Empowering Talent – Inside COVER’s Licensing Business","url":"https://coveredge.cover-corp.com/en/list/2390","date":1751511600,"author":"cover_yamaguchi","guid":502,"unread":true,"content":"<p>I began my career at a major entertainment company right after graduating, involved in a range of areas like music and talent management. After that, I changed to an entertainment company called Tokyo Otaku Mode where I first started working with 2D content and gained experience as a buyer and a licensee as well as managing partners, which is actually the opposite of the role I have now.<p>After I left Tokyo Otaku Mode, I was job hunting when a friend introduced me to COVER. At that time, I had already accepted an offer from another company, but my friend strongly encouraged me to meet with Mr. Tanigo, saying what an interesting person he is and how much I should really talk to him. So I decided to go ahead and meet with him.</p>Back then, I honestly had no intention of joining, and I was pretty skeptical about VTuber culture in general and looking back now, I realize I probably came across as quite dismissive during my conversation with him.&nbsp;<p>Mr. Tanigo shared his strong passion for hololive production and explained his long-term vision in detail when we sat down and talked. He spoke about how genuinely believes more than anyone else in the potential of hololive production and its talents, and he described exactly what he felt was needed to help that potential grow.&nbsp;</p>Hearing his views made me start to feel that maybe I could contribute to this vision as well, and that I wanted to take on the challenge together, which is what ultimately inspired me to join the company.&nbsp;<p>At the time, Mr. Tanigo was actually looking for someone with experience in product planning and development, but during our conversation, he asked me, “How would you feel about looking after licensing? You’re great at talking with people, and I have a feeling this could really suit you,” which led to me being given the opportunity to lead the launch of the licensing business.</p></p>","contentLength":1873,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cover x Epic Games: Exploring New VTuber Horizons with Unreal Engine","url":"https://coveredge.cover-corp.com/en/list/2320","date":1750993200,"author":"cover_hirata","guid":473,"unread":true,"content":"<div><div><div><div><p>Managing Director: Takayuki Kawasaki</p></div></div></div></div><div><div><p>Customer Success Director: Noriaki Shinoyama</p></div></div><div><div><ul><li>Ikko Fukuda, Director and CTO&nbsp;</li><li>Tsuyoshi Okugawa (Manager/Graphics Engineer), Art Engineering Team, Technology Development Division, Creative Production Department&nbsp;</li><li>Akitsugu Hirano (Lead Engineer), Development Team #3, Technology Development Division, Creative Production Department</li><li>Hyogo Ito (3D Designer), Unreal Engine Development Team, Technology Development Division, Creative Production Department&nbsp;&nbsp;</li><li>Noriyuki Hiromoto (Graphics Engineer), SPARK inc.</li></ul></div></div><h2>Pursuing Realism and Living Up to Talent Expectations:How VTuber Live Technology is Evolving with Unreal Engine</h2><div><div><p>&nbsp;I recently watched the one-and-a-half year anniversary live performance from ReGLOSS using Unreal Engine and the quality was incredibly high, genuinely moving me. Could you walk us through what led to the decision to incorporate Unreal Engine into this livestream, and how the technology behind your livestream has evolved over time?</p></div></div><div><div><p>Thank you very much. Music livestreams for hololive production originally began with the use of motion capture VR equipment. At the time, there were many limitations in terms of hardware and available space, and from a technical standpoint, it was quite difficult to handle longer streams. When it comes to graphics, improvements haven’t been just one giant leap forward, but rather from steady, incremental progress over time. What’s particularly worth noting is that this evolution hasn’t been driven by technology alone – it’s also thanks to the ever-improving skills of our on-site team members who handle lighting and camera work, as well as the increasing expertise of our production staff.<p>As we continued to evolve our production, we still felt there were limitations in our ability to recreate a true sense of presence and immersion to capture the sheer amount of information and realism that a live concert delivers and translate that in a virtual space. While grappling with those challenges, I happened to see the </p> demo for Unreal Engine 5 released by Unreal Engine, and I was blown away by the sheer volume of detail, making me excited if we were able to use it for our livestreaming. These days, even individual creators are able to produce high quality content using Unreal Engine, so given this shift, we felt it was essential for us as a company to move quickly and begin offering livestreams powered by Unreal Engine as well. That’s why we made the decision to speed up our timeline and adopt the technology so soon.&nbsp;</p></div></div><div><div><p>How has the response to the livestream powered by Unreal Engine been from fans and talents?</p></div></div><div><div><p>One of the major reasons we adopted Unreal Engine was to better support our talents – to help bring to life performances and expressions they envision. As we gave them progress reports throughout the production process, we could sense their growing excitement – a feeling of “we may finally be able to do the things we weren’t able to before.” In fact, we’ve already received numerous requests from other talents saying they’d love to do a livestream using Unreal Engine as well. Being able to provide an environment where we can meet those expectations is incredibly meaningful.&nbsp;&nbsp;<p>The high quality in graphics really surprised and excited fans and talents, even prompting lively discussions on social media, with people speculating about what might be possible with the Unreal Engine. What really stood out among fan reactions was how attentive they were to technical details and how often they would comment on them. We even got comments from those with seemingly quite specialized knowledge noting the incredible use of skeletal mesh for the water effects for instance. It was really encouraging for us to see that level of engagement.&nbsp;</p><p>Internally, we also treated this as a kind of technical showcase and gave non-engineering staff the opportunity to experience the live environment built with Unreal Engine. It was great to see so many people take interest and participate – even those without a technical background.</p><p>I know we have started using Unreal Engine for music livestreams, but it was originally developed as a game engine. In Japan, what kinds of fields is Unreal Engine currently being used in?</p></p></div></div><div><div><p>In Japan, it has been strongly adopted by television broadcasters in particular. Unreal Engine is highly valued in such environments for its efficiency and speed, qualities that are especially important in the fast-paced world of regular programming and drama production.&nbsp; For example, it’s been used in a variety of settings, including shoots for NHK’s historical dramas that use LEDs and commercial network productions, as well as in virtual studios for visual effects work. Recently, stylized expressions () – including toon shading () – have become increasingly common in the anime production field as well. Unreal Engine is now being used in a wide range of visual and broadcast productions, and the number of projects adopting it continues to grow significantly.</p></div></div><p><sup>Note 1: “Stylized” refers to expressing objects or characters in a unique design or applying a consistent visual styleNote 2: “Toon shading” is a technique used to make 3DCG look like hand-drawn illustrations or cel-style animation.</sup></p><div><div><p>In television and other video production on-site, deadlines tend to be very tight, and in the past, there were only a limited number of iterations () possible. But with real-time rendering, teams can now immediately check the results of their work, making it possible to produce higher-quality content more efficiently. The video industry has long made use of dedicated software and middleware, but the biggest advantage of Unreal Engine is that it provides an environment where high-quality visuals can be created in real time. In addition to Unreal Engine itself, we also provide an ecosystem that includes assets like Megascans (), allowing even small teams to produce high-quality content.</p></div></div><p><sup>Note 3: Iteration refers to a development cycle often used in software development and project management, where design development and testing are repeated in short intervalsNote 4: Megascans is a high-quality 3D asset scans library offered by Quixel. It features physically-based rendering with faithfully captured high-resolution details. These assets can be directly exported into Unreal Engine.</sup></p><div><div><p>In fields like video and anime production, Unreal Engine can dramatically streamline the traditional workflow. In fact, directors or supervisors can even build scenes directly in the editor without needing a storyboard. The ability for users to work in a pipeline that closely resembles real-world production has significantly improved trial-and-error speeds – something that many professionals have found especially valuable.</p></div></div><h2>Blending Photorealism and Anime: Engineers’ Innovative Approach to Unreal Engine</h2><div><div><p>This was your first time implementing Unreal Engine, but could you tell us about the technical challenges you faced and any creative solutions you came up with when using it for your livestream?</p></div></div><div><div><p>For this project, I oversaw development direction, and the biggest benefit we saw from adopting Unreal Engine were in the lighting quality and the overall production speed. The lighting design was completed in just a few weeks, which is an incredibly fast turnaround, and the final visual output far exceeded our expectations in terms of expression. Although, bringing together the photorealistic visuals that Unreal Engine excels at with the anime likeness of the hololive talents into the same space did require a great deal of technical ingenuity.&nbsp;</p></div></div><div><div><p>&nbsp;I was in charge of graphics, and the most challenging part for me was finding the right balance between a sense of realism and an anime-style aesthetic that would look visually appealing in a video. We put a lot of effort into blending the talents naturally into the environment, especially in how they interacted with the background, by implementing a “holo-color grading system” and creating a custom post-effect volume, separate from Unreal Engine’s standard post-effects. This setup allowed us to apply grading () separately to the talents and the background, and then apply master grading on top of both. It also allowed us to separately adjust the color grading of the talents and the background based on parameters like the time of day and weather changes as well as made it possible to naturally blend the stylized talents into a photorealistic background.&nbsp;</p></div></div><p><em><sup>Note 5: “Grading” refers to the process of adjusting colors, tones and contrast in a video to create a particular atmosphere or visual world.</sup></em></p><div><div><p>We also had to get creative with Lumen, Unreal Engine’s global illumination system. Using the default Lumen settings made the talents appear overly three-dimensional – almost like figurines – so we adjusted the lighting to tone down that sense of depth while still retaining the ambient lighting of the environment.</p></div></div><div><div><p>&nbsp;I contributed to the project mainly by modifying the engine including developing existing character shaders and implementing post-effects. One of the key challenges was figuring out how to faithfully translate our existing character expressions into Unreal Engine. That simply wouldn’t have been possible without making modifications to the engine. Because Unreal Engine’s G-buffer couldn’t fully accommodate the range of expressions we needed, we implemented a few workarounds including quantizing and compressing the data, and extending the system to allow constant buffers to be added per material.<p>In environments with such a large number of lights active at once, the standard lighting caused the cel-shaded appearance talents to break up a bit. To address this, we implemented a custom lighting system specially for those talents and thanks to this, we were able to maintain consistent anime-style shading even in scenes with more than 50 real-time light sources.</p><p>We also addressed translucency by rendering the relevant materials in a separate buffer using multipass rendering, which allowed us to have deferred lighting for translucent materials.&nbsp;</p></p></div></div><p><sup>: <em>G-buffers store geometric information about objects in a scene, such as normal vectors, albedo (surface color) and depth information. By using a G-buffer, rendering performance can be improved, and more complex lighting calculations can be performed efficiently.&nbsp;</em></sup></p><p>▼Expressions that use over 50 light sources in real time</p><div><div><p>I am in charge of talent-related development, focusing on ways to introduce outlines into talent expressions.</p></div></div><div><div><p>There are two ways of doing this, either with push or post outlines, and we have to make detailed adjustments so that we can switch between the two as necessary depending on how close the camera is. During the live performance, as a production changes from evening to night as time elapses, we adjusted the color grading in real time using Sequencer () for each time period, so that the talents’ movements appear natural. For normal vectors, we originally used SDF textures to control the shading, but this didn’t suit environments with multiple lights and some appearances ended up being less than ideal, so instead of using SDF textures (), we switched to controlling it with vertex normal, which produced far more beautiful shadows.&nbsp;</p></div></div><p><em><sup>Note 7: Sequencer is an editor for creating cut scenes. A series of scenes can be created by putting cameras and characters onto a timeline.</sup></em></p><p><sup><em>Note 8: A type of texture for 3D shapes that uses SDFs (Signed Distance Fields), which can make shape outlines look very smooth by saving the space between an object’s position and a shape, and stronger textures for objects that enlarge/shrink and animations</em>.</sup></p><div><div><p>SDF textures are effective when there is one source of light, but when multiple light sources hit an object, we get bands that we don’t want. For example, when there is an overlap of SDF textures from light that hits from both the left and right, unnatural 2D bands appear creating an unintentional anime likeness. Therefore, we reverted back to a method of editing vertex normals like those used in fighting video games where anime-like characters appear, which we thought made sense for environments with multiple light sources.&nbsp;</p></div></div><div><div><p>Also, similar to live music concerts, there were camera exposure issues, where talents are too bright when a spotlight, like a real camera at a concert, hits them, or too dark when the exposure is adjusted.&nbsp;</p></div></div><div><div><p>In order to address such issues, we introduced a system that corrects exposure for each camera using a MIDI controller. We needed to calculate outline extrusion to align with camera angles, because live cameras use 200mm or 300mm telephoto lens settings.&nbsp;</p></div></div><div><div><p>We use the Live Link plugin () with our cameras, but this time, we also used the FreeD protocol ( to link our Panasonic camera remotes so we could control the virtual cameras to make it feel like an actual live broadcast, and thanks to our staff, some of whom have knowledge in the TV field, we were able to create a workflow not too dissimilar to that of one. Certain staff members commented that although we are using Unreal Engine, what we are doing is no different from filming in reality, so it turned out to be a good example of combining technology from staff who originally worked in TV and Unreal Engine.&nbsp;</p></div></div><p><em><sup>Note 9: An Unreal Engine plugin for streaming data to Unreal Engine from external software and devices in real time. It can provide a basis for sharing information in real time such as animation data and camera movements.&nbsp;</sup></em></p><p><em><sup>Note 10: A communication standard for directly transmitting camera tracking information (pan, tilt, zoom, etc.) to a control system in order to coordinate camera movements with computer graphics content.</sup></em></p><div><div><p>We feel that there is an extremely high capacity to accommodate livestream concepts and talents’ wishes since using Unreal Engine to continue to develop our content. Original concepts, such as weather changes and the elapsing of time, can be implemented with an exceptionally high level of quality, and it has contributed to improved realism and elevated our livestreams, through such lighting improvements and by being able to have interactive expressions such as water bouncing off of the bodies of talents, or rain streaming down faces.&nbsp;</p></div></div><div><div><p>That’s great. It seems like there are various ways to create anime likenesses from a system optimized for photorealism. What measures did you have in place to optimize frame rate and lag, also vitally important for livestreams?</p></div></div><div><div><p>Our biggest issue was using over 50 light sources once activating ray tracing . First of all, we thoroughly managed channels such as light that did or did not cast a shadow or those that cast light. Also, we used and optimized Variable Rate Shading (VRS) , as a high amount of translucent light puts a lot of stress on the system. Originally, we also wanted to use VRS as it’s adaptive, but we set a constant unit due to time restrictions. In spite of this, we still got 2 to 3 times more speed.We greatly customized DMX (Note 13) light shaft calculations and implemented measures such as reducing the number of steps for ray marching (Note 14), and also optimized packaging. By minimizing the dependency, packaging that has usually taken about an hour was done in roughly 5-10 mins – iteration speed improvements were hugely important from a development efficiency standpoint.&nbsp;</p></div></div><p><em><sup>Note 11: A kind of technology that generates real images by simulating the volume, angle, refraction, and reflection of the rays of light via a computer. This is used to accurately represent the reflection, refraction and shadowing of light.</sup></em></p><p><em><sup>Note 12: Variable Rate Shading (VRS) is a type of rendering technology for improving performance and picture quality in game development that allows for pixel shading rates (frequency of processing one pixel) to be actively adjusted to reduce the load when processing images. </sup></em></p><p><em><sup>Note 13: DMX (Digital Multiplex) is a digital transmission network that is used to control stage lighting and effects for concerts and events, from the more simple to complicated ones.&nbsp;</sup></em></p><p><em><sup>Note 14: A kind of rendering technique in the ray tracing field.</sup></em></p><div><div><p>We were doing a lot of customization within Unreal Engine, such as compressing values to 4 bit to preserve the G-buffer, and packing multiple parameters into single channels. Also, to handle translucent areas, we increased deferred rendering (deferred shading) , and increased speed making use of stencil masking. We are also steadily optimizing how we deal with divergent branches in the Shader.</p></div></div><div><div><p>CPU optimization was also important. In particular, we greatly improved the performance of the editor while in use by introducing C++ to DMX plugin blueprint processes. In the future, we would like to continue this optimization to make both real-time editing using the editor and use in livestreams possible together. In terms of motion capture, we are using COVER’s original system, and we also developed an original system for integrating data from Vicon () to Motion Builder , and then to Unreal Engine.&nbsp;</p></div></div><div><div><p>We could also use Live Link for motion capture as well, but we needed a multi-engine approach because we were developing with both Unreal Engine and Unity (). As well as this, we are developing our own system that takes into account redundancy support and transmission load distribution, so that issues never occur during livestreams.</p></div></div><p><em><sup>Note 15: A type of 3D computer graphics rendering technique where complex light can be effectively expressed in real time. Note 16: The name of a motion capture system<p>Note 17: The name of a real-time 3D character animation software</p>Note 18: The game engine developed by Unity Technologies</sup></em></p><div><div><p>Very interesting. Thank you very much. I’m extremely interested in this example of actual camera operators and lighting staff directly controlling Unreal Engine rather than engineers.</p></div></div><div><div><p>Traditionally, we have needed to allow time to transfer what we designed using the lighting simulation software to the game engine, but this time, we had lighting experts set fixtures () and patch () these designs directly into Unreal Engine. This new trend of having professional lighting staff use the game engine felt really revolutionary.&nbsp;</p></div></div><p><em><sup>(Note 19) Lighting equipment that use and are controlled by DMX protocols</sup></em><em><sup>(Note 20) Making partial revision and changes</sup></em></p><h2>The Importance of a Multitude of Skills and a Support Network: Strengths and Prospects of Development Using Unreal Engine</h2><div><div><p>What advantages and/or costs have you felt there have been by introducing Unreal Engine and how has this affected the organization?</p></div></div><div><div><p>The speed from production to implementation has been so helpful! Unreal Engine is full of high quality features meaning that even non-engineers can help in the development process, so it took just weeks after we had the concept of the concert to finish the stage prototype.</p></div></div><div><div><p>We were able to take a balanced approach over a 7-8 month period for this concert production, by recompiling the engine where necessary while taking full advantage of Unreal Engine’s existing functions, such as the DMX plugin. Unreal Engine is super flexible, being able to handle a variety of different situations, so high quality content can be made in quick time for short-term projects without recompiling the engine, and for large-scale projects, one of the advantages is that our own expressions can be tracked by modifying the source code.<p>There was a mix of staff on this production from a wide range of industries, including gaming, TV, and mobile, and everyone’s skills seemed to fit the project well. However, what kinds of skillsets do Epic Games expect people to have and are required when using Unreal Engine?</p></p></div></div><div><div><p>&nbsp;I deal with a lot of different customers in technical support, but regardless of the industry you are in, it is extremely important to be fully across the concept of real-time rendering. We are coming up with a number of ways to draw frames in such short 16-millisecond amounts of time for the gaming industry, so understanding what can be done under real-time restrictions is so crucial when using Unreal Engine. Sometimes people used to more conventional forms of offline rendering suddenly start using Unreal Engine and are surprised by how restricted they feel. Having gaming industry graphics professionals such as yourself (Okugawa) is such a huge asset to COVER.&nbsp;&nbsp;<p>Oh, and one more thing: when working on livestreams, it is necessary to have multiple instances of Unreal Engine and have them in sync. Within Unreal Engine, there is a range of tools from those on the network level to synchronizing monitors, but you also need the knowledge to understand what is causing any problems that arise.&nbsp;&nbsp;&nbsp;</p></p></div></div><div><div><p>It definitely seems that you need a strong mix of people in an engineering team with a diverse set of skills from fields ranging from gaming to film for the production side of livestreams. Could you please tell us about what kind of support you, at Epic Games, provide Unreal Engine users?</p></div></div><div><div><p>We have our official Epic Pro Support subscription add-on, as well as our <a href=\"https://dev.epicgames.com/community/\">Epic Developer Community</a> forums where users help each other and exchange information. With Epic Pro Support, we provide support for those with highly technical issues and supply the latest information, as well as sharing know-how about profiling and optimization from what we actually do on-site. Our team mainly provides community support: we have recently started sharing Q&amp;As we have received through our pro support subscription with our Epic Developer Community, after removing any confidential information, giving a whole lot of developers access to quality information.</p></div></div><div><div><p>Behind this initiative to share Q&amp;A with the public are the strong intentions from our founder (of Epic Games), Tim Sweeney. Tim himself began his career in game development with just a single computer, and truly wishes to support all those, from indie games developers to students, aiming to be creators, at Epic Games are striving to create an environment where creators can grow and develop.&nbsp;</p></div></div><div><div><p>Also, there are passionate fans of Unreal Engine everywhere organizing their own independent community events, to which we at Epic Games actively try to attend. There are Unreal Engine enthusiasts not only in Tokyo, Osaka and Fukuoka, but in a lot of different places with a real energy to excite the community around them. We really value these fans and want to support them as best we can.&nbsp;</p></div></div><h2>Evolving Technology for a New Future:COVER and Epic Games Redefining Expressions and Entertainment with Unreal Engine</h2><div><div><p>Could you tell us what plans Epic Games has for the future and the direction the development of Unreal Engine is heading?</p></div></div><div><div><p>We are currently envisioning a dual-axis approach to the development of Unreal Engine at the moment: the vertical evolution of quality along with the horizontal expansion of the platform and its userbase. One of the big themes for Epic Games as a whole is the strengthening of our support for mobile platforms, and optimization so that we can provide high-quality content to a wider range of devices.&nbsp;&nbsp;<p>Also, our metaverse vision is also strategically important for us. Our global expansion through Fortnite is a great representation of this, with Unreal Engine at the heart of its technical infrastructure. We are focusing a lot on UGC (user-generated content) through our </p><a href=\"https://www.unrealengine.com/en-US/uses/uefn-unreal-editor-for-fortnite\">UEFN (Unreal Editor for Fortnite)</a>, and creating an environment where not only engineers but regular users can also produce their own content.</p></div></div><div><div><p>From a technological perspective, as well as improvements in graphics, we believe that movement and motion expressions are becoming an important topic. We (at Epic Games) have a comprehensive service where people can create a realistic human called MetaHuman, and we have recently added a function called MetaHuman Animator where, just by filming your own expressions using an iPhone, you can transfer these movements to MetaHuman. At the moment, we are focusing on developing the realism of digital humans, but we also plan to expand this so that this technology can be applied to various talents and other expressions. By further improving the real-time nature of it, we expect that even more natural and interactive expressions will be possible.<p>The use of the fully immersive display </p><a href=\"https://www.thesphere.com/\">Sphere</a> in Las Vegas has also received quite a lot of attention recently. In most cases, a video that has been previously rendered is projected onto a screen, but more and more cases of projections in real-time using Unreal Engine are emerging. For example, the spherical screen of Sphere was used at a live concert for the band Phish in the US, who are known for their improvisation, to project graphics in real-time using Unreal Engine to match their performance. A Fortnite event in New York also had multiple screens around Times Square that were being controlled simultaneously and linked to artists’ performances.&nbsp;</p></div></div><div><div><p>We hope to use Unreal Engine for more of these kinds of immersive experiences within Japan as well. <a href=\"https://www.unrealengine.com/en-US/news/unreal-fest-2025-is-coming-to-a-city-near-you\">Unreal Fest 2025 Tokyo</a> is planned for November 14 and 15 of this year in Takanawa, Tokyo, where all the latest use cases of Unreal Engine in various industries and information on the technology will be shared over the 2 days on an even bigger scale than last year, so we hope you can join us if you are interested.</p></div></div><div><div><p>What challenges would COVER like to undertake using Unreal Engine in the future?</p></div></div><div><div><p>We would like to further improve the quality of our livestreams, and for not only that, but for a range of other events as well. For instance, every summer, hololive holds a swimsuit event and with Unreal Engine, it may even be possible to have talents swimming underwater as an example. We also hope to increase our AR (augmented reality) content combining both real-life and animation together, and with how compatible it is with real-time lighting, we also have our sights on developing our on-location content and TV programs too.&nbsp;&nbsp;&nbsp;&nbsp;We are also considering content that features more interactivity with fans, taking advantage of its unique features as a game engine. At COVER, we are also developing a metaverse, so we hope to use Unreal Engine to develop content in the 3D space, and not just video streams.&nbsp;<p>We can see from this chat that a range of experts are now taking on the challenge of blending the seemingly contradictory elements of realism and animated expression. The similarities of both Epic Games and COVER, who are supporting creators through technological development as well as talents and their aspirations, opens up a whole new world of possibilities for VTuber entertainment.</p></p></div></div>","contentLength":26574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We Held Our Third All- Employee Meeting and Announced the COVER AWARD 2024 Winners","url":"https://coveredge.cover-corp.com/en/list/2275","date":1750989600,"author":"平松梨沙","guid":472,"unread":true,"content":"<p>On May 27, 2025, we held our third COVER Corporation All-Employee Meeting with over 700 employees in attendance, and included opening remarks from the directors, the announcement of the COVER AWARD 2024, a group photo session, and a casual networking reception. President and CEO Motoaki Tanigo reflected on the company’s journey so far, with the message, “Let’s not be afraid of change and evolve.”&nbsp;</p><p>The COVER AWARD 2024 honored individuals and projects from the previous year that exemplified the company’s values and delivered meaningful results.</p><p>Individual awards were presented to a member from the Sales Development Division (for securing national clients and proposing innovative promotions), a member from the Creative Production Division (for promoting Houshou Marine’s Beauty Witch model project and managing quality control for a new 3D model), and a member from the Product Development Division (for delivering live event merchandise on a tight schedule while maintaining a high-quality product). During the awards ceremony, each division head shared their thoughts on why the recipients were selected, followed by presentations from the winners themselves, reflecting on the projects they had worked on over the past year.&nbsp;</p><p>Two projects were selected for the Project Awards.&nbsp;The offline event “hololive GAMERS fes. Cho-Cho-Cho-Cho GAMERS” received their award for delivering an offline experience focused on gameplay.&nbsp;<p>The “Virtual Concert Development with Unreal Engine” project was awarded for bringing ReGLOSS 3D LIVE “Sakura Mirage” to life by evolving our technology through the effective use of existing assets and expertise gained in a Unity-based environment.</p></p><p>The event brought employees from different divisions together, deepening everyone’s mutual understanding and connections with each other, and providing inspiration for the year ahead.</p><p><strong>Note: All division affiliations are as of FY 2024.</strong></p>","contentLength":1939,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["COVER"]}