{"id":"CSXkJZ3MdFx","title":"Dev News","displayTitle":"Dev News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":49,"items":[{"title":"Is the Point Inside the Triangle?","url":"https://alexsyniakov.com/2025/03/22/is-the-point-inside-the-triangle/","date":1742672394,"author":"/u/innochenti","guid":600,"unread":true,"content":"<p>Today we’re tackling something that seems simple: how do you tell if a point is inside a triangle? I’ve got five things I want to do here:</p><p>This post will be long, with some detours and extra thoughts. But I think that’s the best way to get tricky stuff. A story keeps it interesting and helps you remember the important bits. So, this won’t just be words – it’ll be a journey with logic and aha moments.</p><p><strong>Explain the basics simply</strong></p><p>We’ll look at the building blocks and steps to figure out if a point’s in a triangle. Lots of people write about this, but I think they miss stuff or make it too quick. I’ll do it my way.</p><p>I’ll stick to the ideas that help solve this one question.</p><p>Even something small like this takes more understanding than you’d guess. I want to show that.</p><p>This is from a book I’m writing. I’m testing if it works for readers.</p><p>Just as the world is made up of atoms and molecules, at the heart of any 3D program lies the triangle. So, let’s begin our story with the triangle and explore why it’s so crucial for 3D graphics. </p><p>Let’s turn to the definition from Wikipedia:<em>A triangle is a figure consisting of three line segments, each of whose endpoints are connected. This forms a polygon with three sides and three angles.</em></p><p>Sounds terribly boring, doesn’t it? But don’t let that fool you – triangles are amazing. They’re behind your favorite 3D editor, video game, or big movie. Everything 3D on your screen? It’s all triangles doing the heavy lifting, turned into pixels in the end (they call that rasterization). </p><p>Here’s a heads-up: I might bend some geometry rules a bit. Serious math folks could get mad, but I think you can explain tricky stuff in a way that clicks faster than slogging through years of math classes.</p><p><em>So, let’s begin! We have the concept of a triangle: three points connected by three lines. Now, let’s break our molecule down into atoms.</em></p><h3>Digression 1: Why the Triangle?</h3><p>Time for a quick side trip (and there’ll be more, trust me) about why triangles are the go-to for 3D graphics, instead of something like a point or a circle.</p><p><strong>Triangles can fake any shape</strong>Even a circle! Pile up enough triangles, and you can make it look close enough to trick the eye.</p><p>So, why not pick something else? Your screen’s just a bunch of pixels – tiny dots. Smooth shapes like circles don’t fit that world easily. You’ve got to fake them with something chunkier, like triangles.</p><p><strong>Triangles are old friends</strong></p><p>The properties of triangles have been well understood since the days of Pythagoras, making them a reliable and mathematically solid choice.</p><p>Triangles make it easy to figure out things like light, shadows, and textures in graphics, way better than trickier shapes.</p><p>All the modern graphics gear is built to handle triangles fast and smooth.</p><p><em>Side trip done – let’s get back to the atoms of our triangle, the points!</em></p><p>A point marks a spot in space, plain and simple. It doesn’t have width, length, or thickness. But when we put it on paper, we make it a big dot anyway.</p><p>What can we do with points? In other words, what operations can we perform on points?</p><p>You can technically add their parts x, y, z. But does it make sense? Not at all. So, adding points doesn’t really do much.</p><p>Can we subtract points? Yes, and it gives us something new – a . You just take one point’s coordinates and subtract the other’s. Say you’ve got two points in 3D,  and . Subtract them, and you get a vector .</p><p>Let’s take a quick look at vectors before we get back to . A might seem like a at first – same numbers – but it’s not. It’s about and . Take two points,  and . Subtract  – , and you get a vector, , that shows the way from  to . And if you measure that vector’s size – its magnitude, , you’ll get the distance between  and .</p><p>If we draw the vector  on a coordinate system, starting at <img src=\"https://s0.wp.com/latex.php?latex=%280%2C+0%2C+0%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%280%2C+0%2C+0%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%280%2C+0%2C+0%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"(0, 0, 0) \"> and going to <img src=\"https://s0.wp.com/latex.php?latex=%28V_x%2C+V_y%2C+V_z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28V_x%2C+V_y%2C+V_z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28V_x%2C+V_y%2C+V_z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"(V_x, V_y, V_z) \">, we get a line that shows its direction.</p><p>Now, take point  and add vector  to it. What do you get? Point ! So, <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D+%5Cmathbf%7BA%7D+%2B+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D+%5Cmathbf%7BA%7D+%2B+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D+%5Cmathbf%7BA%7D+%2B+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\mathbf{B} = \\mathbf{A} + \\vec{V} \"> it’s like the vector moves you from  to .</p><p>If we take point  and subtract vector  from it, we end up back at point . So, <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D+%5Cmathbf%7BB%7D+-+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D+%5Cmathbf%7BB%7D+-+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D+%5Cmathbf%7BB%7D+-+%5Cvec%7BV%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\mathbf{A} = \\mathbf{B} - \\vec{V} \"> That’s how you can shift a point using a vector.</p><p>Now, if you flip the vector’s sign – make it negative – you get the same , just pointing the other way. For example, <img src=\"https://s0.wp.com/latex.php?latex=%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28V_x%2C+V_y%2C+V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28V_x%2C+V_y%2C+V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28V_x%2C+V_y%2C+V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\overline{\\mathbf{V}} = (V_x, V_y, V_z)\">  turns into <img src=\"https://s0.wp.com/latex.php?latex=-%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28-V_x%2C+-V_y%2C+-V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=-%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28-V_x%2C+-V_y%2C+-V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=-%5Coverline%7B%5Cmathbf%7BV%7D%7D+%3D+%28-V_x%2C+-V_y%2C+-V_z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"-\\overline{\\mathbf{V}} = (-V_x, -V_y, -V_z)\">.</p><p>So, what’s possible with ? We can them or them. No big shock – the result is always another vector. But how do we picture it?</p><p>To show the between two , think of it as adding one to the opposite of the other: <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+%2B+%28-%5Cvec%7B%5Cmathbf%7BV%7D%7D%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+%2B+%28-%5Cvec%7B%5Cmathbf%7BV%7D%7D%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+%2B+%28-%5Cvec%7B%5Cmathbf%7BV%7D%7D%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{U}} + (-\\vec{\\mathbf{V}}) \">. It’s still a parallelogram, just with <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{V}} \"> flipped around.</p><p>Start with vector <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{U}} \"> at the origin. Then take <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{V}} \">, reverse it to make <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{-V}} \">, and put it at the end of <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{U}} \">. Draw the parallelogram using <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{U}} \"> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7B-V%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{-V}} \"> as sides. The diagonal from the origin? That’s <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+-+%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+-+%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BU%7D%7D+-+%5Cvec%7B%5Cmathbf%7BV%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{U}} - \\vec{\\mathbf{V}} \">.</p><h3>A Simple Trick to Remember Operations Between Points and Vectors</h3><p>Want an easy way to handle points and vectors? Here’s a little trick: add a , w, to the usual x, y, z.</p><p>For a , it’s (x, y, z, 1) – the w is 1.For a , it’s (x, y, z, 0) – the w is 0.</p><p>With this, we can sort out all the operations between them. Let’s see how it works.</p><p><em>That fourth coordinate, w, has another big role – we’ll get to it later. For now, let’s just keep it in mind as something neat. Going forward, we’ll drop it to keep things simple.</em></p><h3>Digression 2: Point vs Vector classes</h3><p>In programming, we usually split things into two classes: and . But honestly, a lot of coders find that annoying. They’d rather treat a point like a – just another starting from the . That way, they can do all the math stuff, like adding or subtracting, without worrying about what’s a point and what’s a vector.</p><p>Say you’ve got two points,  and , and you want the angle between them from the . You could write it like this:</p><div><pre title=\"\">Point O(0, 0, 0);\nVector vA = A-O;\nVector vB = B-O;\ndouble angle = std::acos(dot(vA, vB) / (length(vA) * length(vB)));\n</pre></div><p>But if you just treat  and  as radius vectors, it gets shorter:</p><div><pre title=\"\">double angle = std::acos(dot(A, B) / (length(A) * length(B)));\n</pre></div><p>The second way is quicker to write, sure. Still, for us, making sense of things matters more than saving a line or two. Keeping and separate keeps it clear what’s what.</p><h3>Calculating Distance / Vector Length</h3><p>Let’s tackle the distance between two points again. Take <img src=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A(x_1, y_1)\"> and <img src=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"B(x_2, y_2)\">. Picture a straight line between them. To measure it, think of the x-difference, <img src=\"https://s0.wp.com/latex.php?latex=x_2+-+x_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=x_2+-+x_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x_2+-+x_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"x_2 - x_1\">, and the y-difference, <img src=\"https://s0.wp.com/latex.php?latex=y_2+-+y_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=y_2+-+y_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=y_2+-+y_1&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"y_2 - y_1\">. Square those, add them up – <img src=\"https://s0.wp.com/latex.php?latex=%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"(x_2 - x_1)^2 + (y_2 - y_1)^2\">– and then take the square root. That’s your distance: <img src=\"https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"D = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\"></p><p>This works in 3D too. For points <img src=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A(x_1, y_1, z_1)\"> and <img src=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"B(x_2, y_2, z_2)\">, the distance is just: <img src=\"https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2+%2B+%28z_2+-+z_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2+%2B+%28z_2+-+z_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%3D+%5Csqrt%7B%28x_2+-+x_1%29%5E2+%2B+%28y_2+-+y_1%29%5E2+%2B+%28z_2+-+z_1%29%5E2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"D = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}\">. Same idea, one more step.</p><p>Got a <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D%28x%2C+y%2C+z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D%28x%2C+y%2C+z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BV%7D%7D%28x%2C+y%2C+z%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{V}}(x, y, z)\"> with a length <img src=\"https://s0.wp.com/latex.php?latex=D+%5Cneq+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D+%5Cneq+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%5Cneq+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"D \\neq 0\">? Divide its components <img src=\"https://s0.wp.com/latex.php?latex=x%2C+y%2C+z&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=x%2C+y%2C+z&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x%2C+y%2C+z&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"x, y, z\"> by <img src=\"https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"D\">. That gives you a unit , or a . We call this dividing step . These have a of 1 and point the way, making things simpler to figure out. Why are they big in computer graphics?</p><p>Normals show how light lands on surfaces – think brightness, shadows, or shiny reflections.</p><p>They help tricks like Phong shading make lighting look smooth and nice.</p><p><strong>Reflection and Refraction</strong>For stuff like water or glass, normals handle reflections and bending light.</p><p>In games, they tell objects how to hit or bounce off each other.</p><p>Normals fake extra detail in things like normal mapping, no extra shapes needed.</p><p><em>So, you’ve picked up the essentials – how to work with points and vectors, and even how to measure a vector’s length. Just a few more steps, and you’ll have this whole thing figured out!</em></p><p>What else can we do with vectors? There’s this thing called the  or – that’s super useful in geometry, physics, and graphics. Surprise: it gives you a (or ), not a . It’s all about checking how much two vectors go the same way. Can we figure that out? Sure, here’s how:</p><p>Put simply, the dot product here just looks at the  of <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{B}}\"> – how much of it <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}}\">.</p><p>When one vector’s y-part is zero, like <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%281%2C+0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%281%2C+0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%281%2C+0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}}(1, 0)\">, it’s easy to see, like we showed before. But what if the y-part isn’t zero? </p><p>Let’s try two vectors: <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D+%3D+%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D+%3D+%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D+%3D+%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}} = (2, 1)\"> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D+%3D+%283%2C+2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D+%3D+%283%2C+2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D+%3D+%283%2C+2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{B}} = (3, 2)\">. Let’s do something fun: rotate <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{B}}\"> around so it sits on the x-axis. That zeroes out its y-part, leaving it pointing flat across. We’ll swing <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}}\"> the same way too. Then, we just see how much <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}}\"> lines up with this new <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BB%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{B}}\">.</p><p>And <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%7D%7D%282%2C+1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A}}(2, 1)\"> gets the same twist, ending up as:<img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%3D+%282.21880%2C+-0.27735%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%3D+%282.21880%2C+-0.27735%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%3D+%282.21880%2C+-0.27735%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A'}} = (2.21880, -0.27735)\">.</p><p>Now it’s simple: we just find the dot product between a vector flat on the x-axis and one with both x and y parts.</p><p>Since rotating doesn’t change the dot product, we calculate:<img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%5Ccdot+%5Cvec%7B%5Cmathbf%7BB%27%7D%7D+%3D+%282.21880%2C+-0.27735%29+%5Ccdot+%283.60555%2C+0%29+%3D+2.21880+%5Ccdot+3.60555+%2B+%28-0.27735%29+%5Ccdot+0+%3D+8.00000&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%5Ccdot+%5Cvec%7B%5Cmathbf%7BB%27%7D%7D+%3D+%282.21880%2C+-0.27735%29+%5Ccdot+%283.60555%2C+0%29+%3D+2.21880+%5Ccdot+3.60555+%2B+%28-0.27735%29+%5Ccdot+0+%3D+8.00000&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7BA%27%7D%7D+%5Ccdot+%5Cvec%7B%5Cmathbf%7BB%27%7D%7D+%3D+%282.21880%2C+-0.27735%29+%5Ccdot+%283.60555%2C+0%29+%3D+2.21880+%5Ccdot+3.60555+%2B+%28-0.27735%29+%5Ccdot+0+%3D+8.00000&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{A'}} \\cdot \\vec{\\mathbf{B'}} = (2.21880, -0.27735) \\cdot (3.60555, 0) = 2.21880 \\cdot 3.60555 + (-0.27735) \\cdot 0 = 8.00000\"></p><p>So in the end, there is another way to calculate the dot product using the formula: </p><p>The cosine function appears here because it naturally describes alignment:</p><p>Picture this: you’re in a dark room, holding a laser pointer. You aim it at a wall, but not straight on – it’s at an angle. The light doesn’t just hit one spot; it stretches across the wall. How long that streak is depends on your aim:</p><ul><li>When the laser is  at the wall, the projection is maximal, meaning the  is . For unit vectors, the result is 1. </li><li>When the laser is , the projection , resulting in a . </li><li>When the laser is to the wall, there is no projection at all, and the  equals . </li><li>When the laser is pointing in the direction from the wall, the  is . For unit vectors, the result is -1.</li></ul><p><em>Just a heads-up: we’re deep in dot product territory. It’s a big topic, full of details, but super important. If you get the dot product – really get it, with all its twists – it’ll make everything later so much easier. It’s the heart of all the cool stuff coming up.</em></p><p>What if we need to determine the angle <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\theta \"> between two vectors? We can <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctheta+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\theta \"> using the inverse cosine function:</p><p>Think of arccos as a way to <strong>convert a similarity measurement into an actual angle</strong>:</p><p>This is extremely useful in applications like robotics, physics, 3D graphics, and AI, where finding angles between directions is crucial. The function always returns an angle in the range from <img src=\"https://s0.wp.com/latex.php?latex=0%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=0%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"0^\\circ \"> to <img src=\"https://s0.wp.com/latex.php?latex=180%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=180%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=180%5E%5Ccirc+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"180^\\circ \"> (or from <img src=\"https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"0 \"> to <img src=\"https://s0.wp.com/latex.php?latex=%5Cpi+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cpi+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cpi+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\pi \"> radians).</p><p>Now, let’s introduce the final operation you’ll frequently encounter: the , also known as the . Intuitively, this operation is simpler than the  because the result is something you can visualize – a new .</p><h4><em>At this stage, I’d like to ask for your patience – don’t worry about where this formula comes from. Just memorize it for now, and later you’ll “teleport” back to it from another section.</em></h4><p>The cross product can also be expressed as:<img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+%3D+%7C%5Cvec%7B%5Cmathbf%7Ba%7D%7D%7C+%5Ccdot+%7C%5Cvec%7B%5Cmathbf%7Bb%7D%7D%7C+%5Ccdot+%5Csin%28%5Ctheta%29+%5Ccdot+%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+%3D+%7C%5Cvec%7B%5Cmathbf%7Ba%7D%7D%7C+%5Ccdot+%7C%5Cvec%7B%5Cmathbf%7Bb%7D%7D%7C+%5Ccdot+%5Csin%28%5Ctheta%29+%5Ccdot+%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+%3D+%7C%5Cvec%7B%5Cmathbf%7Ba%7D%7D%7C+%5Ccdot+%7C%5Cvec%7B%5Cmathbf%7Bb%7D%7D%7C+%5Ccdot+%5Csin%28%5Ctheta%29+%5Ccdot+%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{c}} = |\\vec{\\mathbf{a}}| \\cdot |\\vec{\\mathbf{b}}| \\cdot \\sin(\\theta) \\cdot \\vec{\\mathbf{n}} \">where <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{n}} \"> is the  perpendicular to the plane containing <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{a}} \"> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{b}} \">.</p><p>The direction of <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{c}} \"> (and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bn%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{n}} \">) is determined by the : curl the fingers of your right hand from <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{a}} \"> to <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{b}} \">, and your thumb will point in the direction of <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bc%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{c}} \">.</p><p><em>Finally, let’s conclude our detour and return to our original task – determining whether a point is inside a triangle! At this stage, we have covered the basic geometric primitives: points, vectors, and operations on them. Now, with this knowledge, let’s finally solve our problem.</em></p><h3>Is the point contained within the triangle? Cross Product Method</h3><p>This method’s easy and makes sense, and it pulls together all the math we’ve gone over. For each edge of the triangle, we compute a cross product between the edge vector and the vector from the test point <img src=\"https://s0.wp.com/latex.php?latex=P+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P \"> to one of the triangle’s vertices. That gives us three vectors that act like normals.</p><p>Each normal <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\mathbf{N}_i\"> represents the orientation of the test point relative to a specific edge of the triangle.If <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> is inside the triangle, all three cross-product normals should point in the same direction. If <strong>all three dot products have the same sign</strong>, then the point lies inside the triangle. Otherwise, at least one normal vector points in the opposite direction, indicating that <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> is outside.</p><p><em>It is very important to note that this method works correctly only when the point lies in the . This condition is often neglected in computer graphics since most algorithms do not require it. However, if you are working with engineering software and require precise calculations, it makes sense to add a check to ensure that the point lies in the triangle’s plane. In practice, such a check is usually performed earlier in the pipeline, calling the point-in-triangle test, as part of collision detection, intersection tests, or geometric preprocessing.</em></p><div><pre title=\"\">bool isPointInsideTriangle(glm::dvec3 P, glm::dvec3 A, glm::dvec3 B, glm::dvec3 C, double epsilon) {\n    // Compute the normal of triangle ABC and check if it's degenerate\n    glm::dvec3 N = glm::cross(B - A, C - A);\n\n    // Compute normals for sub-triangles PBC, PCA, PAB\n    glm::dvec3 U = glm::cross(B - P, C - P);  // Normal of PBC\n    glm::dvec3 V = glm::cross(C - P, A - P);  // Normal of PCA\n    glm::dvec3 W = glm::cross(A - P, B - P);  // Normal of PAB\n\n    // Compute scalar products of sub-triangle normals with the triangle normal\n    double dot_u = glm::dot(U, N);\n    double dot_v = glm::dot(V, N);\n    double dot_w = glm::dot(W, N);\n\n    return sameSigns(dot_u, dot_v, dot_w, epsilon);\n}\n\n</pre></div><p>The  method can run into trouble because of small math mistakes. These pop up when numbers get rounded off during multiplying and subtracting. It’s a problem when vectors are almost the same, especially in really thin (or as we call it – ) triangles, and the answer might be off. Also, the final check – seeing if all normals point the same way – can mess up. If those normals shift and look almost at right angles because of these tiny errors, the test might call an inside point outside. helps by giving some extra space for those mistakes, making the checks better. You need to pick the right epsilon, though, or you might count outside points as inside or miss points that are barely inside.</p><h3>Digression 3. Vector Operations: Functions vs. Operators</h3><p>When handling vectors, libraries like offer functions like and for scalar and vector products, instead of using signs like * or %. This has some good points. For one, and make it clear what math you’re doing, so the code’s easier to understand. Take  – it shows right away it’s a dot product. But  could mean different things, like multiplying by a number, doing it part by part, or even a dot product, depending on the setup. Same with  – it’s obvious it’s a cross product, while % isn’t something everyone knows and might remind people of division leftovers instead.</p><p>Another thing: functions like and make your code safer and easier to keep up. Signs like * can do lots of jobs – like multiplying by a number or working with matrices – which can mix things up if you’re not sure what’s meant or if the pieces don’t fit right. But and are built just for vectors, so you’re less likely to mess up. For instance, if you try with the wrong mix – like a and a plain  – a good library will catch it before the code even runs. With *, it might quietly do something you didn’t want.</p><h3>Digression 4. Degenerate triangles</h3><p>triangles are bad – they don’t help anyone. They hold no real info. Either all their points sit on top of each other, or they line up straight. So, their is pretty much . To spot these, we can figure out the triangle’s area using the  of its edge vectors and see if it’s tiny – below a small limit, . This way, we catch triangles that barely exist, whether because the points overlap or fall in a line, and kick them out before moving on. Here’s some code that checks if a triangle’s degenerate by looking at its area:</p><div><pre title=\"\">bool isDegenerate(glm::dvec3 A, glm::dvec3 B, glm::dvec3 C, double area_epsilon) {\n    // Compute vectors along two edges of the triangle\n    glm::dvec3 AB = B - A;\n    glm::dvec3 AC = C - A;\n\n    // Compute the cross product to get the normal vector\n    glm::dvec3 N = glm::cross(AB, AC);\n\n    // Compute the area as half the magnitude of the normal\n    double area = 0.5 * glm::length(N);\n\n    // Return true if the area is below the threshold, indicating degeneracy\n    return area &lt; area_epsilon;\n}\n</pre></div><p>So, we need to put this check at the beginning of our isPointInsideTriangle method. <em>Next, we would like to address the problem mentioned at the beginning – we want to check if the point lies in the plane of the triangle.</em></p><p>A plane in 3D is like a big, flat sheet that goes on forever, set by a point and a normal vector. And guess what? The  – or , as we call it now – saves the day again, tying all this geometry into a simple idea. Imagine a plane with a <img src=\"https://s0.wp.com/latex.php?latex=P_0%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P_0%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P_0%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P_0(x_0, y_0, z_0)\"> and a <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BN%7D+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\mathbf{N} = (A, B, C)\">, which stands straight up from the plane, at right angles to anything on it.</p><div><pre title=\"\">struct Plane {\n    glm::dvec3 N;  // A,B,C\n    double D;\n};\n</pre></div><p>To check if a point <img src=\"https://s0.wp.com/latex.php?latex=P%28x%2C+y%2C+z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%28x%2C+y%2C+z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%28x%2C+y%2C+z%29+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P(x, y, z) \"> lies on the plane, we evaluate:</p><ul><li>If the result 0 (within a small tolerance), the point lies exactly on the plane.</li><li>If , the point is above the plane.</li><li>If , the point is below the plane.</li></ul><div><pre title=\"\">bool isOnPlane(Plane plane, glm::dvec3 P, double epsilon) {\n    return std::abs(glm::dot(plane.N, P) + plane.D) &lt;= epsilon;\n}\n</pre></div><p>To make a with a and a  vector, start with the plane equation. Plug in your point <img src=\"https://s0.wp.com/latex.php?latex=%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x_0%2C+y_0%2C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"(x_0, y_0, z_0)\"> to figure out the constant <img src=\"https://s0.wp.com/latex.php?latex=D+%3D+-%28A+x_0+%2B+B+y_0+%2B+C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D+%3D+-%28A+x_0+%2B+B+y_0+%2B+C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D+%3D+-%28A+x_0+%2B+B+y_0+%2B+C+z_0%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"D = -(A x_0 + B y_0 + C z_0)\">.</p><div><pre title=\"\">Plane createPlane(glm::dvec3 P, glm::dvec3 N) {\n    N = glm::normalize(N);\n    double D = -glm::dot(N, P);\n    return { N, D };\n}\n</pre></div><p>If you need to see if the point’s on the triangle’s plane, add this step after ruling out a bad triangle. </p><p><em>Think we’re done? Not even close. The method we went over is simple and pretty quick, but it’s got some weak spots, like we said before. Next, we’ll check out other ways to do this, and for that, we’ll need to dig even more into geometry basics.</em></p><h3>Digression 6. Can you compute a cross product in 2D?</h3><p>No, you cannot compute a  in a strictly two-dimensional space in the same way as in 3D – because the result is not a vector in 2D. However, if we extend the 2D vectors into 3D by assuming their <img src=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"z \">-coordinates are zero and use the standard 3D cross product formula:</p><p>and set the <img src=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"z \">-coordinates to zero, treating 2D vectors as lying in the <img src=\"https://s0.wp.com/latex.php?latex=xy+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=xy+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=xy+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"xy \">-plane of 3D space, we obtain:</p><p>Since the only nonzero component is the <img src=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=z+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"z \">-coordinate, we can define the 2D cross product as the scalar:</p><p>This scalar represents the signed area of the parallelogram formed by <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Ba%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{a}} \"> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7B%5Cmathbf%7Bb%7D%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{\\mathbf{b}} \">.</p><p>Thus, the signed area can be used to determine whether a 2D point is inside a 2D triangle. To do this, you need to compute the signed area of the triangle ABC and all the “sub-triangles” ABP, BCP, and CAP. If all the areas have the same sign, then the point lies inside the triangle.</p><div><pre title=\"\">bool isPointInsideTriangle(glm::dvec2 P, glm::dvec2 A, glm::dvec2 B, glm::dvec2 C, double area_epsilon) {\n    // Compute the area of the triangle\n    double triangleArea = area(A, B, C);\n\n    // Check if the triangle is degenerate (area too small)\n    if (std::abs(triangleArea) &lt; area_epsilon)\n        return false;\n\n    // Compute areas of sub-triangles formed by point P\n    double area1 = area(A, B, P);\n    double area2 = area(B, C, P);\n    double area3 = area(C, A, P);\n\n    // Check if all sub-triangle areas have the same orientation (same sign)\n    return sameSigns(triangleArea, area1, area2, area3, area_epsilon);\n}\n</pre></div><p>To get a triangle’s area using the 2D cross product, just take the absolute value of that cross product and cut it in half.</p><div><pre title=\"\">double area(glm::dvec2 A, glm::dvec2 B, glm::dvec2 C) {\n    glm::dvec2 vec1 = B - A;\n    glm::dvec2 vec2 = C - A;\n    double crossProductValue = vec1.x * vec2.y - vec1.y * vec2.x;\n    return 0.5 * crossProductValue;\n}\n</pre></div><p>Now, let’s finally return to where we started – the Triangle!</p><p>Phew! We took a detour, but now let’s return to the triangle. Consider a triangle defined by three vertices: <img src=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%28x_1%2C+y_1%2C+z_1%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A(x_1, y_1, z_1)\">, <img src=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%28x_2%2C+y_2%2C+z_2%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"B(x_2, y_2, z_2)\">, and <img src=\"https://s0.wp.com/latex.php?latex=C%28x_3%2C+y_3%2C+z_3%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C%28x_3%2C+y_3%2C+z_3%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=C%28x_3%2C+y_3%2C+z_3%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"C(x_3, y_3, z_3)\">. Some useful properties we can compute are:</p><p>Picture a different triangle with the same barycentric coordinates <img src=\"https://s0.wp.com/latex.php?latex=%28u%2C+v%2C+w%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28u%2C+v%2C+w%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28u%2C+v%2C+w%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"(u, v, w)\"> for a point. They still tell us how it sits relative to the corners.</p><p><em>How do barycentric coordinates solve our first question? It’s pretty easy. If all the weights u, v, w are between 0 and 1, the point is inside the triangle.</em></p><h3>Digression 7. Lines and Segments</h3><p>For example: take <img src=\"https://s0.wp.com/latex.php?latex=P%5Cleft%28%5Cfrac%7B1%7D%7B3%7D%2C+0%2C+0%5Cright%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%5Cleft%28%5Cfrac%7B1%7D%7B3%7D%2C+0%2C+0%5Cright%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%5Cleft%28%5Cfrac%7B1%7D%7B3%7D%2C+0%2C+0%5Cright%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\\left(\\frac{1}{3}, 0, 0\\right)\">. The weights come out as <img src=\"https://s0.wp.com/latex.php?latex=u+%3D+%5Cfrac%7B2%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=u+%3D+%5Cfrac%7B2%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=u+%3D+%5Cfrac%7B2%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"u = \\frac{2}{3}\"> and <img src=\"https://s0.wp.com/latex.php?latex=w+%3D+%5Cfrac%7B1%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=w+%3D+%5Cfrac%7B1%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=w+%3D+%5Cfrac%7B1%7D%7B3%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"w = \\frac{1}{3}\">. That means <img src=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A\"> is pulling harder here.</p><p>Start with the segment vector <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D+%3D+B+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D+%3D+B+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D+%3D+B+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AB} = B - A\">, and the vector from <img src=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A\"> to <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\">, which is <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%3D+P+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%3D+P+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%3D+P+-+A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AP} = P - A\">. Think of <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> as sitting somewhere along the line:</p><p>Now, use the dot product with <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AB}\"> on both sides:</p><p>Then, the other weight is just <img src=\"https://s0.wp.com/latex.php?latex=u+%3D+1+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=u+%3D+1+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=u+%3D+1+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"u = 1 - w\">.</p><h3>Barycentric Coordinates: Triangle</h3><p>Let’s figure out if point <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> is inside triangle <img src=\"https://s0.wp.com/latex.php?latex=ABC&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=ABC&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=ABC&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"ABC\">, but this time, let’s keep it really simple. Imagine you’re at <img src=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A\">, and you’ve got paths to <img src=\"https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"B\"> and <img src=\"https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"C\">. Any point inside the triangle is like a mix of steps toward <img src=\"https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"B\"> and <img src=\"https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"C\">.</p><p>So how do we find <img src=\"https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"v\"> and <img src=\"https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"w\">? Think of it as measuring how much <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> leans toward each path. We can “ask” <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AP}\"> how much it matches <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAB%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AB}\"> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAC%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAC%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAC%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AC}\"> using dot products – like checking shadows:</p><p>This gives us two clues:<img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAB%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAB%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAB%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAB%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAB%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAB%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAB%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAB%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAB%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AP} \\cdot \\vec{AB} = v (\\vec{AB} \\cdot \\vec{AB}) + w (\\vec{AC} \\cdot \\vec{AB})\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAC%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAC%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAC%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAC%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAC%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAC%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cvec%7BAP%7D+%5Ccdot+%5Cvec%7BAC%7D+%3D+v+%28%5Cvec%7BAB%7D+%5Ccdot+%5Cvec%7BAC%7D%29+%2B+w+%28%5Cvec%7BAC%7D+%5Ccdot+%5Cvec%7BAC%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\vec{AP} \\cdot \\vec{AC} = v (\\vec{AB} \\cdot \\vec{AC}) + w (\\vec{AC} \\cdot \\vec{AC})\"></p><p>To find the coordinates, we solve the two equations above, and the results are:</p><p>Then, the last piece is:<img src=\"https://s0.wp.com/latex.php?latex=u+%3D+1+-+v+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=u+%3D+1+-+v+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=u+%3D+1+-+v+-+w&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"u = 1 - v - w\"></p><p><em>Keep in mind that this method, like the previous one, assumes that the point lies in the .</em></p><div><pre title=\"\">bool isPointInTriangle(glm::dvec3 P, glm::dvec3 A, glm::dvec3 B, glm::dvec3 C, double epsilon) {\n    // Define vectors along edges\n    glm::dvec3 AB = B - A;\n    glm::dvec3 AC = C - A;\n    glm::dvec3 AP = P - A;\n\n    // Compute the normal of triangle ABC\n    glm::dvec3 N = glm::cross(AB, AC);\n\n    // Compute dot products needed for barycentric coordinates\n    double dotABAB = glm::dot(AB, AB);\n    double dotABAC = glm::dot(AB, AC);\n    double dotACAC = glm::dot(AC, AC);\n    double dotAPAB = glm::dot(AP, AB);\n    double dotAPAC = glm::dot(AP, AC);\n\n    // Compute denominator\n    double denom = dotABAB * dotACAC - dotABAC * dotABAC;\n    if (std::abs(denom) &lt; epsilon) \n        return false; // Denominator too small, unstable result\n    \n    double invDenom = 1.0 / denom;\n\n    // Compute barycentric coordinates (v, w, u)\n    double v = (dotACAC * dotAPAB - dotABAC * dotAPAC) * invDenom;\n    double w = (dotABAB * dotAPAC - dotABAC * dotAPAB) * invDenom;\n    double u = 1.0 - v - w;\n\n    // Check if the point lies inside the triangle within epsilon tolerance\n    return (u &gt;= -epsilon) &amp;&amp; (v &gt;= -epsilon) &amp;&amp; (w &gt;= -epsilon);\n}\n\n</pre></div><p><em>If you still think this is the end, it’s not! There is another method that is often used in more precise engineering applications. </em></p><h3>Projection onto a 2D Plane</h3><p>Every triangle in 3D lies flat on a plane, so we can shrink this problem down to 2D. Pick a plane – like the xy-plane, xz-plane, or yz-plane – and project everything onto it.</p><p>Here’s the trick: first, find the plane the triangle’s on. Check if the point’s near it, with a little room for error. If it is, we just need to see if it’s inside the triangle. Since the triangle’s flat in 3D, it’s a 2D question at heart. To keep it easy, we drop one coordinate from the triangle and the point, them down.</p><p><em>Note that checks have been added for  and whether the point lies on the .</em><em>Also, note that in the code below, the from the isDegenerate method could have been reused, but I left it as is for better clarity.</em></p><div><pre title=\"\">bool isPointInsideTriangle(glm::dvec3 P, glm::dvec3 A, glm::dvec3 B, glm::dvec3 C, double epsilon, double area_epsilon) {\n    // Check if the triangle is degenerate\n    if (isDegenerate(A, B, C, area_epsilon))\n        return false;  // Triangle is degenerate\n\n    // Compute the normal of triangle ABC\n    glm::dvec3 N = glm::cross(B - A, C - A);\n\n    // Create the plane from point A and normal, then check if P lies in it\n    Plane plane = createPlane(A, N);\n    if (!isOnPlane(plane, P, epsilon))\n        return false;  // Point is not in the plane\n\n    // Determine the dominant axis for projection\n    glm::dvec3 absN = glm::abs(N);\n    uint8_t dominantAxis = (absN.x &gt; absN.y &amp;&amp; absN.x &gt; absN.z) ? 0 :\n                       (absN.y &gt; absN.z ? 1 : 2);\n\n    // Lambda to drop the dominant axis\n    auto drop = [dominantAxis](glm::dvec3 v) -&gt; glm::dvec2 {\n        return (dominantAxis == 0) ? glm::dvec2(v.y, v.z) :  // Drop X\n               (dominantAxis == 1) ? glm::dvec2(v.x, v.z) :  // Drop Y\n                                     glm::dvec2(v.x, v.y);   // Drop Z\n    };\n\n    // Project the point onto the plane. Note that we checked it with epsilon before, now we want to lie exactly on the plane \n    glm::dvec3 projectedP = projectPointOntoPlane(P, plane);\n\n    // Test in 2D\n    return isPointInsideTriangle(drop(projectedP), drop(A), drop(B), drop(C), epsilon);\n}\n\n</pre></div><p>We drop the coordinate with the largest component in the normal, since it contributes the least to variations across the triangle. The normal shows how the triangle is tilted in space, and its points to the axis the plane is nearly perpendicular to. Removing that axis gives us a reliable 2D projection for checking point inclusion.</p><p>The final thing is to project our point <img src=\"https://s0.wp.com/latex.php?latex=P%28x_p%2C+y_p%2C+z_p%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%28x_p%2C+y_p%2C+z_p%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%28x_p%2C+y_p%2C+z_p%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P(x_p, y_p, z_p)\"> onto the plane, which follows the rule <img src=\"https://s0.wp.com/latex.php?latex=Ax+%2B+By+%2B+Cz+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Ax+%2B+By+%2B+Cz+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=Ax+%2B+By+%2B+Cz+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"Ax + By + Cz + D = 0\">. We do this by sliding <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> along the plane’s normal <img src=\"https://s0.wp.com/latex.php?latex=N+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=N+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=N+%3D+%28A%2C+B%2C+C%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"N = (A, B, C)\"> until it hits the plane at <img src=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P'\">. That’s written as:<img src=\"https://s0.wp.com/latex.php?latex=P%27+%3D+P+-+%5Clambda+N&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%27+%3D+P+-+%5Clambda+N&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%27+%3D+P+-+%5Clambda+N&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P' = P - \\lambda N\">, where <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"\\lambda\"> is how far we need to move.</p><p>Since <img src=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P'\"> is on the plane, it fits the equation:<img src=\"https://s0.wp.com/latex.php?latex=A+x%27+%2B+B+y%27+%2B+C+z%27+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A+x%27+%2B+B+y%27+%2B+C+z%27+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=A+x%27+%2B+B+y%27+%2B+C+z%27+%2B+D+%3D+0&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"A x' + B y' + C z' + D = 0\">.</p><p>Thus, <img src=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P%27&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P'\"> is the projection of <img src=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x\" alt=\"P\"> onto the plane.</p><div><pre title=\"\">glm::dvec3 projectPointOntoPlane(glm::dvec3 P, Plane plane) {\n    double lambda = (glm::dot(plane.N, P) + plane.D) / glm::dot(plane.N, plane.N);\n    return P - lambda * plane.N;\n}\n</pre></div><p>Usually, this is where you’d expect a conclusion for the article – something about which method is better, which one’s worse, and why we picked this over that other one. But since this is just a from a book, there’s no big wrap-up here. Instead, it rolls right into a more advanced, high-level problem down the road. So, for this post, the conclusion isn’t about crowning a winner among the methods. It’s more about how cool it is to break a problem down into tiny bricks and figure it out step by step with some neat math tools. What I really wanted to show here is how you can tackle the same task – checking if a point’s inside a triangle – in totally different ways. We’ve got the , the  trick, and that  move, each with its own strengths and trade-offs. They all get the job done, but they’re like different paths up the same hill. And by now, you’ve probably felt how much you need to know to code this stuff – , , nuances, and a touch of with to keep it all steady. It’s a lot of concepts to juggle! In the end, I hope messing around with this brought you some fun and maybe a little buzz from seeing how math and code can dance together.</p>","contentLength":25851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jhgr3t/is_the_point_inside_the_triangle/"},{"title":"Database Protocols Are Underwhelming - byroot","url":"https://byroot.github.io/performance/2025/03/21/database-protocols.html","date":1742670677,"author":"/u/Smooth-Zucchini4923","guid":599,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jhg3xz/database_protocols_are_underwhelming_byroot/"},{"title":"3 Traits of Good Test Suites","url":"https://elijahpotter.dev/articles/3_traits_of_good_test_suites","date":1742670185,"author":"/u/ChiliPepperHott","guid":598,"unread":true,"content":"<p>As ev­i­denced by my pre­vi­ous posts on <a href=\"https://elijahpotter.dev/articles/LLM_assisted_fuzzing\">LLM-Assisted Fuzzing</a>, I’ve been ded­i­cat­ing a lot of my men­tal band­width to main­tain­ing a low false-pos­i­tive rate while we im­prove <a href=\"https://github.com/automattic/harper\">Harper’s rule cov­er­age</a>. Part of that is through fuzzing and dog­food­ing, some can be through sta­tis­tics, but the ﬁrst lines of de­fense will con­tinue to be unit and in­te­gra­tion test­ing. This past week par­tic­u­larly, I’ve been read­ing up on how other <a href=\"https://github.com/rust-lang/rust-analyzer\">big lint­ing pro­grams</a> ap­proach this prob­lem.</p><h2>1. Test Features, Not Code</h2><p>I of­ten ask my­self: am I spend­ing more time think­ing or talk­ing about the thing, or am I spend­ing more time do­ing the thing? I’ve per­son­ally seen how pro­jects fall into de­cline be­cause their lead­ers are more in­ter­ested in plan­ning than do­ing.</p><p>In the con­text of soft­ware test­ing, this mantra is trans­formed intotest fea­tures, not code.” To my eye, good code is ﬂex­i­ble and self-ex­plana­tory. Tests that hook deeply into ap­pli­ca­tion or li­brary in­ter­nals make code less ﬂex­i­ble and harder to read.</p><p>I es­pe­cially like Alex Kladov’s heuris­tic for this: the neural net­work test.</p><blockquote><p>Can you re-use the test suite if your en­tire soft­ware is re­placed with an opaque neural net­work?” - Alex Kladov</p></blockquote><p>It’s not a ques­tion of whether a neural net­work would pass the test suite, only whether the test suite could work for it. If the an­swer is no, the tests are likely test­ing code, not fea­tures.</p><p>The speed at which you can build and run tests (unit, sta­tic, in­te­gra­tion, etc.) is a force-mul­ti­plier for every­thing else. You can val­i­date ideas sooner, run  faster, and get con­trib­u­tors on-boarded in less time.</p><p>Our goal to be fast at run­time dove­tails quite nicely into this, so it’s some­thing Harper al­ready does quite well. Moving for­ward, we need to make sure that we don’t rely on any kind of  in our tests, since that con­tin­ues to be the slow­est part of most Harper in­te­gra­tions.</p><p>We can sim­plify pro­grams like Harper down into a sin­gle func­tion which con­sumes text and re­turns a list of ob­served prob­lems.</p><pre><code>(text: ) &lt;Lint&gt;{\n    \n}\n</code></pre><p>Most test­ing we are in­ter­ested can be done with as­ser­tion func­tions that de­clare what qual­i­ties the out­put should have with a spe­ciﬁc in­put.</p><p>For ex­am­ple, we have a func­tion called as­sert_­sug­ges­tion_re­sult, which runs a gram­mat­i­cally in­cor­rect string through Harper, ap­plies the ﬁrst re­sult­ing sug­ges­tion and checks whether the edited string matches a given value.</p><pre><code>() {\n    (\n        ,\n        ThenThan::(),\n        ,\n    );\n}\n</code></pre><p>It’s also vi­tal that these as­ser­tions show good, read­able er­ror mes­sages when they fail. Each time I’ve im­proved their logs, I get un­prompted pos­i­tive feed­back from con­trib­u­tors.</p><p>Moving for­ward, I’d like to cre­ate a more di­verse ar­ray as­ser­tions like this, as well as bet­ter-doc­u­ment their use. A lot of the cur­rent back-and-forth for rule con­tri­bu­tions is re­lated this.</p><p>I hope some­one does. Good test suites are some­thing I’m con­tin­u­ing <a href=\"https://automattic.com/creed/never-stop-learning/\">to learn how to build</a>. I un­der­stand that a lot of what I’ve said here does­n’t ap­ply to other kinds of ap­pli­ca­tions or code­bases. If there’s nu­ance I’m not cov­er­ing here, let me know!</p>","contentLength":3448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jhfx0a/3_traits_of_good_test_suites/"},{"title":"KEDA, prometheus, scale from 0","url":"https://www.reddit.com/r/kubernetes/comments/1jhfpvn/keda_prometheus_scale_from_0/","date":1742669679,"author":"/u/Scheftza","guid":553,"unread":true,"content":"<p>I have a very simple spring-boot application, now what I want to achieve is to scale the app from 0 based on a prometheus metric, the problem is that when I try to trigger scaling up with an http request it doesn't work as there's no pod running. How can I overcome this?</p>","contentLength":271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Hosting Go Modules","url":"https://nicktrevino.com/self-hosting-go-modules.html","date":1742666558,"author":"/u/Hyperlisk","guid":601,"unread":true,"content":"<p>I've always been a proponent of self-hosting. Recently I've been doing more work with Go and wanted to host my own modules. I'm not super fond of hosting my code on GitHub because I feel like I don't totally own anything that I don't have root on. So I decided to look into how Go fetches modules and what it takes to host my own.</p><p>There are a few reasons why I like to self-host:</p><ul><li> Like I mentioned, if I don't have root on the box that is hosting my code then I don't feel in control.</li><li> Touching every part of the stack gives you insights into how everything fits together.</li><li> I really enjoy building software, so learning new things is always interesting to me.</li></ul><p>While I've used Go, I haven't had to think too much about how it actually works under-the-hood. Luckily, <a target=\"_blank\" href=\"https://pkg.go.dev/cmd/go#hdr-Remote_import_paths\">Go's documentation</a> is very clear about how it fetches modules, if a little dense. Here's the gist of it:</p><ul><li>: The  path in  can provide a VCS specifier (to point to a repo), or no specifier (to use HTTP).</li><li>: A  tag pulled from the module path (taken as an HTTP URL) points to a VCS or <a target=\"_blank\" href=\"https://go.dev/ref/mod#serving-from-proxy\"></a> can be used to point to a URL that implements the GOPROXY protocol for the module.</li><li>: If using a VCS, then it is dependent on the VCS. Using  and the GOPROXY protocol only requires a few static files to specify versions and module bundles.</li></ul><p>Once I realized that all Go needs is a directory structure containing , , , and  files to fetch modules, I realized it was easily doable. This is when the idea of  came to be.</p><div><div>A simple static file host is enough to serve Go modules reliably.</div></div><p><a href=\"https://code.nicktrevino.com/conr\" target=\"_blank\">CONR</a> (ode nly, o epo) is a tool I wrote originally in TypeScript/<a target=\"_blank\" href=\"https://google.github.io/zx/\">zx</a> and translated into Go with ChatGPT. The goal: point CONR at your existing Git repositories, and it automatically does the heavy lifting to package them for self-hosting. Here's a quick rundown:</p><ol><li>: Make sure your Go modules have valid semantic version tags (e.g. ).</li><li>: <code>go install code.nicktrevino.com/conr/cmd/conr@latest</code></li><li>: <code>conr --mod /path/to/myrepo --outDir ./dist --showSkips</code></li></ol><div><div>You can pass  multiple times if you have multiple modules you'd like to package up.</div></div><p>When done, the  folder will contain a structure that Go's module downloader loves:</p><code><pre>dist/&lt;module/path&gt;/@v/list\ndist/&lt;module/path&gt;/@v/v1.0.0.mod\ndist/&lt;module/path&gt;/@v/v1.0.0.zip\ndist/&lt;module/path&gt;/@v/v1.0.0.info\ndist/&lt;module/path&gt;/@latest\n</pre></code><p>These files map directly to the requests Go makes when you  a module. From here all that is left is to throw them on the internet at  with the proper  tag. For example, I host my modules at  on this subdomain and have my <code>&lt;meta name=\"go-import\" content=\"code.nicktrevino.com mod https://code.nicktrevino.com/go.mod\"&gt;</code> tag on both <a target=\"_blank\" href=\"https://code.nicktrevino.com/conr/\">CONR's home page</a>, and the <a target=\"_blank\" href=\"https://code.nicktrevino.com/\">root of the subdomain</a>. I placed it on both pages because Go will follow and double-check the meta tags.</p><h2>Hosting the Generated Files</h2><p>So on to hosting the files. I've mentioned Caddy before, but it has to be one of my favorite pieces of software recently. Caddy makes static file hosting a breeze with <a target=\"_blank\" href=\"https://caddyserver.com/docs/caddyfile/directives/file_server\">its  directive</a>. While I use Cloudflare Tunnel to expose my services, Caddy makes a lot of sense for self-hosting due to automatic TLS handling (if configured). Here's an example  that will serve everything from  which you can dump the output into:</p><code><pre>{\n  admin off\n  debug on\n}\n\n:443 {\n  log\n\n  handle /go.mod/* {\n    header Content-Type text/plain\n  }\n\n  file_server browse {\n    root /www\n  }\n}\n</pre></code><div><div>Note that I purposefully add the  parameter to , but you can certainly leave it off if it makes you more comfortable.</div></div><p>Once Caddy was running, I tested with fetching another module and  fetched from my domain successfully!</p><p>Finishing this project felt liberating. I discovered that Go modules are fundamentally a simple set of files served at predictable URLs, and was able to easily follow the Go documentation to build . In the end, combining that with a minimal Caddy setup gave me everything I needed for a robust, self-hosted module repository that I trust.</p><p>Hopefully I've inspired others to take the self-hosting journey for their own Go modules!</p>","contentLength":3969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jheiz6/selfhosting_go_modules/"},{"title":"California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers","url":"https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers","date":1742666138,"author":"thoughtpeddler","guid":175,"unread":true,"content":"<p align=\"center\"><em>Californians have the right to direct the company to delete their genetic data</em></p><p>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p><p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p><p><b>To Delete Genetic Data from 23andMe:</b></p><ol><li>Consumers can delete their account and personal information by taking the following steps:</li><li>Log into your 23andMe account on their website.&nbsp;</li><li>Go to the “Settings” section of your profile.</li><li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li><li>Click “View” next to “23andMe Data”</li><li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li><li>Scroll to the “Delete Data” section.&nbsp;</li><li>Click “Permanently Delete Data.”&nbsp;</li><li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li></ol><p><b>To Destroy Your 23andMe Test Sample:</b></p><p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p><p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p><p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p><p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p><p>To learn more about the CCPA, please visit&nbsp;<a href=\"https://oag.ca.gov/privacy/ccpa\">here</a>.&nbsp;&nbsp;</p>","contentLength":2952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43447421"},{"title":"Map Features in OpenStreetMap with Computer Vision","url":"https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/","date":1742665330,"author":"Brysonbw","guid":174,"unread":true,"content":"<p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href=\"https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai\"></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href=\"https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><p>Data is one of the most important components of any AI application, and <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><p>As a long-time user and contributor to <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href=\"https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai\"></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li> with <a href=\"https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai\"></a>, by <a href=\"https://www.ultralytics.com/?ref=blog.mozilla.ai\" rel=\"noreferrer\">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li>with <a href=\"https://ai.meta.com/sam2/?ref=blog.mozilla.ai\"></a>, by <a href=\"https://ai.meta.com/?ref=blog.mozilla.ai\" rel=\"noreferrer\">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href=\"https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai\"></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3>The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>For fetching OpenStreetMap data, we use:</p><p>Once all the polygons have been downloaded, you can choose a <a href=\"https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai\"></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href=\"https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai\"></a> from <a href=\"https://www.mapbox.com/?ref=blog.mozilla.ai\"></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href=\"https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format\"></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href=\"https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai\"></a> (or any other model supported by Ultralytics) is quite easy. </p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href=\"https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><p>After a point is selected, a bounding box is computed around it based on the argument.</p><p>All the existing elements of interest are downloaded from <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a>, and all the tiles are downloaded from <a href=\"https://www.mapbox.com/?ref=blog.mozilla.ai\"></a> and joined to create a stacked image.</p><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href=\"https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai\">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href=\"https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai\"></a>) to obtain a segmentation mask.</p><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as are displayed one by one for manual verification and filtering.</p><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href=\"https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai\"></a></p><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href=\"https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p>","contentLength":5530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43447335"},{"title":"Deep vs Shallow interfaces in Go","url":"https://tpaschalis.me/shallow-vs-deep-interfaces/","date":1742663402,"author":"/u/hyperTrashPanda","guid":551,"unread":true,"content":"<p>One of the core concepts explored in this book is the distinction between\n“deep” vs “shallow” modules (in the author’s terms a module is any kind of\nabstraction, separated into the user-facing interface and the underlying\nimplementation).</p><p>The author argues that “the best modules are those that provide powerful\nfunctionality yet have simple interfaces”. The argument is  about absolute\nsize, but rather the ratio of utility afforded by the abstraction compared to\nthe size of the abstraction itself, in other words, a cost/benefit tradeoff.</p><p>In our case, the main mechanism for composable abstractions in Go is the\n type, so let’s examine the concept through this lens.</p><p>To me, maybe the best example of a deep interface is .</p><div><div><pre><code></code></pre></div></div><p>It couldn’t possibly get  smaller than that, right? It’s simple enough\nthat you won’t ever need to look it up again. Searching the Go standard library, one will find\n<a href=\"https://cs.opensource.google/search?q=Read%5C(%5Cw%2B%5Cs%5C%5B%5C%5Dbyte%5C)&amp;ss=go%2Fgo\">numerous implementations</a>\nincluding reading from files, from network connections, compressors, ciphers\nand more.</p><p>This abstraction is both easy to understand and use; the docstring tells you\neverything you, as a user, need to know. The underlying implementation can be\nbuffered, may allow reading from streams or remote locations like an S3\nbucket. But crucially, consumers of this API don’t need to worry about \nreading happens — implementation can be deep and non-trivial,\nbut a user doesn’t have to care. Furthermore, it allows for very little\nambiguity when reasoning about what the code does.</p><p>All these properties are  important for core functionality that’s\nused frequently.</p><p>On the other hand, an example of a shallow interface I’ve used recently is from\nthe <a href=\"https://github.com/redis/go-redis\">redis-go</a> client.</p><p>I’ve trimmed it down for the purposes of this post, but you can see it \n<a href=\"https://github.com/redis/go-redis/blob/11efd6a01ebf1c3a0f8cc41d0a1d54c5afbae26f/commands.go#L160-L230\">here</a>\nin its entirety. It contains 45 methods  uses 19 other interfaces as\nextensions for a total of ~200 methods.</p><div><div><pre><code></code></pre></div></div><p>While the functionality provided by Redis is much larger than just ‘reading’,\neach of these methods has a much simpler implementation; they do exactly one\nthing, and they’re small enough you could possibly replicate them just by their\nname and arguments. The ratio of the functionality provided to the size of\nthe abstraction is very different than before.</p><div><div><pre><code></code></pre></div></div><p>This also shifts the responsibility of doing the right thing towards the\nuser, as they have to understand the nuances between individual methods.\nIn a code review, this makes it harder to reason about what happens at a\nglance.</p><div><div><pre><code></code></pre></div></div><p>So, is this another post criticizing other dev practices? Not really. As\nalways, things exist on a spectrum and these previous examples show the two\nextremes.</p><p>As a developer, it can often feel more natural to write shallower interfaces.\nSimilar ‘shallow’ examples (that are not strictly interfaces) are the\n<a href=\"https://pkg.go.dev/github.com/aws/aws-sdk-go/aws/session#Options\">aws-sdk-go’s session Options</a> or\n<a href=\"https://github.com/spf13/viper/blob/d319333b0ffd91a9681feaf2d202ce9332df8ecc/viper.go\">Viper’s</a>\npublic API. Why?</p><ul><li>It makes methods smaller and easier to test</li><li>It maps more closely to the mental map of the system itself</li><li>It takes less time to think up-front about how the user will consume it</li><li>Usually, it only affords a single implementation, maybe two, so it’s easier\nto imagine how it will be used</li></ul><p>In contrast,  offers additional advantages:</p><ul><li>Can be easily retrofitted to other use cases</li><li>Requires no state checks to use properly</li><li>Interfaces like this tend to remain stable over time, while a shallower version would often grow to accommodate more and more features</li><li>It allows for natural composability into other abstractions, like a  or a </li></ul><div><div><pre><code>type ReadCloser interface {\n\tReader\n\tCloser\n}\n</code></pre></div></div><p>This not a  comparison, as you will rarely write such core functionality\nas the Reader from scratch, and well, Cmdable is not an abstraction but rather\na driver covering the entirety of Redis operations.</p><p>But still, does that client API  five different methods for saving and\nshutting down? Does a user of the client need to deal with both running\ncommands and getting meta-information around the DB connection and runtime\nmetrics at the same time? Is each of the datatypes different enough to have\ntheir own interface? And do I as a reviewer, need to know beforehand whether\nthe code needs to ,  or ?</p><p>So, next time you design or review an abstraction, take a closer look.\n<a href=\"https://www.youtube.com/watch?v=XpqqjU7u5Yc\">How “deep” is your API</a>?\nIn what ways could you mold it into something simpler that hides complexity\nfrom the user and reduces cognitive load?</p><p>And that’s all for today! If you have any comments, remarks or ideas, feel free\nto reach out to me on <a href=\"https://bsky.app/profile/tpaschalis.me\">Bluesky</a>.</p><p>What are your favorite interfaces? Any specific one that you think touches the\nPlatonic ideal? Any that disgusts you beyond imagination and makes you wanna\nquit, move to the countryside and grow tomatoes? Let me know!</p>","contentLength":4646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1jhdapl/deep_vs_shallow_interfaces_in_go/"},{"title":"ReactOS 0.4.15 released","url":"https://reactos.org/project-news/reactos-0415-released/","date":1742661114,"author":"/u/LAUAR","guid":576,"unread":true,"content":"<p>We are pleased to announce the release of ReactOS 0.4.15!\nThis release offers Plug and Play fixes, audio fixes, memory management fixes, registry healing, improvements to accessories and system tools including Notepad, Paint, RAPPS, the Input Method Editor, and shell improvements.</p><p>We chose to release this version of ReactOS in honor of Eric Kohl’s <a href=\"https://git.reactos.org/?p=reactos.git;a=commit;h=108fcceee22216b169b448932bb3f86c81b9448e\">first commit</a> to the ReactOS code base, which dates back to 1999.\nEric Kohl is the oldest active contributor of the project, and this is his 26th ReactOS anniversary!\nHe has participated in the project as a developer since its beginnings.</p><p>This release is a culmination of the work of numerous contributors since 0.4.14 was branched in 2020.\nThis has been the largest release to date.\nThere are nearly 8 times more commits going into this release than in 0.4.14.\nWe are proud of the progress we have made, and are eager to continue with this growth.\nLet’s dive in and see what’s new.</p><p>Victor Perevertkin (Extravert-ir) has accomplished major rewrites to the Plug and Play Manager in the ReactOS kernel.\nWith these changes, ReactOS now has the ability to run more third party drivers and to boot from USB devices.\nThis also allows ReactOS to boot on chipsets with EHCI, OHCI, and UHCI controllers.\nThis work is a stepping stone to ReactOS being truly compatible with vendor drivers for the Windows operating system.</p><p>Johannes Anderwald (janderwald) solved an issue where the USB driver would enter an infinite loop when a USB device would not enter the ready state.\nFixing this infinite loop allowed ReactOS to boot on more hardware.</p><p>Thanks to the work of Oleg Dubinskiy, 0.4.15 features many audio improvements.\nOleg added support for more audio formats, looped playback of wave files, higher sample rates, and multiple output channels.\nIn addition, Victor Perevertkin imported the open source AC’97 driver from the Windows Driver Kit (WDK).\nThis enables sound out of the box in VirtualBox when the virtual machine is configured to use the ICH AC’97 Audio Controller and various motherboards from 2004 and earlier.</p><h2>Memory Manager and Cache Controller</h2><p>Section Objects have been refactored by Jérôme Gardou (zefklop) for better compatibility with Windows.\nThis fixes a long-standing bug preventing executables from starting in remote locations, such as network shares or virtual machine shared folders.</p><p>Because of improvements in the memory manager and cache controller, we can now import the open source Microsoft FAT filesystem driver from the Windows Driver Kit (WDK).\nThis FAT filesystem driver is a huge improvement from the old one, which was slower and less stable.\nAdditionally, external drives with FAT filesystem can now be properly ejected thanks to this new driver.</p><h2>Registry Healing and Caching</h2><p>Fundamental mechanisms of the system registry have been implemented courtesy of George Bișoc (GeoB99).\nThese mechanisms include registry healing, flushing, and caching.\nRegistry healing and flushing are both designed to improve system stability when faced with an unexpected power outage or a crash.\nHealing applies appropriate fixes to a corrupted registry and flushing periodically writes registry changes to the disk, ensuring registry changes persist even if the system is not cleanly shut down.\nCaching improves performance when accessing it.</p><p>George Bișoc also improved the Security Subsystem (Se) of the kernel.\nPrior to George’s work, kernel access checks always passed, allowing anyone to access any system object.\nNow, kernel access checks are fully functional and prevent unauthorized access to system objects.\nAs a result, the Windows kernel now works with the vast majority of modules from ReactOS.</p><p>Katayama Hirofumi MZ (katahiromz) has been hard at work making quality of life improvements, performance enhancements, and new features in system accessories such as the text tool in Paint and the “Now Printing” dialog in Notepad.\nKatayama Hirofumi MZ has also improved the Input Method Editor (or IME), which is a component that types characters not originally present on the connected input devices by using a sequence of characters.\nHis work improved CJK support and allows for the installation of custom IMEs for different locales. For example, Japanese ReactOS can now utilize MZ-IME Japanese input.\nWhindmar Saksit (whindsaks) made several bugfixes to improve the stability of RAPPS and Hermès Bélusca-Maïto (HBelusca) implemented a minimal view mode for RAPPS for uninstalling programs.</p><div itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\"><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/project-news/reactos-0415-released/ime.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/project-news/reactos-0415-released/ime.png\" alt=\"Japanese input with MZ-IME and CJK font\"></a><figcaption><p>Japanese input with MZ-IME and CJK font</p></figcaption></figure></div><p>In 0.4.15 the graphical shell was improved by several contributors.\nCarl Bialorucki (cbialorucki) added support for large taskbar icons.\nMark Jansen (learn-more) added native ZIP archive support.\nDoug Lyons (DougLyons) made several fixes to address an issue where incorrect icons were displayed in programs such as Microsoft Office 2000, Microsoft Visual Basic 6, and Hoyle Cards.\nKatayama Hirofumi MZ added support for the “Internet Browser” icon on the desktop.\nIn addition, Whindmar Saksit made several bugfixes to improve the stability of Shell32, a critical component of the ReactOS shell.</p><div itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\"><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/project-news/reactos-0415-released/large_taskbar2.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/project-news/reactos-0415-released/large_taskbar2.png\" alt=\"Large taskbar icons\"></a></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/project-news/reactos-0415-released/zip_extract_wizard2.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/project-news/reactos-0415-released/zip_extract_wizard2.png\" alt=\"ZIP extraction wizard\"></a></figure><figure itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\"><a href=\"https://reactos.org/img/project-news/reactos-0415-released/zip_virtual_folder2.png\" itemprop=\"contentUrl\"><img itemprop=\"thumbnail\" src=\"https://reactos.org/img/project-news/reactos-0415-released/zip_virtual_folder2.png\" alt=\"ZIP virtual folder\"></a></figure></div><p>In this release, we also decided to set the default visual style and wallpaper to Mizu.\nMore visual styles and wallpapers are available in RAPPS.</p><p>ReactOS is a community of people focused around the Windows ecosystem and free open source software.\nIt includes various projects to research and document Windows internals, use Windows software in a free enviornment, and make life easier for the broader Windows developer community.</p><p>0.4.15 was branched 6 months ago.\nSince then, many new and exciting features have been worked on in the master branch.\nUEFI support, symmetric multiprocessing (SMP), a new graphical installer, a new NTFS filesystem driver, power management, and newer application support are just a few features being worked on.\nWe are excited to share this journey with you as ReactOS improves and matures.</p><p>Resolved Jira issues: 1,319</p><p>Oldest Jira issue resolved: <a href=\"https://jira.reactos.org/browse/CORE-1091\">CORE-1091</a> from 19 December 2005</p>","contentLength":6007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jhcfbj/reactos_0415_released/"},{"title":"Facebook to stop targeting ads at UK woman after legal fight","url":"https://www.bbc.co.uk/news/articles/c1en1yjv4dpo","date":1742660568,"author":"dijksterhuis","guid":173,"unread":true,"content":"<p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>\"Organisations must respect people's choices about how their data is used,\" a spokesperson for the ICO said. \"This means giving users a clear way to opt out of their data being used in this way.\"</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, \"which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook\".</p><p>She said that she did not want to stop using Facebook, saying that it is \"filled with all of those connections and family and friends, and entire chapters of my life\".</p>","contentLength":1092,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43446821"},{"title":"Rust in 2025: Language interop and the extensible compiler","url":"https://smallcultfollowing.com/babysteps/blog/2025/03/18/lang-interop-extensibility/","date":1742655784,"author":"/u/kibwen","guid":605,"unread":true,"content":"<p>For many years, C has effectively been the “lingua franca” of the computing world. It’s pretty hard to combine code from two different programming languages in the same process–unless one of them is C. The same could theoretically be true for Rust, but in practice there are a number of obstacles that make that harder than it needs to be. Building out <strong>silky smooth language interop</strong> should be a core goal of helping Rust to target <a href=\"https://smallcultfollowing.com/babysteps/\n/blog/2025/03/10/rust-2025-intro/\">foundational applications</a>. I think the right way to do this is not by extending rustc with knowledge of other programming languages but rather by building on Rust’s core premise of being an extensible language. By investing in building out an  we can allow crate authors to create a plethora of ergonomic, efficient bridges between Rust and other languages.</p><h2>We’ll know we’ve succeeded when…</h2><p>When it comes to interop…</p><ul><li>It is easy to create a Rust crate that can be invoked from other languages and across multiple environments (desktop, Android, iOS, etc). Rust tooling covers the full story from writing the code to publishing your library.</li><li>It is easy to carve out parts of an existing codebase and replace them with Rust. It is  easy to integrate Rust into C/C++ codebases.</li></ul><p>When it comes to extensibility…</p><ul><li>Rust is host to wide variety of extensions ranging from custom lints and diagnostics (“clippy as a regular library”) to integration and interop (ORMs, languages) to static analysis and automated reasoning^[math].</li></ul><p>In my head, I divide language interop into two core use cases. The first is what I call  (LCD), where people would like to write one piece of code and then use it in a wide variety of environments. This might mean authoring a core SDK that can be invoked from many languages but it also covers writing a codebase that can be used from both Kotlin (Android) and Swift (iOS) or having a single piece of code usable for everything from servers to embedded systems. It might also be creating <a href=\"https://bytecodealliance.org/\">WebAssembly components</a> for use in browsers or on edge providers.</p><p>What distinguishes the LCD use-case is two things. First, it is primarily unidirectional—calls mostly go  the other language  Rust. Second, you don’t have to handle all of Rust. You really want to expose an API that is “simple enough” that it can be expressed reasonably idiomatically from many other languages. Examples of libraries supporting this use case today are <a href=\"https://mozilla.github.io/uniffi-rs/latest/\">uniffi</a> and <a href=\"https://rust-diplomat.github.io/book/\">diplomat</a>. This problem is not new, it’s the same basic use case that <a href=\"https://component-model.bytecodealliance.org/\">WebAssembly components</a> are targeting as well as old school things like <a href=\"https://en.wikipedia.org/wiki/Component_Object_Model\">COM</a> and <a href=\"https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture\">CORBA</a> (in my view, though, each of those solutions is a bit too narrow for what we need).</p><p>When you dig in, the requirements for LCD get a bit more complicated. You want to start with simple types, yes, but quickly get people asking for the ability to make the generated wrapper from a given language more idiomatic. And you want to focus on calls  Rust, but you also need to support callbacks. In fact, to really integrate with other systems, you need generic facilities for things like logs, metrics, and I/O that can be mapped in different ways. For example, in a mobile environment, you don’t necessarily want to use tokio to do an outgoing networking request. It is better to use the system libraries since they have special cases to account for the quirks of radio-based communication.</p><p>To really crack the LCD problem, you also have to solve a few other problems too:</p><ul><li>It needs to be easy to package up Rust code and upload it into the appropriate package managers for other languages. Think of a tool like <a href=\"https://github.com/PyO3/maturin\">maturin</a>, which lets you bundle up Rust binaries as Python packages.</li><li>For some use cases,  is a very important constraint. Optimizing for size right now is hard to start. What’s worse, your binary has to include code from the standard library, since we can’t expect to find it on the device—and even if we could, we couldn’t be sure it was ABI compatible with the one you built your code with.</li></ul><h2>Needed: the “serde” of language interop</h2><p>Obviously, there’s enough here to keep us going for a long time. I think the place to start is building out something akin to the “serde” of language interop: the <a href=\"https://crates.io/crates/serde\">serde</a> package itself just defines the core trait for serialization and a derive. All of the format-specific details are factored out into other crates defined by a variety of people.</p><p>I’d like to see a universal set of conventions for defining the “generic API” that your Rust code follows and then a tool that extracts these conventions and hands them off to a backend to do the actual language specific work. It’s not essential, but I think this core dispatching tool should live in the rust-lang org. All the language-specific details, on the other hand, would live in crates.io as crates that can be created by anyone.</p><h2>Lang interop: the “deep interop” use case</h2><p>The second use case is what I call the  problem. For this use case, people want to be able to go deep in a particular language. Often this is because their Rust program needs to invoke APIs implemented in that other language, but it can also be that they want to stub out some part of that other program and replace it with Rust. One common example that requires deep interop is embedded developers looking to invoke gnarly C/C++ header files supplied by vendors. Deep interop also arises when you have an older codebase, such as the Rust for Linux project attempting to integrate Rust into their kernel or companies looking to integrate Rust into their existing codebases, most commonly C++ or Java.</p><p>Some of the existing deep interop crates focus specifically on the use case of invoking APIs from the other language (e.g., <a href=\"https://github.com/rust-lang/rust-bindgen\">bindgen</a> and <a href=\"https://duchess-rs.github.io/duchess/\">duchess</a>) but most wind up supporting bidirectional interaction (e.g., <a href=\"https://pyo3.rs/v0.23.5/\">pyo3</a>, [npapi-rs][], and <a href=\"https://neon-rs.dev\">neon</a>). One interesting example is <a href=\"https://cxx.rs\">cxx</a>, which supports bidirectional Rust-C++ interop, but does so in a rather opinionated way, encouraging you to make use of a subset of C++’s features that can be readily mapped (in this way, it’s a bit of a hybrid of LCD and deep interop).</p><h2>Interop with all languages is important. C and C++ are just more so.</h2><p>I want to see smooth interop with all languages, but C and C++ are particularly important. This is because they have historically been the language of choice for foundational applications, and hence there is a lot of code that we need to integrate with. Integration with C today in Rust is, in my view, “ok” – most of what you need is there, but it’s not as nicely integrated into the compiler or as accessible as it should be. Integration with C++ is a huge problem. I’m happy to see the Foundation’s <a href=\"https://rustfoundation.org/interop-initiative/\">Rust-C++ Interoperability Initiative</a> as well a projects like Google’s <a href=\"https://github.com/google/crubit\">crubit</a> and of course the venerable <a href=\"https://github.com/dtolnay/cxx\">cxx</a>.</p><p>The traditional way to enable seamless interop with another language is to “bake it in” i.e., Kotlin has very smooth support for invoking Java code and Swift/Zig can natively build C and C++. I would prefer for Rust to take a different path, one I call . The idea is to enable interop via, effectively, supercharged procedural macros that can integrate with the compiler to supply type information, generate shims and glue code, and generally manage the details of making Rust “play nicely” with another language.</p><p>In some sense, this is the same thing we do today. All the crates I mentioned above leverage procedural macros and custom derives to do their job. But procedural macrods today are the “simplest thing that could possibly work”: tokens in, tokens out. Considering how simplistic they are, they’ve gotten us remarkably, but they also have distinct limitations. Error messages generated by the compiler are not expressed in terms of the macro input but rather the Rust code that gets generated, which can be really confusing; macros are not able to access type information or communicate information between macro invocations; macros cannot generate code on demand, as it is needed, which means that we spend time compiling code we might not need but also that we cannot integrate with monomorphization. And so forth.</p><p>I think we should integrate procedural macros more deeply into the compiler. I’d like macros that can inspect types, that can generate code in response to monomorphization, that can influence diagnostics and lints, and maybe even customize things like method dispatch rules. That will allow all people to author crates that provide awesome interop with all those languages, but it will also help people write crates for all kinds of other things. To get a sense for what I’m talking about, check out <a href=\"https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/\">F#’s type providers</a> and what they can do.</p><p>The challenge here will be figuring out how to keep the stabilization surface area as small as possible. Whenever possible I would look for ways to have macros communicate by generating ordinary Rust code, perhaps with some small tweaks. Imagine macros that generate things like a “virtual function”, that has an ordinary Rust signature but where the body for a particular instance is constructed by a callback into the procedural macro during monomorphization. And what format should that body take? Ideally, it’d just be Rust code, so as to avoid introducing any new surface area.</p><h2>Not needed: the Rust Evangelism Task Force</h2><p>So, it turns out I’m a big fan of Rust. And, I ain’t gonna lie, when I see a prominent project pick some other language, at least in a scenario where Rust would’ve done equally well, it makes me sad. And yet I also know that if  project were written in Rust, that would be . I mean, who would we steal good ideas from?</p><p>I really like the idea of focusing our attention on <em>making Rust work well with other languages</em>, not on convincing people Rust is better . The easier it is to add Rust to a project, the more people will try it – and if Rust is truly a better fit for them, they’ll use it more and more.</p><p>This post pitched out a north star where</p><ul><li>a single Rust library can be easily used across many languages and environments;</li><li>Rust code can easily call and be called by functions in other languages;</li><li>this is all implemented atop a rich procedural macro mechanism that lets plugins inspect type information, generate code on demand, and so forth.</li></ul><p>How do we get there? I think there’s some concrete next steps:</p><ul><li>Build out, adopt, or extend an easy system for producing “least common denominator” components that can be embedded in many contexts.</li><li>Look for ways to extend proc macro capabilities and explore what it would take to invoke them from other phases of the compiler besides just the very beginning.<ul><li>An aside: I also think we should extend rustc to support compiling proc macros to web-assembly and use that by default. That would allow for strong sandboxing and deterministic execution and also easier caching to support faster build times.</li></ul></li></ul>","contentLength":10819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1jhaf9a/rust_in_2025_language_interop_and_the_extensible/"},{"title":"PyTorch Internals: Ezyang's Blog","url":"https://blog.ezyang.com/2019/05/pytorch-internals/","date":1742654344,"author":"Anon84","guid":172,"unread":true,"content":"<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p><p>Hi everyone!  Today I want to talk about the internals of <a href=\"https://pytorch.org/\">PyTorch</a>.</p><p>This talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p><p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p><p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p><p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p><p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p><p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p><p>So, what do strides have to do with this picture?</p><p>Suppose that I want to access the element at position  in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p><p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p><p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p><p>Using advanced indexing support, I can just write  to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p><blockquote><p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p><p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href=\"https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak\">fixed this in Java 7u6</a>.</p></blockquote><p>A more interesting case is if I want to take the first column:</p><p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p><p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href=\"https://ezyang.github.io/stride-visualizer/index.html\">Stride Visualizer</a>.</p><p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p><p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p><p>One thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with ).</p><blockquote>\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote><p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call , two dispatches happen:</p><p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a  dispatch because these kernels may live in separate libraries (e.g.,  versus ), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p><p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on  is different from the code for .  It stands to reason you need separate kernels for each dtype.</p><p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p><p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p><p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p><ul><li>The , the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li><li>The , which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li><li>The , which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li></ul><p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in  the combination could make sense, and thus we support expressing it, at the very least.</p><p>There's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p><p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p><p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href=\"https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h\">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are  important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p><p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p><p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p><p>...and fill in the missing code that actually computes the gradients of your network:</p><p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p><ol><li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation\">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute ; then, the first thing we do in the blue part of the program is compute .   was computed from , so we compute .  Technically, these variables which we call  are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them  and mostly everyone knows what we mean.</li><li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the  operation is translated into a  operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced , the backward operation takes  as an input.</li></ol><p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p><p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls  in their PyTorch script.</p><blockquote>\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href=\"https://github.com/pytorch/pytorch/issues/13638\">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote><p>We also have to update our picture about dispatch:</p><p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p><p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p><p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p><p>Enough about concepts, let's look at some code.</p><p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href=\"https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure\">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p><ul><li>First,  contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li><li>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li><li>, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li><li>, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li></ul><p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in .</p><p>Let's see how this separation of code breaks down in practice:</p><p>When you call a function like , what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p><ol><li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li><li>We handle  dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li><li>We handle  dispatch (Type)</li><li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li></ol><p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p><p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like .   is the implementation of one such implementation.</p><p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class  to actually pull out C++ objects out of the Python  and ; we then call a  function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor .  On its way back, we rewrap the returned  back into a .</p><p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p><p>When we call the  method on the  class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named .  This happens to be because we have an implementation of  that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like .  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p><blockquote>\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing  dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote><p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p><p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p><p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p><p>We generally think of a kernel in PyTorch consisting of the following parts:</p><ol><li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li><li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li><li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li><li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li><li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).</li><li>Finally, you need to access the data and do the computation you wanted to do!</li></ol><p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p><p>To take advantage of all of the code generation which PyTorch brings, you need to write a  for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md\">README in native</a> is for more information about this format.</p><p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, , which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of , and most important data types in PyTorch have  implementations.</p><p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each  into a , which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g.,  tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the  metadata.</p><p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write  operators: , which operates on a preallocated output (this implements the  keyword argument), , which operates inplace, and , which is the plain old functional version of an operator.</p><p>Most of the time,  is the real workhorse, and  and  are just thin wrappers around ; but sometimes writing specialized implementations for each case are warranted.</p><p>To do dtype dispatch, you should use the  macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p><p>This macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want , but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href=\"https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62\">Dispatch.h</a> for how to select the correct one for your use-case.</p><p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p><p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p><ol><li>If you just want to get a value at some specific location, you should use .  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like , we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a , which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha:  defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li><li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the .   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li><li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The  class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like  then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li></ol><p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p><ol><li>It's written in C style, no (or very little) use of C++.</li><li>It's manually refcounted (with manual calls to  to decrease refcounts when you're done using tensors), and</li><li>It lives in  directory, which means that we are actually going to compile the file multiple times, but with different .</li></ol><p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p><p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, : it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p><p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"</p><ol><li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li><li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href=\"https://github.com/pytorch/ossci-job-dsl\">download and run the Docker images locally</a></li><li>The <a href=\"https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache\">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li><li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li></ol><p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p><p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled  mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged\">high priority</a> or look up issues specific to some module, e.g., <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22\">autograd</a> or find issues which we think are <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall\">small</a> (word of warning: we're sometimes wrong!)</p><p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I  merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>","contentLength":30265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445931"},{"title":"Show HN: FastOpenAPI – automated docs for many Python frameworks","url":"https://github.com/mr-fatalyst/fastopenapi","date":1742652630,"author":"mr_Fatalyst","guid":163,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445720"},{"title":"Landrun: Sandbox any Linux process using Landlock, no root or containers","url":"https://github.com/Zouuup/landrun","date":1742651819,"author":"Zoup","guid":171,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445662"},{"title":"[Media] Perfect!","url":"https://www.reddit.com/r/rust/comments/1jh8zmp/media_perfect/","date":1742651803,"author":"/u/wpg4665","guid":609,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Easy to use SQL library","url":"https://www.reddit.com/r/golang/comments/1jh8uad/easy_to_use_sql_library/","date":1742651333,"author":"/u/pimpaa","guid":549,"unread":true,"content":"<p>Hello everyone, I've been working on something for a couple weeks now and wanted to share!</p><p>sqlz (naming is hard, I know) is a wrapper around  similar to sqlx. Primary features are:</p><ul><li>Easy to use: it has a simple API for everyday use that can handle several use cases.</li><li>Lightweight: performance similar to sqlx and only one dependency (it uses <a href=\"https://github.com/georgysavva/scany\">scany</a> under the hood to do the scanning job, which is also well optimized).</li><li>Support for non-english utf-8 characters in named queries.</li></ul><p>It has only 3 main methods that can handle different use cases:</p><p><code>go Query(ctx context.Context, dst any, query string, args ...any) error QueryRow(ctx context.Context, dst any, query string, args ...any) error Exec(ctx context.Context, query string, args ...any) (sql.Result, error) </code></p><p>Both / execute the query and scan the returned rows into the  parameter, it works with default or named placeholders, passing a struct or map as  works as a named query.</p><p> is similar, passing array/slice as  will be treated as a batch insert.</p><p>Let me know what y'all think, specially the API, do you think it's better to have a separate method for named queries? Been using it and it feels nice but want some inputs.</p><p>Obvious question so I'll answer in advance. I've been using sqlx for some time, and while it's miles ahead the stdlib, I feel it's too verbose for simple cases, e.g. can't use 'IN' clause with named queries directly, gotta first use sqlx.Named, handle the error, then sqlx.In, handle the error, then Rebind, then Select.</p><p>An abstraction on top of sqlx is not fun, so I took the opportunity to learn and created sqlz, which turned out pretty good.</p><p>Performance-wise, it's close to sqlx, you can see <a href=\"https://github.com/rfberaldo/sqlz/blob/master/benchmarks/results.txt\">results here</a> (lower delta is better for sqlz).</p>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More with pipelines","url":"https://www.reddit.com/r/golang/comments/1jh8gda/more_with_pipelines/","date":1742650153,"author":"/u/kwargs_","guid":546,"unread":true,"content":"<p>Fun update to share about the gliter✨ library. I recently pushed new stage handlers that unlock a variety of new powerful async pipeline patterns. Some of these were ideas you guys had in the comments of my original post (thank you!).</p><p>Most notable additions are `Merge` and `Option` stages. </p><pre><code>gliter.NewPipeline(streamFromRedis). Stage( preprocessFeatures, // Normalize, extract relevant fields ). Stage( runFraudModel, // Model inference on transactions checkBusinessRules, // Non-ML heuristic checks ). Merge( aggregateResults, // Combine outputs from ML &amp; rules ). Stage( sendToAlertSystem, // Notify if fraud detected storeInDatabase, // Log for later analysis ). Run() </code></pre><p>The elevator pitch is: Gliter (Golang iter) enables Go developers to express complex and nuanced async patterns without having to manage races, deadlocks, channel states, and goroutine leaks.</p><p>Actually just the other day, I encountered a deadlock scenario in some old code at work. If gliter becomes a thing, I can imagine a world where these types of painful bugs are much less common because the surface area of gnarly async code is reduced, centralized, and solved. That's what excites me about the project.</p><p>Anyway, if you're curious to read more about why I built gliter and what I learned along the way, the original blog post from a few months back is here:</p><p>Consider using gliter in your next project, I think you'll really enjoy it!</p>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Would love to see people share their homelab/projects using k8s along with their diagram and techstack used","url":"https://www.reddit.com/r/kubernetes/comments/1jh8b4j/would_love_to_see_people_share_their/","date":1742649705,"author":"/u/mapoztofu","guid":555,"unread":true,"content":"<div><p>I am very new to the K8 world(at theory phase right now and using rancher desktop for some hands on)and want to see and learn from folks here what projects they are working on along with their diagram.</p><p>That would be very interesting and helpful. Thanks</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mapoztofu\"> /u/mapoztofu </a>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Programming Jobs Plunge 27.5% in Two Years","url":"https://developers.slashdot.org/story/25/03/22/1211202/us-programming-jobs-plunge-275-in-two-years?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1742648400,"author":"msmash","guid":214,"unread":true,"content":"Computer programming jobs in the US have declined by more than a quarter over the past two years, placing the profession among the 10 hardest-hit occupations of 420-plus jobs tracked by the Bureau of Labor Statistics and potentially signaling the first concrete evidence of artificial intelligence replacing workers. \n\nThe timing coincides with OpenAI's release of ChatGPT in late 2022. Anthropic researchers found people use AI to perform programming tasks more than those of any other job, though 57 percent of users employ AI to augment rather than automate work. \"Without getting hysterical, the unemployment jump for programming really does look at least partly like an early, visible labor market effect of AI,\" said Mark Muro of the Brookings Institution. \n\nWhile software developer positions have remained stable with only a 0.3 percent decline, programmers who perform more routine coding from specifications provided by others have seen their ranks diminish to levels not seen since 1980. Economists caution that high interest rates and post-pandemic tech industry contraction have also contributed to the decline in programming jobs, which typically pay $99,700 compared to $132,270 for developers.","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Great Introductory course","url":"https://www.reddit.com/r/kubernetes/comments/1jh78g5/great_introductory_course/","date":1742646144,"author":"/u/Far_Dimension_6413","guid":557,"unread":true,"content":"<p>I highly suggest their basic of kubernetes course and it is free, but rancher-desktop is pretty heavy to use on my system, since my system is not that heavy to take a load.</p>","contentLength":172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My first days with Rust from the perspective of an experienced C++ programmer (continued)","url":"https://www.reddit.com/r/rust/comments/1jh78e2/my_first_days_with_rust_from_the_perspective_of/","date":1742646138,"author":"/u/Rough-Island6775","guid":604,"unread":true,"content":"<p>Using AIs with questions such as how do I do this and that in Rust describing things that I know are there makes the transition smooth.</p><p>What first seemed like elaborate syntax makes perfect sense and probably as good as it can be.</p><p>I will read the Rust book and the reference to get formally educated but for now AI acts as a tutor answering things that it has seen plenty of times, noob questions.</p><p>The binary is larger, as expected, primarily (I think) due to the initial data structure is built in a function instead of hard-coded as a global.</p><p>Somewhat larger binary is expected and acceptable due to the built in safeties of Rust.</p><p>Without AI the learning curve is a bit steep and for a programming noob is probably off-putting. For an experienced C++ programmer is just: \"yeah, that's better\" and it keeps giving me a tiny smile every time that happens.</p><p>I begin to understand the cult like following Rust has because once a learning step in the curve is taken it feels like there is no going back.</p><p>I have a lot to learn, but for now, for my toy bare-metal application, I feel that this is the way forward.</p><p>p.s. I was pleasantly surprised by how extensive the core library is and that it works in [no_std] builds.</p>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Faults and Fault Tolerance in Distributed Systems","url":"https://newsletter.scalablethread.com/p/understanding-faults-and-fault-tolerance","date":1742645492,"author":"/u/scalablethread","guid":603,"unread":true,"content":"<p>Software applications rely on distributed systems for data storage, computation, and real-time processing. These systems spread workloads across multiple nodes (servers, databases, or services) to improve scalability and availability. However, distributed systems face a critical challenge: faults. Their inherent complexity makes them susceptible to various types of failures. Fault tolerance is the ability of a system to continue operating correctly despite faults in some of its components.</p><ul><li><ul><li><p>Example: A server in a cloud cluster loses network connectivity due to a faulty router.</p></li></ul></li><li><ul><li><p>Example: A microservice crashes after encountering an unhandled null pointer exception.</p></li></ul></li><li><ul><li><p>Example: A developer deploys a buggy update that corrupts a distributed cache.</p></li></ul></li><li><ul><li><p>Example: A database replica in Europe becomes unreachable due to an undersea cable cut.</p></li></ul></li><li><p><a href=\"https://newsletter.scalablethread.com/p/what-is-the-byzantine-generals-problem\" rel=\"\">sending conflicting or malicious information</a></p></li></ul><p>When faults occur in a distributed system, they can impact the overall system functionality and user experience.</p><ul><li><p><a href=\"https://newsletter.scalablethread.com/p/single-leader-multi-leader-and-leaderless\" rel=\"\">replication </a></p></li><li><p><a href=\"https://newsletter.scalablethread.com/i/146142578/partition-tolerance\" rel=\"\">network partitions</a></p></li></ul><p>This involves creating multiple copies of critical data or components across different nodes. If one replica fails, others can take over, ensuring continued availability and data durability.</p><ul><li><p><strong>Passive Replication (Standby):</strong></p></li></ul><p>This includes having mechanisms to detect when a component has failed.</p><ul></ul><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ❤️ like button.</em></p><p><em>If you think someone else will benefit from this, then please 🔁 share this post.</em></p>","contentLength":1455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jh71qn/understanding_faults_and_fault_tolerance_in/"},{"title":"Will linking a Go program \"manually\" lose any optimizations?","url":"https://www.reddit.com/r/golang/comments/1jh6w2o/will_linking_a_go_program_manually_lose_any/","date":1742644922,"author":"/u/Forumpy","guid":548,"unread":true,"content":"<p>Generally, if I have a Go program of e.g. 3 packages, and I build it in such a way that each package is individually built in isolation, and then linked manually afterwards, would the resulting binary lose any optimizations that would've been there had the program been built entirely using simply ?</p>","contentLength":299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing two new open source PebbleOS watches!","url":"https://ericmigi.com/blog/introducing-two-new-pebbleos-watches","date":1742642118,"author":"/u/B3_Kind_R3wind_","guid":577,"unread":true,"content":"<p>We’re excited to announce two new smartwatches that run open source PebbleOS and are compatible with thousands of your beloved Pebble apps. </p><ul><li> has an ultra crisp black and white display, polycarbonate frame, costs $149 and starts shipping in July.</li><li> has a larger 64-colour display, metal frame, costs $225 and starts shipping in December.</li></ul><p>Both are available in limited quantities, with worldwide shipping. Prices are in USD. Pre-ordering is the only way to get one - they will not be sold in stores. Pre-order today at <a href=\"http://store.repebble.com/\">store.rePebble.com</a>!</p><h2>Why are we making new Pebble-like smartwatches?</h2><p>Pretty simple - because we want one! No company has made a perfect smartwatch for people like <a href=\"https://www.youtube.com/watch?v=lKie-vgUGdI\">us</a>, so we’re going to make the exact smartwatch we want. Read the <a href=\"https://ericmigi.com/blog/why-were-bringing-pebble-back\">full story on my blog</a>, but it comes down to 5 key features:</p><ul><li>Simple and beautiful design</li></ul><p>No smartwatch on the market since Pebble offers this combination of features…until today!</p><p>I think you might recognize this one 😉 It’s almost exactly a Pebble 2, upgraded with modern chips and new tricks. Duo is short for ‘Do-over’.</p><p><em><strong>Similar to Pebble 2, it features</strong></em></p><ul><li>Ultra crisp 1.26” black and white e-paper display</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Lightweight polycarbonate frame in two colour options - White or Black</li><li>Water resistant (targeting IPX8)</li></ul><ul><li>30 day battery life (up from 7)</li><li>Linear resonance actuator (quieter and stronger than vibrating motor)</li><li>More reliable buttons (up to 30% longer lifetime in testing)</li><li>Barometer and compass sensors</li></ul><p>Since this watch will look and feel just like a Pebble 2, you can refamiliarize yourself with it via<a href=\"https://www.youtube.com/watch?v=KQh1b_srGM4\"> videos</a>, or<a href=\"https://www.starkinsider.com/2016/10/pebble-2-review-smartwatch-perfected.html?utm_source=chatgpt.com\"> reviews</a>. For people interested in hacking on PebbleOS firmware, we’re offering an optional JTAG connector. I recommend buying 2 units if you want to hack, just in case!</p><p>This is my dream watch. It’s everything Pebble Time 2 was going to be and more! </p><ul><li>64-colour 1.5” e-paper display. Same display as Pebble Time 2 - much more room for text and details (53% bigger and 88% more pixels)</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Metal frame and buttons (Black/White and likely a 3rd colour option as well)</li><li>30 day battery life (estimate)</li><li>Flat glass lens (less glare and reflections than Pebble Time family curved lens)</li><li>Water resistant (targeting IPX8)</li><li>Linear resonance actuator (vibrator)</li><li>Standard 22mm watch strap</li></ul><p>The industrial design is closely based on Pebble 2, which I really love. It’s slightly bigger to accommodate the larger display. Both the frame and buttons are made of metal (most likely CNC milled aluminum). More details, including final colour options, will be shared later this year. </p><p>Left: Core 2 Duo - Right: Core Time 2</p><table><tbody><tr></tr><tr></tr><tr></tr><tr><td>6-axis IMU, compass, barometer</td></tr><tr></tr><tr><td><em><strong>Linear resonance actuator (vibrator)</strong></em></td></tr><tr></tr><tr></tr><tr></tr><tr><td>Heart rate, step and sleep tracking</td></tr><tr></tr></tbody></table><p>Each watch runs open source&nbsp;<a href=\"https://github.com/pebble-dev/pebble-firmware\">PebbleOS</a>. This enables all the baseline Pebble features like receiving notifications, timeline, watchfaces, alarms, timers, calendar, music control, basic fitness tracking, etc. </p><p>The really fun part is that most of the existing 10,000+ PebbleOS watchfaces and apps will immediately work on these new watches, though some may try to access web services that no longer exist. Browse the full appstore on <a href=\"https://apps.rebble.io/en_US/watchfaces\">apps.rebble.io</a>.</p><p>Existing apps/faces will show up with a border on Core Time 2 until developers update them, since it has a larger display (200x228 vs 144x168 pixels). Read more about on the <a href=\"https://developer.rebble.io/developer.pebble.com/blog/2016/10/11/Emery-SDK-Beta/index.html\">old Pebble dev blog</a>.</p><p>We will publish a companion mobile app for Android and iOS. My friend and past Pebble colleague, Steve, recently joined us to lead this effort. He’s joining crc32, long-time <a href=\"https://github.com/pebble-dev/mobile-app/actions/runs/13609363513\">Cobble</a> developer, who has been working with me since last summer. We’ll also be working on an updated SDK for creating new PebbleOS watchfaces or apps.</p><p>These watches will be sold exclusively through <a href=\"http://store.repebble.com/\">store.rePebble.com</a>. Due to limited supply of display inventory, both watches will be manufactured in limited quantities. I highly recommend placing a pre-order - we will be manufacturing fewer watches than the number of people who have signed up on rePebble already! A pre-order secures your watch but gives you flexibility if you change your mind - you can get a refund at any time up until your watch ships. </p><p>Left: Core 2 Duo running PebbleOS - Right: Engineering samples</p><p>Schedule-wise,  is quite far along. We’ve already produced dozens of Core 2 Duo watches for testing and development. We’ve tested and confirmed our <a href=\"https://i.imgur.com/HwdTncx.mp4\">button improvements</a>. PebbleOS has been compiled for the new architecture and runs on the watch. All firmware development is open source - you can follow the fun on <a href=\"https://github.com/pebble-dev/pebble-firmware/commits/main/\">Github</a> and <a href=\"https://discord.gg/aRUAYFN\">Discord</a>. Our current schedule calls for shipments to begin in July. </p><p>How are we so far along, given that PebbleOS was only open-sourced in January? Two things helped: a) I took a monetary risk and began product development a bit earlier 😉,&nbsp;and b) we found a supplier who still had inventory of some Pebble 2 components. </p><p>Core Time 2 display lighting up! </p><p>For , we’ve finished component selection, initial industrial and mechanical design, and found sources for long lead time components. Now, we’re in the middle of creating the first prototypes. </p><p>The grand irony of hardware development is that software development is usually the slowest part of the project. Not this time! We’re extraordinarily thankful to Google for open sourcing PebbleOS, which gave us a massive boost.</p><p>We’ll share more details (like Core Time 2 frame colour options) later this year, as we get closer to mass production. Our current schedule shows shipments beginning in December. Follow along on this <a href=\"https://ericmigi.com/\">blog</a>, via <a href=\"https://repebble.com/signup\">email</a>, on <a href=\"https://bsky.app/profile/ericmigi.com\">Bluesky</a> or <a href=\"https://twitter.com/ericmigi\">Twitter</a>.</p><h3>Why were these specific specifications selected?</h3><p>Building a smartwatch is an exhausting (😂) exercise of constraint maximization. Think of it as linear algebra - we’re solving multiple equations with multiple unknown variables. The primary constraint is display selection. This choice governs the size and shape of the physical design, as well as being the component with the most power drain and biggest cost. Other variables are Bluetooth chip (governs: cost, software compatibility, engineering time), factory selection (cost, quality, speed, risk), sensors (power, cost, software), battery (size, battery life, cost) and more!</p><p>Despite what Pebble’s second Kickstarter branding proclaimed, solving this inherently requires compromise. It’s a fine line to walk and generally the data is not 100% known upfront, so it inevitably requires some degree of trusting your gut. </p><p>: we’re adding a touchscreen to Core Time 2. Why? Very specifically, I want to add the concept of ‘complications’ to watchfaces and widgets. Like on Apple Watch, these complications/widgets will show glanceable information like weather, next calendar event, step count, etc. The touch screen adds the ability to tap on the complication and directly open the associated app. This is much faster to use than opening an app via the button menu, and saves your quick launch (long-press on the buttons) for other apps. The touchscreen may be used for other interactions, like swiping to rapidly scroll down a list, but that will be lower priority. </p><p>Core Time 2 will be the first PebbleOS watch to ship with a touchscreen, but not the first that we designed! In 2015, Pebble designed it into a watch called Cutts (or C2) and did some (very) preliminary software development work to integrate a touchscreen into PebbleOS.</p><p>: I’m really excited that battery life will be increased from 7 days to 1 month! This is due to massive improvements in Bluetooth chip power efficiency over the last 10 years. </p><p>: we’re adding this primarily for potential use in apps that benefit from audio output, like a ChatGPT or other AI agent app. It can’t easily be used for making voice calls, since the watches will not support Bluetooth Classic (required for headset profiles), but theoretically someone could write a custom voice calling client (eg SIP or something).</p><p>: neither watch will support smartstraps. Sorry. Most people don’t even remember this feature even existed, which is kinda the answer to why it will not be supported. RIP.</p><h3>You shouldn’t get one if…</h3><p><strong>You need a perfectly polished smartwatch.</strong> This project is a labour of love rather than a startup trying to sell millions of watches. There may be some rough edges (literally). Things will get delayed. Some features will not be ready at launch. Things could break. Things could not last as long as you’d like. The only thing we can guarantee is that it will be awesome and a lot of fun! Every time you look down at your watch, you will smile 🙂</p><p><strong>You’re looking for a fitness or sports watch</strong>. That’s not what we’re making. From what we hear, Garmin watches are great for runners/cyclists/triathletes!   </p><p><strong>You’re comparing this to an Apple Watch</strong>. There is NO way for a 3rd party smartwatch to compete with Apple Watch. Apple restricts 3rd parties in major ways - read <a href=\"https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones\">my blog post</a> for more information. For example, 3rd party watches on iOS cannot send replies to notifications.   </p><p>These watches are not made for everyone. We want to be upfront with you about what to expect. </p><blockquote><p>Watch images above feature impeccably designed watchfaces from <a href=\"https://ttmm.is/pebble/\">TTMM</a>, including one of my all-time favourites - <a href=\"https://apps.rebble.io/en_US/application/57812aa56c21044501000ed5?query=ttmm&amp;section=watchfaces\">TTMMBRN</a>. Thank you Albert!</p></blockquote>","contentLength":9212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jh661j/introducing_two_new_open_source_pebbleos_watches/"},{"title":"Firefox: Mozilla is working on Progressive Web Apps (PWA) support","url":"https://www.ghacks.net/2025/03/17/firefox-mozilla-is-working-on-progressive-web-apps-pwa-support/","date":1742641708,"author":"/u/B3_Kind_R3wind_","guid":579,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jh62fx/firefox_mozilla_is_working_on_progressive_web/"},{"title":"How to create a github action to build Go binaries for Linux, Darwin and Windows in all archs?","url":"https://www.reddit.com/r/golang/comments/1jh4noo/how_to_create_a_github_action_to_build_go/","date":1742635591,"author":"/u/DarqOnReddit","guid":547,"unread":true,"content":"<p>I'm not sure if Darwin has other archs than x86_64. But Linux has at least amd64 and arm64 and so does Windows.</p><p>I never used Github actions and I have this Go repo where I'd like to provide prebuilt binaries for especially Windows users. It's a repo about live streaming and most run obs on Windows at home, so I'd like to make it as painless as possible for them to run my software.</p><p>I have googled but found nothing useful.</p><p>If you're using github and have a pure Go repo, how do you configure github actions to build binaries and turn those into downloadable releases?</p>","contentLength":566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Built a fun chat app on kubernetes (AWS EKS)!","url":"https://www.reddit.com/r/kubernetes/comments/1jh46ul/built_a_fun_chat_app_on_kubernetes_aws_eks/","date":1742633483,"author":"/u/Ammb305","guid":558,"unread":true,"content":"<p>Just finished a fun project: a MERN chat app on EKS, fully automated with Terraform &amp; GitLab CI/CD. Think \"chat roulette\" but for my sanity. 😅</p><ul><li> Terraform (S3 state, obvs)</li><li> Fancy VPC with all the subnets &amp; gateways.</li><li> EKS + Helm Charts (rollbacks ftw!)</li><li> GitLab, baby! (Docker, ECR, deploy!)</li><li> NLB + AWS LB Controller.</li><li> Not in this project yet </li></ul><p>I'm eager to learn from your experiences and insights! Thanks in advance for your feedback :)</p>","contentLength":430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ever wanted a “go back” button when debugging JavaScript in Chrome Developer Tools?","url":"https://youtu.be/ft9zQljooL8","date":1742633066,"author":"/u/Physical-Purple-5621","guid":602,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1jh43ng/ever_wanted_a_go_back_button_when_debugging/"},{"title":"When you deleted /lib on Linux while still connected via SSH (2022)","url":"https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/","date":1742628245,"author":"todsacerdoti","guid":170,"unread":true,"content":"<p>Let’s first not talk about why this can happen, but deleting , , or some other essential runtime files happens quite a lot (as you can see: <a href=\"https://unix.stackexchange.com/questions/704581/accidentally-deleted-lib-and-lib64-from-on-rhel\">here</a>, <a href=\"https://askubuntu.com/questions/47608/accidently-deleted-usr-lib-so\">here</a>, <a href=\"https://serverfault.com/questions/977095/accidentally-unlinked-usr-lib-libcrypt-a-is-there-a-way-to-recover-from-this\">here</a>,<a href=\"https://unix.stackexchange.com/questions/704581/accidentally-deleted-lib-and-lib64-from-on-rhel\"></a>and <a href=\"https://stackoverflow.com/questions/12249547/how-to-recover-after-deleting-the-symbolic-link-libc-so-6\">here</a>). In this post, I will only discuss what happens when you delete  on Linux and how to recover from that.</p><p>The easy solution for everything is to replace the missing files, but this can be difficult if  is deleted because we won’t have , which is needed to run any dynamic executable. When you deleted , all non-static executable (such as , , , will output): </p><div><pre title=\"\">No such file or directory\n</pre></div><p>You will also be unable to open any new connection using ssh, or open a new tmux window/pane if you are using tmux. So you can only rely on your current shell built in, and some static executables that you have on the system.</p><p>If you have a static  installed, then it can be your rescue. You can use  from  to download libraries from a clean system.  For your information: Debian has  installed by default, but the default is not the static version.</p><p>If you are worried that this kind of problem might happen to you in the future: Install the static version of the busybox binary, and confirm that it is the correct version.</p><p>I assume right now that you don’t have a static busybox, and you don’t even have any static executables (which is the situation in many cases, like in the default install of minimal Debian). My solution for this is to download a static busybox from another machine. </p><p>I also assume that you have bash installed (which is the default for most systems).  Bash has a lot of default built-ins that we can use.  There is a <a href=\"https://unix.stackexchange.com/posts/421403/revisions\">solution from here</a> that can be used to download a file using only built-in bash functions. Other <a href=\"https://unix.stackexchange.com/questions/83926/how-to-download-a-file-using-just-bash-and-nothing-else-no-curl-wget-perl-et/421403#421403\">solutions on this thread </a>rely on external command (such as ).  Please note that you need to set the environment  variable  to ; Otherwise, this script will incorrectly handle Unicode bytes.</p><p>Of course, we can’t  the destination file to be executable, so we need to overwrite an existing executable. If you have busybox installed (even if it is the non-static version), you can overwrite this file. At this point, you can start the rescue mission: for example, use  to download fresh  from another system.</p><p>Please note that busybox can’t function with a name that is not a busybox applet name. So if you overwrite for example, the  binary with , then it won’t work (it will say: ).  If you don’t have , I suggest overwriting , then you can use  to create a copy of  as  (which will be executable).</p><p>If you have a more advanced shell (e.g: zsh), <a href=\"https://zsh.sourceforge.io/Doc/Release/TCP-Function-System.html\">it has TCP modules already built in</a>. You can easily use  from another machine to send a file to the target machine. Now, let’s assume that you have a very basic shell, for example: . Most shell  (including dash), has  as built-in, and we can use this to construct binary files. </p><p>Most (all?) shell’s built-in   implementation supports  where  is 3 digit octal. First approach is to just convert , but this file is quite big (2 megabyte). Copy-pasting  large  commands is tedious and is error-prone. We need a small static binary that can help us.</p><p>This  trick will also work for other OS, if you can create a small binary for that OS.</p><h2>Creating a small ELF for Linux</h2><p>You can create a very tiny executable if you use assembly directly, but let’s try to do this using C, so it can be portable across different architectures. The smallest useful program that I can think of is just to copy from stdin to stdout, so we can prepare  on a machine:</p><p><code>cat busybox | nc -v -l -p 10000</code></p><p>and then we can do this from the borked machine:</p><p>The source code can be like this:</p><div><pre title=\"\">#include \"unistd.h\"\n\nint main()\n{\n        char x;\n        while (1) {\n                int c = read(0, &amp;x, 1);\n                if (c!=0) break;\n                c = write(1, &amp;x, 1);\n                if (c!=0) break;\n        }\n        return 0;\n}\n</pre></div><p>If we try to compile this with standard C library (on AMD64 machine), the result is 776KB. </p><p><code>gcc -Os -Wl,--build-id=none -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd</code></p><p>We get a 4536 bytes binary. Quite good. If we add , we can even get a smaller size. </p><p><code>gcc -Os -Wl,--build-id=none -z max-page-size=0x04 -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd</code></p><p>It is now 672 bytes. Small enough to transfer. We can convert this using Python.</p><div><pre title=\"\">import sys\n\nwith open(sys.argv[1], \"rb\") as f:\n    data = f.read()\n\nstart = 0\nwidth = 20\ntargetname = sys.argv[2]\nwhile True:\n    part = data[start:start+width]\n    if part=='':\n        break\n    a = ''.join(['\\\\'+(oct(ord(i)).zfill(3))[-3:] for i in part])\n    dest = '&gt;'\n    if start&gt;0:\n        dest += '&gt;'\n    dest += ' ' + targetname\n    print(\"printf '{}' {} \".format(a, dest))\n    start += width\n\n</pre></div><p>We can then copy paste this to our ssh session, then do the  redirection trick.</p><p>Of course, we can also write a complete program that makes the TCP connection instead of relying on bash redirection.</p><h2>I hope you will never need this knowledge</h2><p>This problem occurred to me a few days ago when I updated my <a href=\"https://solar.yohanes.mobi/\">Solar Powered Pi Zero</a>, and somehow  got deleted (not sure what caused it). This is not a very important machine, and I could have just reimaged the MicroSD card and be done with it, but I was curious if I could recover from the error. </p><p>I hope you will never have this error on your production/important machine, but if you have this problem in the future, I hope this post will help you recover from the situation.</p>","contentLength":5503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43444160"},{"title":"This Week in Plasma: 6.4 Improvements","url":"https://blogs.kde.org/2025/03/22/this-week-in-plasma-6.4-improvements/","date":1742625666,"author":"/u/gabriel_3","guid":575,"unread":true,"content":"<p>Welcome to a new issue of \"This Week in Plasma\"! Every week we cover the highlights of what's happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.</p><p>This week Plasma 6.4 continued to take shape, with a number of additional user-visible modernizations and improvements — in particular some nice progress on the topics of keyboard navigation, accessibility, and customizing apps' presentation in launcher menus.</p><p>Added keyboard navigation and interaction to the User Switcher widget's popup. (Marco Martin, <a href=\"https://bugs.kde.org/show_bug.cgi?id=501649\">link</a>)</p><p>KMenuEdit has received a UI modernization, including a simplification of the default toolbar contents, showing a well-curated hamburger menu by default, and adopting modern-style tool view tabs. (Oliver Beard, <a href=\"https://invent.kde.org/plasma/kmenuedit/-/merge_requests/42\">link 1</a> and <a href=\"https://invent.kde.org/plasma/kmenuedit/-/merge_requests/40\">link 2</a>)</p><p>When installing apps in Discover, progress feedback now appears right in the place you were just looking, rather than in the opposite corner of the window. (Nate Graham, <a href=\"https://bugs.kde.org/show_bug.cgi?id=475845\">link</a>)</p><p>Plasma's Calculator widget now announces the calculated result using the screen reader, if one is active. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/698\">link</a>)</p><p>Improved keyboard navigation for Plasma's Calculator widget; now navigation wraps around on the same row or column. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/696\">link</a>)</p><p>In System Monitor's table views, you can now press + to select everything, as you would expect. (Arjen Hiemstra, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499112\">link</a>)</p><p>After editing a page in System Monitor, the app now scrolls to show a newly-added row if it would otherwise appear out of the visible area. (Arjen Hiemstra, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499052\">link</a>)</p><p>KWin's \"Dim Screen for Administrator Mode\" plugin is now on by default, helping to focus attention on authentication prompts so you're less likely to miss them or lose them, especially with a cluttered desktop full of stuff. (Vlad Zahorodnii, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7368\">link</a>)</p><p>OSDs are now consistently referred to as \"OSDs\" in all the places you can disable them. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2897\">link 1</a> and <a href=\"https://invent.kde.org/plasma/plasma-pa/-/merge_requests/331\">link 2</a>)</p><p>System Monitor now displays something more user-friendly when asked to report the battery charge level of a device whose battery technology it can't determine. (David Redondo, <a href=\"https://invent.kde.org/frameworks/solid/-/merge_requests/204\">link</a>)</p><p>Narrowed the range of data that the Plasma clipboard accepts to be more in line with what it did in Plasma 6.2 and earlier. This prevents Plasma form crashing when copying certain types of content in certain apps. (Fushan Wen, <a href=\"https://bugs.kde.org/show_bug.cgi?id=501623\">link</a>)</p><p>Triggering the system bell a zillion times in rapid succession (e.g. by holding down  in an XWayland-using GTK app; don't do that) no longer freezes the entire system. (Nicolas Fella, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500916\">link</a>)</p><p>Fixed a bug in the Weather Report widget that caused it to fail to save its settings (size, position, even its existence) under certain circumstances when placed on the desktop, rather than a panel. (Marco Martin, <a href=\"https://bugs.kde.org/show_bug.cgi?id=501066\">link</a>)</p><p>The User Switcher widget now lays out text for the username more sensibly, neither getting too big with huge horizontal panels, nor overflowing with not-huge vertical panels. (Marco Martin and Nate Graham, <a href=\"https://bugs.kde.org/show_bug.cgi?id=356603\">link 1</a> and <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/712\">link 2</a>)</p><p>Fixed a regression in the new spatial quick tiling code that made global actions to instantly quarter-tile the active window not always work correctly the first time they were invoked. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=501731\">link</a>)</p><p>Fixed a regression that broke pasting numbers into Plasma's Calculator widget. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/713\">link</a>)</p><p>Fixed a regression that caused the icons of Folder View popups in list mode to be displayed with too much transparency. (Nate Graham, <a href=\"https://bugs.kde.org/show_bug.cgi?id=501797\">link</a>)</p><p>Fixed a bug that caused KMenuEdit to be unable to save changes you made to systemwide-installed Flatpak apps that had previously been customized using the properties dialog. (Oliver Beard, <a href=\"https://bugs.kde.org/show_bug.cgi?id=394476\">link</a>)</p><p>Fixed several minor issues affecting the Media Frame widget, including flickering on image change and not remembering customized popup sizes when living in a panel. (Marco Martin, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/709\">link</a>)</p><p>Non-full-screen screen recordings taken in Spectacle now have significantly higher visual quality when using a fractional screen scale factor. (Vlad Zahorodnii and Noah Davis, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7322\">link 1</a>, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7310\">link 2</a>, <a href=\"https://invent.kde.org/libraries/plasma-wayland-protocols/-/merge_requests/99\">link 3</a>, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7326\">link 4</a>, <a href=\"https://invent.kde.org/graphics/spectacle/-/merge_requests/445\">link 5</a>)</p><p>KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.</p><p>You can help KDE by becoming an active community member and <a href=\"https://community.kde.org/Get_Involved\">getting involved</a> somehow. Each contributor makes a huge difference in KDE — you are not a number or a cog in a machine!</p><p>You don’t have to be a programmer, either. Many other opportunities exist:</p><p>You can also help us by <a href=\"https://kde.org/donate\">making a donation!</a> Any monetary contribution — however small — will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.</p><p>Enter your email address to follow this blog and receive notifications of new posts by email.</p>","contentLength":4702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jh2gww/this_week_in_plasma_64_improvements/"},{"title":"Fastrace: A Modern Approach to Distributed Tracing in Rust","url":"https://www.reddit.com/r/rust/comments/1jh2fzg/fastrace_a_modern_approach_to_distributed_tracing/","date":1742625553,"author":"/u/RealisticBorder8992","guid":608,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/RealisticBorder8992\"> /u/RealisticBorder8992 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping RKE2","url":"https://www.reddit.com/r/kubernetes/comments/1jh0xi7/bootstrapping_rke2/","date":1742619361,"author":"/u/Due_Leave6941","guid":556,"unread":true,"content":"<p>For people using RKE2 in production. How are you bootstrapping your RKE2 upstream local cluster (where rancher management is installed)? </p><p>We've been looking into CAPI and Terraform. Also considering Kairos though it seems to working with k3s.</p>","contentLength":241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GoAvatar – Generate Unique Identicons in Go (Highly Customizable!)","url":"https://www.reddit.com/r/golang/comments/1jh0ro7/goavatar_generate_unique_identicons_in_go_highly/","date":1742618748,"author":"/u/__muhammadsaim","guid":550,"unread":true,"content":"<p>I recently built <a href=\"https://github.com/MuhammadSaim/goavatar\">GoAvatar</a> – a lightweight Go package that generates unique, symmetric identicons based on input strings (like usernames, emails, or any text). It’s perfect for user avatars, profile placeholders, and visual identity generation!</p><ul><li>Deterministic Identicons – Same input = same avatar every time</li><li>Symmetric &amp; Unique Designs – Visually appealing, mirror-like patterns</li><li>Highly Customizable – Set size, colors, and more!</li><li>Fast &amp; Lightweight – Minimal dependencies for quick avatar generation</li></ul><p>import ( \"github.com/MuhammadSaim/goavatar\" \"image/png\" \"os\" \"image/color\" )</p><p>func main() { // Generate the avatar with a custom foreground and background color options := goavatar.Options{ Width: 128, // Set custom image width (default is 256) Height: 128, // Set custom image height (default is 256) BgColor: color.RGBA{170, 120, 10, 255}, // Change background color (default is light gray) FgColor: color.RGBA{255, 255, 255, 255}, // Change foreground color (default is extracted from hash) } avatar := goavatar.Make(\"EchoFrost7\", options)</p><pre><code>// Generates an avatar with a brownish background and white foreground, saving it as avatar.png file, err := os.Create(\"avatar.png\") if err != nil { panic(err) } defer file.Close() png.Encode(file, avatar) </code></pre><ul><li>Size: Adjust width and height as needed</li><li>Colors: Set custom foreground and background colors</li><li>Hashing Algorithm: Modify how identicons are generated</li></ul><p>Would love to hear your feedback or ideas for improvements!</p>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scallop – A Language for Neurosymbolic Programming","url":"https://www.scallop-lang.org/","date":1742618708,"author":"andsoitis","guid":169,"unread":true,"content":"<img src=\"https://www.scallop-lang.org/img/icon-solver.png\"><p>\n                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and\n                  differentiable modes of reasoning.\n                  These modes are configurable to suit the needs of different AI applications.\n                </p>","contentLength":275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43443640"},{"title":"Why do strings have to be valid UTF-8?","url":"https://www.reddit.com/r/rust/comments/1jgxh3y/why_do_strings_have_to_be_valid_utf8/","date":1742607799,"author":"/u/SaltyMaybe7887","guid":606,"unread":true,"content":"<p>fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; { let mut file = std::fs::File::open(\"number\")?; let mut buf = [0_u8; 128]; let bytes_read = file.read(&amp;mut buf)?;</p><pre><code>let contents = &amp;buf[..bytes_read]; let contents_str = std::str::from_utf8(contents)?; let number = contents_str.parse::&lt;i128&gt;()?; println!(\"{}\", number); Ok(()) </code></pre><p>Why is it necessary to convert the slice of bytes to an ? When I run , it will validate that  is valid UTF-8. But to parse this string into an integer, I only care that each byte in the slice is in the ASCII range for digits as it will fail otherwise. It seems like the  adds unnecessary overhead. Is there a way I can avoid having to validate UTF-8 for a string in a situation like this?</p><p> I probably should have mentioned that the file is a cache file I write to. That means it doesn’t need to be human-readable. I decided to represent the number in little endian. It should probably be more efficient than encoding to / decoding from UTF-8. Here is my updated code to parse the file:</p><p>fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; { const NUM_BYTES: usize = 2;</p><pre><code>let mut file = std::fs::File::open(\"number\")?; let mut buf = [0_u8; NUM_BYTES]; let bytes_read = file.read(&amp;mut buf)?; if bytes_read &gt;= NUM_BYTES { let number = u16::from_le_bytes(buf); println!(\"{}\", number); } Ok(()) </code></pre><p>If you want to write to the file, you would do something like , so it’s the other way around.</p>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monster Cables picked the wrong guy to threaten (2008)","url":"https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/","date":1742603437,"author":"wallflower","guid":168,"unread":true,"content":"<blockquote><p><em>… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. <strong>If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation. </strong> If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive </em><em>all of the information requested above, I will assume that you have abandoned these claims and closed your file.</em></p><p><em> As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so.</em></p><p><em> I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s </em><em>modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points.</em></p><p><em> After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where <strong>I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle.</strong> In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and <strong>I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds.</strong> As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not.</em></p><p><em> I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: <strong>there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. </strong>It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. <strong>Not only am I unintimidated by litigation; I sometimes rather miss it.</strong></em></p></blockquote><p>I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales.</p>","contentLength":3715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43442178"},{"title":"Choose Freedom, Not Trialware","url":"https://news.opensuse.org/2025/03/21/choose-freedom-not-trailware/","date":1742594174,"author":"/u/gabriel_3","guid":574,"unread":true,"content":"<p>A few weeks ago, a customer walked into a <a href=\"https://www.bestbuy.com/\">Best Buy</a>, which is a well-known retailer in North America that sells computers, appliances and gadgets of all kinds.</p><p>On a mission to buy a new laptop for a family member, the customer sought something simple, reliable and future-proof. After browsing through the selection, a solid <a href=\"https://www.lenovo.com\">Lenovo</a> laptop stood out; it had good hardware, decent battery life and no unnecessary gimmicks.</p><p>As the customer approached the checkout and began paying for the new laptop, the salesperson turned and asked an unexpected question:</p><p><strong>“Would you like to prepay for Windows?”</strong></p><p>The customer blinked in surprise.</p><p><strong>“These computers you are displaying don’t include Windows?”</strong></p><p> the salesperson responded. <strong>“Windows comes with a 30-day trial offer.”</strong></p><p>Without hesitation, the customer responded firmly:</p><p><strong>“Absolutely not. I’m putting Linux on it.”</strong></p><p>The salesperson hesitated, as if hearing this statement for the first time.</p><p>What many people don’t realize is that in certain regions, antitrust laws require manufacturers to sell computers with an option not to preinstall Windows. This is a response to decades of monopolistic practices, where users were forced to pay for a Microsoft license whether they wanted it or not. Some manufacturers have introduced Linux-preinstalled models or “no OS” options, though these remain relatively niche. In regions without such regulations, like the U.S., Windows-free computers are still uncommon outside of specialized retailers and business contracts.</p><p>Some stores (depending on the country) are legally required to offer a refund for the cost of the Windows license if the customer chooses not to use it. This is, however, rarely advertised. Many people don’t even know they have a choice or that there is a choice for freedom.</p><p>If you’re in the market for a new computer, don’t let the assumption of Windows dictate your purchase. Here’s how you can take control of your computing experience:</p><ul><li><strong>Ask for a non-Windows option</strong>: Many stores don’t advertise it, but some models are available without a preinstalled OS.</li><li>: If your country has consumer protection laws, you might be entitled to a refund for the Windows license if you don’t use it.</li><li><strong>Download and install openSUSE</strong>: Head to get.opensuse.org and choose the version that best fits your needs. Leap for long-term stability or Tumbleweed for the latest rolling updates.</li><li><strong>Enjoy a system without restrictions</strong>: No licensing fees, no activation headaches, and no locked-down software.</li></ul><p>Linux has always been there, unlocked and ready to go.</p><p>So the next time you walk into a store looking for a new computer, remember; you have a choice. And that choice can be freedom.</p><p><small> This is part of a series on <a href=\"https://news.opensuse.org/category/upgrade-to-freedom\">Upgrade to Freedom</a> where we offer reasons to transition from Windows to Linux.</small></p>","contentLength":2785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jgsozm/choose_freedom_not_trialware/"},{"title":"Davinci Resolved Add ProRes Encode Support on Linux","url":"https://www.newsshooter.com/2025/03/20/davinci-resolve-19-1-4-update/","date":1742592605,"author":"/u/Blackstar1886","guid":578,"unread":true,"content":"<p>Blackmagic Design has released <a rel=\"noreferrer noopener\" aria-label=\"DaVinci Resolve version 19.1.4 (opens in a new tab)\" href=\"https://www.blackmagicdesign.com/support/family/davinci-resolve-and-fusion\" target=\"_blank\">DaVinci Resolve version 19.1.4</a>. The big news is that it adds Apple ProRes encode support on Windows and Linux. Windows users have been waiting for this for a very long time and it will be welcome news for so many people who have had to do work around for years.</p><p>It also features additional software for remote monitoring and utilities for setting up hardware control panels, and it installs Blackmagic RAW Player and Proxy Generator into the applications folder.</p><h2>New in DaVinci Resolve 19.1.4</h2><ul><li>Support for Blackmagic RAW SDK 4.5.</li><li>Apple ProRes encode support on Windows and Linux.</li><li>Support for Samsung Log LUTs.</li><li>Addressed network decode performance for large embedded AAFs.</li><li>Addressed issues with exporting embedded AAFs.</li><li>Addressed audio stutters when recording interlaced media to tape.</li><li>Addressed inspector audio track selections for source multicams.</li><li>Addressed issues decoding some transport stream clips.</li><li>Support for Photon 4.10.8.</li><li>Addressed issue with importing large Dolby audio files.</li><li>Addressed playback delay after import for large Dolby audio files.</li><li>Addressed issue with default ISO selection for Canon RAW clips.</li><li>Addressed centre crop issue playing back ARRI RAW clips.</li><li>General performance and stability improvements.</li></ul><p>The free version of DaVinci Resolve 19 includes the same high-quality processing as DaVinci Resolve 19 Studio and can handle unlimited-resolution media files. However, it limits project mastering and output to Ultra HD resolutions or lower and only supports a single processing GPU on Windows and Linux.</p><p>If you need features such as support for multiple GPUs, 4K output, motion blur effects, temporal and spatial noise reduction, multiple AI-based tools, HDR tools, camera tracker, voice isolation, surround sound and immersive audio, multiple Resolve FX, 3D stereoscopic tools and remote rendering, upgrade to <a href=\"https://www.bhphotovideo.com/c/product/1196612-REG/blackmagic_design_dv_resstud_davinci_resolve.html/BI/7759/KBID/8285/DFF/d10-v21-t1-x1288059/SID/DFF\">DaVinci Resolve 19 Studio</a>. ($295 USD)</p>","contentLength":1884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1jgs3ue/davinci_resolved_add_prores_encode_support_on/"},{"title":"LoadBalancer and/or Reverse Proxy?","url":"https://www.reddit.com/r/kubernetes/comments/1jgrswe/loadbalancer_andor_reverse_proxy/","date":1742591788,"author":"/u/myridan86","guid":554,"unread":true,"content":"<p>In your opinion, what is the best practice?</p><p>I know that these are two services with different functions, but they can be used for the same purpose...</p><p>Today I have a cluster with an application that will be used on the public internet by users.</p><p>What is better, using the <strong>LoadBalancer service with a certificate</strong> or using a  to the cluster, with a certificate?</p>","contentLength":354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is the standard library for cryptographic operations in RUST.","url":"https://www.reddit.com/r/rust/comments/1jgqw88/what_is_the_standard_library_for_cryptographic/","date":1742589460,"author":"/u/paulex101","guid":607,"unread":true,"content":"<p>I've stumbled on quite some libraries but this seem to be the tops: - Ring</p><p>And for everyone there's always a warning \"Use at your own Risk\" i must say i find this funny and bothering at the same time coming from stable ecosystems e.g Java/Kotlin/JS </p><p>For context: I really just want to generate ECDH Key Pair, compute shared secrets and key derivations. </p><p>I'm just a few days new to Rust so please be nice!.</p>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"France rejects backdoor mandate","url":"https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate","date":1742589311,"author":"hn_acker","guid":205,"unread":true,"content":"<p><a href=\"https://www.lemonde.fr/societe/article/2025/03/21/l-assemblee-vote-pour-le-maintien-de-la-confidentialite-des-messageries-cryptees-lors-d-une-nuit-agitee_6584121_3224.html\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2019/01/give-ghost-backdoor-another-name\"></a><a href=\"https://www.justsecurity.org/64968/why-the-ghost-keys-solution-to-encryption-is-no-solution/\"></a><a href=\"https://www.internetsociety.org/resources/doc/2020/fact-sheet-ghost-proposals/\"></a></p><p><a href=\"https://www.laquadrature.net/en/warondrugslaw/\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2024/03/european-court-human-rights-confirms-undermining-encryption-violates-fundamental\"></a></p><p><a href=\"https://www.eff.org/deeplinks/2024/12/defending-encryption-us-and-abroad\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440513"},{"title":"Pen and Paper Exercises in Machine Learning (2022)","url":"https://arxiv.org/abs/2206.13446","date":1742587632,"author":"ibobev","guid":167,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440267"},{"title":"Use Long Options in Scripts","url":"https://matklad.github.io/2025/03/21/use-long-options-in-scripts.html","date":1742587020,"author":"OptionOfT","guid":166,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440184"},{"title":"Show HN: A terminal emulator in pure PHP","url":"https://github.com/soloterm/screen","date":1742579005,"author":"aarondf","guid":162,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43438797"},{"title":"IronRDP: a Rust implementation of Microsoft's RDP protocol","url":"https://github.com/Devolutions/IronRDP","date":1742571327,"author":"mikece","guid":165,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43436894"},{"title":"Show HN: Torch Lens Maker – Differentiable Geometric Optics in PyTorch","url":"https://victorpoughon.github.io/torchlensmaker/","date":1742563751,"author":"fouronnes3","guid":161,"unread":true,"content":"<p>Hello HN! For the past 6 months I've been working on an open source python library that implements differentiable geometric optics in PyTorch. It's very experimental still, but eventually the goal is to use it to design optical systems with a state of the art optimization framework and a beautiful code based API. Think OpenSCAD, but for optical systems.</p><p>Not only is PyTorch's autograd an amazing general purpose optimizer, but torch.nn (the neural network building blocks) can be used pretty much out of the box to model an optical system. This is because there is a strong analogy to be made between layers of a neural network, and optical elements in a so-called sequential optical system. So the magic is that we can stack lenses as if we were stacking Conv2D and ReLu layers and everything works out. Instead of Conv2D you have ray-surface collision detection, instead of ReLu you have the law of refraction. Designing lenses is surprisingly like training a neural network.</p><p>You should be able to `pip install torchlensmaker` to try it out, but I just set it up so let me know if there's any trouble.</p><p>I was part of the Winter 1'24 batch at the Recurse Center (<a href=\"https://www.recurse.com/\" rel=\"nofollow\">https://www.recurse.com/</a>) working on this project pretty much full time. I'm happy to talk about that experience too!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43435438"},{"title":"Show HN: My Attempt to Organize the World of AI Dev Tools","url":"https://aicode.danvoronov.com/tools/","date":1742556164,"author":"dan_voronov","guid":160,"unread":true,"content":"<h2>Integrated Development Environments</h2><p>Closed or unmaintained projects: <a target=\"_blank\" href=\"https://github.com/codestoryai/aide\">aide</a> (Feb 25, 2025).</p><h2>AI-Coding Extensions for IDEs</h2><p>Closed or unmaintained projects: <a target=\"_blank\" href=\"https://github.com/rubberduck-ai/rubberduck-vscode\">rubberduck</a> (Feb 4, 2024).</p><h2>Command Line Interface (CLI) Tools</h2><h2>Web AI-Powered Generators</h2><h2>AI-Enhanced Development Tools</h2><p>Instead of paying for individual  for every AI service, there are cost-effective and flexible alternatives. One option is to use&nbsp;<a target=\"_blank\" href=\"https://openrouter.ai/\">OpenRouter</a>, which acts as a unified API for various models. This can potentially consolidate costs and simplify integration.</p><p>In addition to OpenRouter's own  in the programming section, you can always check <a target=\"_blank\" href=\"https://lmarena.ai/?arena\">Chatbot Arena (formerly LMSYS)</a> to see what model is currently performing better with programming or WebDev or Copilot tasks.</p><p>If you have powerful hardware and prioritize privacy or want more control, you can run large language models (LLMs) locally. Tools like&nbsp;<a target=\"_blank\" href=\"https://ollama.com/\">Ollama</a>&nbsp;and&nbsp;<a target=\"_blank\" href=\"https://lmstudio.ai/\">LM Studio</a> make it easier to deploy and manage these models on your own machine. For code autocompletion specifically, consider solutions with local-first option like&nbsp;<a target=\"_blank\" href=\"https://www.tabbyml.com/\">TabbyML</a>.</p>","contentLength":1048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43434325"},{"title":"ChatLoopBackOff Episode 51 (WasmEdge)","url":"https://www.youtube.com/watch?v=Cxz7pC9Lq2k","date":1742533740,"author":"CNCF [Cloud Native Computing Foundation]","guid":379,"unread":true,"content":"<article>WasmEdge is a CNCF Sandbox project that is a high-performance WebAssembly runtime optimized for cloud-native and edge computing use cases. It’s designed to run WebAssembly (Wasm) applications at the edge and in the cloud, offering a fast, efficient, and secure way to execute containerized applications.\n\nUnlike traditional container runtimes, WasmEdge is designed to work efficiently with cloud-native ecosystems. Join CNCF Ambassador, Faeka Ansari as she explores how WasmEdge allows developers to run Wasm modules with low overhead.</article>","contentLength":537,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Cxz7pC9Lq2k?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Road Not Taken is Guaranteed Minimum Income","url":"https://blog.codinghorror.com/the-road-not-taken-is-guaranteed-minimum-income/","date":1742513593,"author":"Jeff Atwood","guid":396,"unread":true,"content":"<img src=\"https://blog.codinghorror.com/content/images/2025/03/IMG_7003-1.jpg\" alt=\"The Road Not Taken is Guaranteed Minimum Income\"><p>What is the American Dream?</p><p>In 1931, at the height of the Great Depression, James Truslow Adams <a href=\"https://en.wikipedia.org/wiki/American_Dream\" rel=\"noreferrer\">first defined</a> the American Dream as</p><blockquote>“[...] a land in which life should be better and richer and fuller for everyone, with opportunity for each according to ability or achievement. [...] not a dream of motor cars and high wages merely, but a dream of social order in which [everyone] shall be able to attain to the fullest stature of which they are innately capable, and be recognized by others for what they are, regardless of the fortuitous circumstances of birth or position”</blockquote><p>I wanted to know what these words meant to us today. I needed to know what parts of the American Dream we all still had in common. I had to make some sense of what was happening to our country. I’ve been writing on my blog since 2004, and on November 7th, I started writing the most difficult piece I have ever written. </p><p>I asked so many Americans to tell me what the American Dream personally meant to them, and I wrote it all down.</p><p>Later in November, I attended a theater performance of The Outsiders at my son’s public high school - an adaptation of <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/The_Outsiders_(novel)&amp;sa=D&amp;source=docs&amp;ust=1742507328643679&amp;usg=AOvVaw2GgwTbUV3qWhPvsHuYFgy7\" rel=\"noreferrer\">the 1967 novel by S.E. Hinton</a>. All I really knew was the famous “stay gold” line from the 1983 movie. But as I sat there in the audience among my neighbors, watching the complete story acted out in front of me by these teenagers, I slowly realized what \"stay gold\" meant: <em>sharing the American Dream.</em></p><p>We cannot merely attain the Dream. The dream is incomplete until we share it with our fellow Americans. That act of sharing is the final realization of everything the dream stands for.</p><p>Thanks to S.E. Hinton, I finally had a name for my essay, <a href=\"https://blog.codinghorror.com/stay-gold-america/\" rel=\"noreferrer\">“Stay Gold, America.”</a> I published it on January 7th, with a <strong>Pledge to Share the American Dream</strong>.</p><p>I encourage every American to <a href=\"https://www.charitynavigator.org/\" rel=\"noreferrer\">contribute soon</a>, however you can, to organizations you feel are effectively helping those most currently in need.</p><p>But short term fixes are not enough.</p><p>The <strong>Pledge To Share The American Dream</strong> requires a much more ambitious second act – deeper, long term changes that will take decades. Over the next five years, my family pledges half our remaining wealth to plant a seed toward foundational long term efforts ensuring that all Americans continue to have the same fair access to the American Dream.</p><p>Let me tell you about my own path to the American Dream. It was rocky. My parents were born into deep poverty in Mercer County, West Virginia, and Beaufort County, North Carolina. Our family eventually clawed our way to the bottom of the middle class in Virginia. </p><p>I won’t dwell on it, but every family has their own problems. We did not remain middle class for long. But through all this, my parents got the most important thing right: they loved me openly and unconditionally. That is everything. It’s the only reason I am standing here in front of you today.</p><p>With my family’s support, I managed to achieve a solid public education in Chesterfield County, Virginia, and had the incredible privilege of an affordable state education at the <a href=\"https://www.virginia.edu/aboutuva/\" rel=\"noreferrer\">University of Virginia</a>. This is a college uniquely rooted in the beliefs of one of the most prominent Founding Fathers, Thomas Jefferson. He was a living paradox. A man of profound ideals and yet flawed – trapped in the values of his time and place. </p><p>Still, he wrote “Life, liberty, and the pursuit of happiness” at the top of the Declaration of Independence. These words were, and still are, revolutionary. They define our fundamental shared American values, although we have not always lived up to them. The American Dream isn’t about us succeeding, alone, by ourselves, but about connecting with each other and succeeding together as Americans.</p><p>I’ve been concerned about wealth concentration in America ever since I watched a <a href=\"https://www.youtube.com/watch?v=QPKKQnijnsM\" rel=\"noreferrer\">2012 video by politizane</a> illustrating just how extreme wealth concentration already was. </p><p>I had no idea how close we were to the American Gilded Age from the late 1800s. This period was given a name in the 1920s by historians referencing Mark Twain’s 1873 novel, <a href=\"https://en.wikipedia.org/wiki/The_Gilded_Age:_A_Tale_of_Today\" rel=\"noreferrer\">The Gilded Age, A Tale of Today</a>. </p><p>During this time, labor strikes often turned violent, with the <a href=\"https://en.wikipedia.org/wiki/Homestead_strike\" rel=\"noreferrer\">Homestead Strike of 1892</a> resulting in deadly confrontations between workers and Pinkerton guards hired by factory owners. Rapid industrialization created hazardous working conditions in factories, mines, and railroads, where thousands died due to insufficient safety regulations and employers who prioritized profit over worker welfare.</p><p>In January 2025, while I was still writing <a href=\"https://blog.codinghorror.com/stay-gold-america/\" rel=\"noreferrer\">“Stay Gold, America”</a>, we entered the period of greatest wealth concentration in the entirety of American history. As of 2021, the <a href=\"https://usafacts.org/articles/how-this-chart-explains-americans-wealth-across-income-levels/\" rel=\"noreferrer\">top 1% of households controlled 32% of all wealth</a>, while the bottom 50% only have 2.6%. It's difficult to find more recent data, but wealth concentration has only intensified in the last four years.</p><p>We can no longer say “Gilded Age”.</p><p>We must now say “The First Gilded Age”.</p><p>Today, in our  Gilded Age, more and more people find their path to the American Dream blocked. When Americans face unaffordable education, lack of accessible healthcare, or lack affordable housing, they aren't just disadvantaged – they're trapped, often burdened by massive debt. They have no stable foundation to build their lives. They watch desperately, working as hard as they can, while life simply passes them by, without even the freedom to choose their own lives.</p><p>They don't have time to build a career. They don't have time to learn, to improve. They don't get to start a business. They can’t choose where their kids will grow up, or whether to have children at all, because they can’t afford to. Here in the land of opportunity, the pursuit of happiness has become an endless task for too many. </p><p>We are denying people any real chance of achieving the dream that we promised them – that we promised the entire world – when we founded this nation. It is such a profound betrayal of everything we ever dreamed about. Without a stable foundation to build a life on, our fellow Americans cannot even  the American Dream, much less achieve it.</p><p>I ask you this: as an American, what is the purpose of a dream left unshared with so many for so long? What’s happening to our dream? Are we really willing to let go of our values so easily? We’re Americans. We fight for our values, the values embodied in our dream, the ones we founded this country on. </p><p>Why aren’t we sharing the American Dream? </p><p>Why aren’t we giving everyone a fair chance at Life, Liberty, and the Pursuit of Happiness by providing them the fundamentals they need to get there?</p><p>The Dream worked for me, decades ago, and I deeply believe that the American Dream can still work for everyone – if we ensure every American has the same fair chance we did. The American Dream was never about a few people being extraordinarily wealthy. It’s about everyone having an equal chance to succeed and pursue their dreams – their own happiness. It belongs to them. I think we owe them at least that. I think we owe  at least that.</p><p>What can we do about this? There are no easy answers. I can’t even pretend to have the answer, because there isn’t any one answer to give. Nothing worth doing is ever that simple. But I can tell you this: all the studies and all the data I’ve looked at have strongly pointed to one foundational thing we can do here in America over the next five years.</p><p>Natalie Foster, co-founder of the <a href=\"https://economicsecurityproject.org/\" rel=\"noreferrer\">Economic Security Project</a>, makes a powerful case for the idea that, with all this concentrated wealth, we can offer a <strong>Guaranteed Minimum Income</strong> in the poorest areas of this country – the areas of most need, where money goes the farthest – to unlock vast amounts of untapped American potential. </p><p>This isn’t a new idea. We’ve been doing this a while now in different forms, but we never called it Guaranteed Minimum Income.</p><p>In , Thomas Paine proposed a retirement pension funded by estate taxes. It didn’t go anywhere, but it planted a seed. Much later we implemented the Social Security Act in  . The economic chaos of the Great Depression coupled with the inability of private philanthropy to provide economic security inspired Franklin Roosevelt’s New Deal government programs. The most popular and effective program to emerge from this era was Social Security, providing a guaranteed income for retirees. Before Social Security,  of seniors lived in poverty. Today only 10% of seniors live in poverty.</p><p>In his  book <a href=\"https://en.wikipedia.org/wiki/Where_Do_We_Go_from_Here:_Chaos_or_Community%3F\" rel=\"noreferrer\">Where Do We Go From Here: Chaos or Community</a>, Martin Luther King Jr made the moral case for a form of UBI, Universal Basic Income. King believed that economic insecurity was at the root of all inequality. He stated that a guaranteed income — direct cash disbursements — was the simplest and best way to fight poverty.</p><p>In , Congress established the Supplemental Security Income (SSI) program, providing direct cash assistance to low-income elderly, blind, and disabled individuals with little or no income. This cash can be used for food, housing, and medical expenses, the essentials for financial stability. As of January, 2025, over 7.3 million people receive SSI benefits.</p><p>In 1975, Congress passed the Tax Reduction Act, establishing the Earned Income Tax Credit. This tax credit benefits working-class parents with children, encouraging work by increasing the income of low-income workers. In 2023, it lifted about 6.4 million people out of poverty, including 3.4 million children. <a href=\"https://www2.census.gov/library/publications/2024/demo/p60-283.pdf\" rel=\"noreferrer\">According to the Census Bureau</a>, it is the second most effective anti-poverty tool after Social Security.</p><p>In 2019, directly inspired by King, mayor <a href=\"https://en.wikipedia.org/wiki/Michael_Tubbs\" rel=\"noreferrer\">Michael Tubbs</a> – at age 26, one of the youngest mayors in American history – launched the $3 million <a href=\"https://www.stocktondemonstration.org/\" rel=\"noreferrer\">Stockton Economic Empowerment Demonstration</a>. It provided 125 residents with $500 per month in unconditional cash payments for two years. The program found that recipients experienced improved financial stability, increased full-time employment, and enhanced well-being.</p><p>In my “Stay Gold, America” blog post, I referenced the Robert Frost \"Stay Gold\" poem and S.E. Hinton's famous famous novel The Outsiders, urging us to retain our youthful ideals as we grow older. Ideals embodied in the American Dream.</p><p>Which brings us to another Robert Frost poem, the <a href=\"https://www.poetryfoundation.org/poems/44272/the-road-not-taken\" rel=\"noreferrer\">Road Not Taken</a>. Our proposal to ensure access to the American Dream is to follow the path less travelled by:  <strong>Guaranteed Minimum Income. </strong>GMI is a simpler, more practical, more scalable plan to directly address the root of economic insecurity with minimum bureaucracy.</p><p>We are partnering with GiveDirectly, who oversaw <a href=\"https://www.givedirectly.org/united-states/\" rel=\"noreferrer\">the most GMI studies</a> in the United States, and OpenResearch, who just completed the <a href=\"https://www.openresearchlab.org/studies/unconditional-cash-study/study\" rel=\"noreferrer\">largest, most detailed GMI study</a> ever conducted in this country in 2023. We are working together to launch a new Guaranteed Minimum Income initiative in rural American communities.</p><p>Network effects within communities explain why equality of opportunity is so effective, and why a shared American Dream is the most powerful dream of all. The potential of the American Dream becomes vastly greater as more people have access to it, .</p><p>They share it with their families, their friends, and their neighbors. The groundbreaking, massive <a href=\"https://www.openresearchlab.org/studies/unconditional-cash-study/documentation\" rel=\"noreferrer\">2023 OpenResearch UBI study data</a> showed that when you give money to the poorest among us, they consistently go  to share that money with others in desperate need.</p><p>The power of opportunity is not in what it can do for one person, but how it connects and strengthens bonds between people. When you empower a couple, you allow them to build a family. When you empower families, you allow them to build a community. When you guarantee fundamentals, you're providing a foundation for those connections to grow and thrive. This is the incredible power and value of community. That is what we are investing in – each other.</p><p>A system where there are no guarantees creates conflict. It creates inequality. A massive concentration of wealth in so few hands weakens connections between us and prevents new ones. America began as a place of connection. Millions of us came together to build this nation, not individually, but together. Equality is connection, and connection is more valuable than any product any company will ever sell you.</p><p>You may ask, why focus on rural communities? There are consistently higher poverty rates in rural counties, with fewer job opportunities, lower wages, and worse access to healthcare and education. It’s not a new problem, either — places like Appalachia, the Mississippi Delta, and American Indian reservations have been stuck in poverty for decades, with some counties like Oglala Lakota, SD (55.8%) and McDowell, WV (37.6%) hitting extreme levels. Meanwhile, urban counties rarely see numbers that high. The data from the US Census and USDA Economic Research Service make it clear: if you’re poor in America, being rural makes it even harder to escape.</p><p>Rural areas offer smaller populations, which is helpful because we need to start small with lots of tightly controlled studies that we can carefully scale and improve on for larger areas. We hope to build a large body of scientific data showing that GMI really does improve the lives, and the communities, of our fellow Americans.</p><h3>The initial plan is to target a few counties that I have a personal connection to, and are still currently in poverty, decades later:</h3><ul><li>My father was born in <a href=\"https://en.wikipedia.org/wiki/Mercer_County,_West_Virginia\" rel=\"noreferrer\">Mercer County</a>, West Virginia, where the collapse of coal mining left good people struggling to survive. Their living and their way of life is now all but gone, and good jobs are hard to find.</li><li>My mother’s birthplace, <a href=\"https://en.wikipedia.org/wiki/Beaufort_County,_North_Carolina\" rel=\"noreferrer\">Beaufort County</a>, North Carolina, has been hit just as hard, with farming and factory jobs disappearing and families left wondering what’s next.</li><li>Our third county is yet to be decided, but will be a community also facing the same systemic, generational obstacles to economic stability and achieving the American Dream.</li></ul><p>We will work with existing local groups to coordinate GMI studies where community members choose to enroll. We will conduct outreach and and provide mentorship to these opt-in study participants. It will be teamwork between Americans.</p><p>We hope Veterans will play a crucial role in our effort. We plan to work with local communities and veteran-serving organizations to engage veterans to support and execute our GMI programs – the same veterans who served our country with distinction, returning home with exceptional leadership skills and a deep commitment to their communities. Their involvement ensures these programs reflect core American values of self-reliance and community service to fellow Americans. </p><p>We'll also partner with established community organizations — churches, civic groups, community colleges, local businesses. These partnerships help integrate our GMI studies with existing support systems, rather than creating new ones.</p><p>GiveDirectly and OpenResearch will build on their existing body of work, gathering extensive data from these refined studies. We'll measure employment, entrepreneurship, education, health, and community engagement. We'll conduct regular interviews with participants to understand their experience. How is this working for you? How can we make it better? You tell us. How can we make it better together?</p><p>Economic security isn't only about individual well-being – it's the bedrock of democracy. When people aren't constantly worried about feeding themselves, feeding their family, having decent healthcare, having a place to live… we have given them room to breathe. We have given them freedom. The freedom to raise their children, the freedom to start businesses, the freedom to choose where they work, the freedom to volunteer... the freedom to .</p><p>This isn’t about ideology or government. It’s about us, as Americans, working together to invest in our future – possibly the greatest unlocking of human potential in our entire history. I do not say these things lightly. I’ve seen it work. I’ve looked at all the existing study data. A little bit of money is incredibly transformational for people in poverty – the people who need it the most – the people who cannot live up to their potential because they’re so busy simply trying to survive. Imagine what they could do if we gave them just a little breathing room.</p><p>GMI is a long term investment in the future of what America should be, the way we wrote it down in the Declaration of Independence, perhaps incompletely – but our democracy was always meant to be malleable, to change, to adapt, and improve.</p><p>I’d like to conclude by mentioning Aaron Swartz. He was a precocious teenage programmer much like myself. Aaron helped develop RSS web feeds, co-founded Reddit, and worked with Creative Commons to create flexible copyright licenses for the common good. He used technology to make information universally accessible to everyone.</p><p>Aaron created a system to download public domain court documents from PACER, a government database that charged fees for accessing what he believed should be freely available public information. A few years later, while visiting MIT under their open campus policy and as a research fellow at Harvard, he used MIT's network to download millions of academic articles from JSTOR, another fee-charging online academic journal repository, intending to make this knowledge freely accessible. Since taxpayers had funded much of this research, why shouldn’t that knowledge be freely available to everyone?</p><p>What Aaron saw as an act of academic freedom and information equality, authorities viewed as a crime—he was arrested in January 2011 and charged with multiple felonies for what many considered to be nothing more than accessing knowledge that should have been freely available to the public in the first place.</p><p>Despite JSTOR declining to pursue charges and MIT eventually calling for leniency, federal prosecutors aggressively pursued felony charges against Aaron with up to 35 years in prison. Facing overwhelming legal pressure and the prospect of being labeled a felon, Aaron took his own life at 26. This sparked widespread criticism of prosecutorial overreach and prompted discussions about open access to information. Deservedly so. Eight days later, in this very hall, there was a standing room only memorial service praising Aaron for his commitment to the public good.</p><p>Aaron pursued what was right for we, the people. He chose to build the public good despite knowing there would be risks. He chose to be an activist. I think we should all choose to be activists, to be brave, to stand up for our defining American principles. </p><h2>There are two things I ask of you today.</h2><ul><li>Visit <a href=\"https://www.givedirectly.org/rural-us/\">https://www.givedirectly.org/rural-us</a> where we'll be documenting our journey and findings from the initial three GMI rural county studies. Let’s find out together how guaranteed minimum income can transform American lives.</li><li>Talk about Guaranteed Minimum Income in your communities. Meet with your state and local officials. Share the existing study data. Share outcomes. Ask them about conducting GMI studies like ours in your area. We tell ourselves stories about why some people succeed and others don't. Challenge those stories. Economic security is not . It is an  in vast untapped American potential in the poorest areas of this country.</li></ul><p>My family is committing 50 million dollars to this endeavor, but imagine if we had even more to share. Imagine how much more we could do, if we build this together, starting today. Decades from now, people will look back and wonder why it took us so long to share our dream of a better, richer, and fuller life with our fellow Americans.</p><p>I hope you join us on this grand experiment to share our American Dream. I believe everyone deserves a fair chance at what was promised when we founded this nation: Life, Liberty, and the pursuit of The American Dream.</p><ul><li> – <em>Where Do We Go From Here: Chaos or Community?</em> (1967)</li><li><strong>Mark Twain &amp; Charles Dudley Warner</strong> – <em>The Gilded Age: A Tale of Today</em> (1873)</li></ul><ul><li> – <em>Utopia for Realists: How We Can Build the Ideal World</em> (2014)</li><li> – <em>Give People Money: How a Universal Basic Income Would End Poverty, Revolutionize Work, and Remake the World</em> (2018)</li><li> – <em>The War on Normal People: The Truth About America’s Disappearing Jobs and Why Universal Basic Income Is Our Future</em> (2018)</li><li> – <em>Basic Income: And How We Can Make It Happen</em> (2017)</li><li> – <em>Not Enough: Human Rights in an Unequal World</em> (2018)</li></ul><h3><strong>Wealth Inequality and Labor History</strong></h3><ul><li> – <em>The Price of Inequality: How Today's Divided Society Endangers Our Future</em> (2012)</li><li> –  (2023)</li><li> – <em>Nickel and Dimed: On (Not) Getting By in America</em> (2001)</li><li> – <em>Capital in the Twenty-First Century</em> (2013)</li><li> – <em>The Divide: A Brief Guide to Global Inequality and Its Solutions</em> (2017)</li></ul><ul><li> – <em>The Guarantee: Inside the Fight for America's Next Economy</em> (2024)</li><li> – <em>The Deeper the Roots: A Memoir of Hope and Home</em> (2021)</li></ul>","contentLength":20811,"flags":null,"enclosureUrl":"https://blog.codinghorror.com/content/images/2025/03/IMG_7003-1.jpg","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}