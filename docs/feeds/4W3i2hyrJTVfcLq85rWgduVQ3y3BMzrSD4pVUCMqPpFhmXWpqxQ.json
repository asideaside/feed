{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":13,"items":[{"title":"California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers","url":"https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers","date":1742666138,"author":"thoughtpeddler","guid":175,"unread":true,"content":"<p align=\"center\"><em>Californians have the right to direct the company to delete their genetic data</em></p><p>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p><p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p><p><b>To Delete Genetic Data from 23andMe:</b></p><ol><li>Consumers can delete their account and personal information by taking the following steps:</li><li>Log into your 23andMe account on their website.&nbsp;</li><li>Go to the “Settings” section of your profile.</li><li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li><li>Click “View” next to “23andMe Data”</li><li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li><li>Scroll to the “Delete Data” section.&nbsp;</li><li>Click “Permanently Delete Data.”&nbsp;</li><li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li></ol><p><b>To Destroy Your 23andMe Test Sample:</b></p><p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p><p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p><p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p><p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p><p>To learn more about the CCPA, please visit&nbsp;<a href=\"https://oag.ca.gov/privacy/ccpa\">here</a>.&nbsp;&nbsp;</p>","contentLength":2952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43447421"},{"title":"Map Features in OpenStreetMap with Computer Vision","url":"https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/","date":1742665330,"author":"Brysonbw","guid":174,"unread":true,"content":"<p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href=\"https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai\"></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href=\"https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><p>Data is one of the most important components of any AI application, and <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><p>As a long-time user and contributor to <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href=\"https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai\"></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li> with <a href=\"https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai\"></a>, by <a href=\"https://www.ultralytics.com/?ref=blog.mozilla.ai\" rel=\"noreferrer\">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li>with <a href=\"https://ai.meta.com/sam2/?ref=blog.mozilla.ai\"></a>, by <a href=\"https://ai.meta.com/?ref=blog.mozilla.ai\" rel=\"noreferrer\">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href=\"https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai\"></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3>The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>For fetching OpenStreetMap data, we use:</p><p>Once all the polygons have been downloaded, you can choose a <a href=\"https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai\"></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href=\"https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai\"></a> from <a href=\"https://www.mapbox.com/?ref=blog.mozilla.ai\"></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href=\"https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format\"></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href=\"https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai\"></a> (or any other model supported by Ultralytics) is quite easy. </p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href=\"https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><p>After a point is selected, a bounding box is computed around it based on the argument.</p><p>All the existing elements of interest are downloaded from <a href=\"https://www.openstreetmap.org/?ref=blog.mozilla.ai\"></a>, and all the tiles are downloaded from <a href=\"https://www.mapbox.com/?ref=blog.mozilla.ai\"></a> and joined to create a stacked image.</p><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href=\"https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai\">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href=\"https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai\"></a>) to obtain a segmentation mask.</p><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as are displayed one by one for manual verification and filtering.</p><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href=\"https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai\"></a></p><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href=\"https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai\"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p>","contentLength":5530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43447335"},{"title":"Facebook to stop targeting ads at UK woman after legal fight","url":"https://www.bbc.co.uk/news/articles/c1en1yjv4dpo","date":1742660568,"author":"dijksterhuis","guid":173,"unread":true,"content":"<p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>\"Organisations must respect people's choices about how their data is used,\" a spokesperson for the ICO said. \"This means giving users a clear way to opt out of their data being used in this way.\"</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, \"which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook\".</p><p>She said that she did not want to stop using Facebook, saying that it is \"filled with all of those connections and family and friends, and entire chapters of my life\".</p>","contentLength":1092,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43446821"},{"title":"PyTorch Internals: Ezyang's Blog","url":"https://blog.ezyang.com/2019/05/pytorch-internals/","date":1742654344,"author":"Anon84","guid":172,"unread":true,"content":"<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p><p>Hi everyone!  Today I want to talk about the internals of <a href=\"https://pytorch.org/\">PyTorch</a>.</p><p>This talk is for those of you who have used PyTorch, and thought to yourself, \"It would be great if I could contribute to PyTorch,\" but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a \"tensor library that supports automatic differentiation\", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p><p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of \"extension points\", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p><p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p><p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p><p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p><p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p><p>So, what do strides have to do with this picture?</p><p>Suppose that I want to access the element at position  in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p><p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p><p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p><p>Using advanced indexing support, I can just write  to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p><blockquote><p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p><p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href=\"https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak\">fixed this in Java 7u6</a>.</p></blockquote><p>A more interesting case is if I want to take the first column:</p><p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a \"stride\": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p><p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href=\"https://ezyang.github.io/stride-visualizer/index.html\">Stride Visualizer</a>.</p><p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p><p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p><p>One thing to realize is that there is always a pair of Tensor-Storage, even for \"simple\" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with ).</p><blockquote>\nBy the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote><p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call , two dispatches happen:</p><p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a  dispatch because these kernels may live in separate libraries (e.g.,  versus ), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p><p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on  is different from the code for .  It stands to reason you need separate kernels for each dtype.</p><p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p><p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p><p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p><ul><li>The , the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li><li>The , which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li><li>The , which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li></ul><p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in  the combination could make sense, and thus we support expressing it, at the very least.</p><p>There's one last way you can make an \"extension\" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p><p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p><p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like \"What's the size of a tensor?\" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href=\"https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h\">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all \"tensor\"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are  important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p><p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p><p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p><p>...and fill in the missing code that actually computes the gradients of your network:</p><p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p><ol><li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation\">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations \"backward\" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute ; then, the first thing we do in the blue part of the program is compute .   was computed from , so we compute .  Technically, these variables which we call  are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them  and mostly everyone knows what we mean.</li><li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the  operation is translated into a  operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced , the backward operation takes  as an input.</li></ol><p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p><p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls  in their PyTorch script.</p><blockquote>\nThis is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href=\"https://github.com/pytorch/pytorch/issues/13638\">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote><p>We also have to update our picture about dispatch:</p><p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p><p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p><p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p><p>Enough about concepts, let's look at some code.</p><p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href=\"https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure\">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p><ul><li>First,  contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li><li>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li><li>, short for \"A Tensor Library\" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the \"native\" operators, which are modern, C++ implementations of operators, and the \"legacy\" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li><li>, which is a pun on Caffe2 and A\"Ten\" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li></ul><p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in .</p><p>Let's see how this separation of code breaks down in practice:</p><p>When you call a function like , what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p><ol><li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li><li>We handle  dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li><li>We handle  dispatch (Type)</li><li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li></ol><p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p><p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like .   is the implementation of one such implementation.</p><p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class  to actually pull out C++ objects out of the Python  and ; we then call a  function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor .  On its way back, we rewrap the returned  back into a .</p><p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p><p>When we call the  method on the  class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a \"Type\" object.  This method is the actual virtual method (this is why I say Type is just a \"gadget\" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named .  This happens to be because we have an implementation of  that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like .  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p><blockquote>\nHopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing  dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote><p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p><p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p><p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p><p>We generally think of a kernel in PyTorch consisting of the following parts:</p><ol><li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li><li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li><li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li><li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li><li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are \"implicitly\" parallelized, since their programming model is built on top of massive parallelization).</li><li>Finally, you need to access the data and do the computation you wanted to do!</li></ol><p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p><p>To take advantage of all of the code generation which PyTorch brings, you need to write a  for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md\">README in native</a> is for more information about this format.</p><p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, , which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of , and most important data types in PyTorch have  implementations.</p><p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each  into a , which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g.,  tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the  metadata.</p><p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write  operators: , which operates on a preallocated output (this implements the  keyword argument), , which operates inplace, and , which is the plain old functional version of an operator.</p><p>Most of the time,  is the real workhorse, and  and  are just thin wrappers around ; but sometimes writing specialized implementations for each case are warranted.</p><p>To do dtype dispatch, you should use the  macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p><p>This macro doesn't just \"do dispatch\", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want , but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href=\"https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62\">Dispatch.h</a> for how to select the correct one for your use-case.</p><p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p><p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p><ol><li>If you just want to get a value at some specific location, you should use .  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like , we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a , which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha:  defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li><li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the .   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li><li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The  class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like  then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li></ol><p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p><ol><li>It's written in C style, no (or very little) use of C++.</li><li>It's manually refcounted (with manual calls to  to decrease refcounts when you're done using tensors), and</li><li>It lives in  directory, which means that we are actually going to compile the file multiple times, but with different .</li></ol><p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p><p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, : it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p><p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: \"It's hard to work on PyTorch.\"</p><ol><li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li><li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href=\"https://github.com/pytorch/ossci-job-dsl\">download and run the Docker images locally</a></li><li>The <a href=\"https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache\">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li><li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li></ol><p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p><p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled  mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged\">high priority</a> or look up issues specific to some module, e.g., <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22\">autograd</a> or find issues which we think are <a href=\"https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall\">small</a> (word of warning: we're sometimes wrong!)</p><p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I  merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>","contentLength":30265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445931"},{"title":"Show HN: FastOpenAPI – automated docs for many Python frameworks","url":"https://github.com/mr-fatalyst/fastopenapi","date":1742652630,"author":"mr_Fatalyst","guid":163,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445720"},{"title":"Landrun: Sandbox any Linux process using Landlock, no root or containers","url":"https://github.com/Zouuup/landrun","date":1742651819,"author":"Zoup","guid":171,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43445662"},{"title":"When you deleted /lib on Linux while still connected via SSH (2022)","url":"https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/","date":1742628245,"author":"todsacerdoti","guid":170,"unread":true,"content":"<p>Let’s first not talk about why this can happen, but deleting , , or some other essential runtime files happens quite a lot (as you can see: <a href=\"https://unix.stackexchange.com/questions/704581/accidentally-deleted-lib-and-lib64-from-on-rhel\">here</a>, <a href=\"https://askubuntu.com/questions/47608/accidently-deleted-usr-lib-so\">here</a>, <a href=\"https://serverfault.com/questions/977095/accidentally-unlinked-usr-lib-libcrypt-a-is-there-a-way-to-recover-from-this\">here</a>,<a href=\"https://unix.stackexchange.com/questions/704581/accidentally-deleted-lib-and-lib64-from-on-rhel\"></a>and <a href=\"https://stackoverflow.com/questions/12249547/how-to-recover-after-deleting-the-symbolic-link-libc-so-6\">here</a>). In this post, I will only discuss what happens when you delete  on Linux and how to recover from that.</p><p>The easy solution for everything is to replace the missing files, but this can be difficult if  is deleted because we won’t have , which is needed to run any dynamic executable. When you deleted , all non-static executable (such as , , , will output): </p><div><pre title=\"\">No such file or directory\n</pre></div><p>You will also be unable to open any new connection using ssh, or open a new tmux window/pane if you are using tmux. So you can only rely on your current shell built in, and some static executables that you have on the system.</p><p>If you have a static  installed, then it can be your rescue. You can use  from  to download libraries from a clean system.  For your information: Debian has  installed by default, but the default is not the static version.</p><p>If you are worried that this kind of problem might happen to you in the future: Install the static version of the busybox binary, and confirm that it is the correct version.</p><p>I assume right now that you don’t have a static busybox, and you don’t even have any static executables (which is the situation in many cases, like in the default install of minimal Debian). My solution for this is to download a static busybox from another machine. </p><p>I also assume that you have bash installed (which is the default for most systems).  Bash has a lot of default built-ins that we can use.  There is a <a href=\"https://unix.stackexchange.com/posts/421403/revisions\">solution from here</a> that can be used to download a file using only built-in bash functions. Other <a href=\"https://unix.stackexchange.com/questions/83926/how-to-download-a-file-using-just-bash-and-nothing-else-no-curl-wget-perl-et/421403#421403\">solutions on this thread </a>rely on external command (such as ).  Please note that you need to set the environment  variable  to ; Otherwise, this script will incorrectly handle Unicode bytes.</p><p>Of course, we can’t  the destination file to be executable, so we need to overwrite an existing executable. If you have busybox installed (even if it is the non-static version), you can overwrite this file. At this point, you can start the rescue mission: for example, use  to download fresh  from another system.</p><p>Please note that busybox can’t function with a name that is not a busybox applet name. So if you overwrite for example, the  binary with , then it won’t work (it will say: ).  If you don’t have , I suggest overwriting , then you can use  to create a copy of  as  (which will be executable).</p><p>If you have a more advanced shell (e.g: zsh), <a href=\"https://zsh.sourceforge.io/Doc/Release/TCP-Function-System.html\">it has TCP modules already built in</a>. You can easily use  from another machine to send a file to the target machine. Now, let’s assume that you have a very basic shell, for example: . Most shell  (including dash), has  as built-in, and we can use this to construct binary files. </p><p>Most (all?) shell’s built-in   implementation supports  where  is 3 digit octal. First approach is to just convert , but this file is quite big (2 megabyte). Copy-pasting  large  commands is tedious and is error-prone. We need a small static binary that can help us.</p><p>This  trick will also work for other OS, if you can create a small binary for that OS.</p><h2>Creating a small ELF for Linux</h2><p>You can create a very tiny executable if you use assembly directly, but let’s try to do this using C, so it can be portable across different architectures. The smallest useful program that I can think of is just to copy from stdin to stdout, so we can prepare  on a machine:</p><p><code>cat busybox | nc -v -l -p 10000</code></p><p>and then we can do this from the borked machine:</p><p>The source code can be like this:</p><div><pre title=\"\">#include \"unistd.h\"\n\nint main()\n{\n        char x;\n        while (1) {\n                int c = read(0, &amp;x, 1);\n                if (c!=0) break;\n                c = write(1, &amp;x, 1);\n                if (c!=0) break;\n        }\n        return 0;\n}\n</pre></div><p>If we try to compile this with standard C library (on AMD64 machine), the result is 776KB. </p><p><code>gcc -Os -Wl,--build-id=none -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd</code></p><p>We get a 4536 bytes binary. Quite good. If we add , we can even get a smaller size. </p><p><code>gcc -Os -Wl,--build-id=none -z max-page-size=0x04 -fno-asynchronous-unwind-tables -fno-ident -s -nostdlib -nodefaultlibs -static -include nolibc.h fd.c -lgcc -o fd</code></p><p>It is now 672 bytes. Small enough to transfer. We can convert this using Python.</p><div><pre title=\"\">import sys\n\nwith open(sys.argv[1], \"rb\") as f:\n    data = f.read()\n\nstart = 0\nwidth = 20\ntargetname = sys.argv[2]\nwhile True:\n    part = data[start:start+width]\n    if part=='':\n        break\n    a = ''.join(['\\\\'+(oct(ord(i)).zfill(3))[-3:] for i in part])\n    dest = '&gt;'\n    if start&gt;0:\n        dest += '&gt;'\n    dest += ' ' + targetname\n    print(\"printf '{}' {} \".format(a, dest))\n    start += width\n\n</pre></div><p>We can then copy paste this to our ssh session, then do the  redirection trick.</p><p>Of course, we can also write a complete program that makes the TCP connection instead of relying on bash redirection.</p><h2>I hope you will never need this knowledge</h2><p>This problem occurred to me a few days ago when I updated my <a href=\"https://solar.yohanes.mobi/\">Solar Powered Pi Zero</a>, and somehow  got deleted (not sure what caused it). This is not a very important machine, and I could have just reimaged the MicroSD card and be done with it, but I was curious if I could recover from the error. </p><p>I hope you will never have this error on your production/important machine, but if you have this problem in the future, I hope this post will help you recover from the situation.</p>","contentLength":5503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43444160"},{"title":"Scallop – A Language for Neurosymbolic Programming","url":"https://www.scallop-lang.org/","date":1742618708,"author":"andsoitis","guid":169,"unread":true,"content":"<img src=\"https://www.scallop-lang.org/img/icon-solver.png\"><p>\n                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and\n                  differentiable modes of reasoning.\n                  These modes are configurable to suit the needs of different AI applications.\n                </p>","contentLength":275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43443640"},{"title":"Monster Cables picked the wrong guy to threaten (2008)","url":"https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/","date":1742603437,"author":"wallflower","guid":168,"unread":true,"content":"<blockquote><p><em>… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. <strong>If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation. </strong> If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive </em><em>all of the information requested above, I will assume that you have abandoned these claims and closed your file.</em></p><p><em> As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so.</em></p><p><em> I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s </em><em>modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points.</em></p><p><em> After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where <strong>I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle.</strong> In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and <strong>I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds.</strong> As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not.</em></p><p><em> I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: <strong>there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. </strong>It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. <strong>Not only am I unintimidated by litigation; I sometimes rather miss it.</strong></em></p></blockquote><p>I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales.</p>","contentLength":3715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43442178"},{"title":"Pen and Paper Exercises in Machine Learning (2022)","url":"https://arxiv.org/abs/2206.13446","date":1742587632,"author":"ibobev","guid":167,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440267"},{"title":"Use Long Options in Scripts","url":"https://matklad.github.io/2025/03/21/use-long-options-in-scripts.html","date":1742587020,"author":"OptionOfT","guid":166,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43440184"},{"title":"IronRDP: a Rust implementation of Microsoft's RDP protocol","url":"https://github.com/Devolutions/IronRDP","date":1742571327,"author":"mikece","guid":165,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43436894"},{"title":"Crabtime: Zig’s Comptime in Rust","url":"https://crates.io/crates/crabtime","date":1742409851,"author":"klaussilveira","guid":164,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43415820"}],"tags":["dev","hn"]}